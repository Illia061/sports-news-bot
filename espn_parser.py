import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from zoneinfo import ZoneInfo
import google.generativeai as genai
import os

KIEV_TZ = ZoneInfo("Europe/Kiev")

class ESPNSoccerParser:
    def __init__(self):
        self.base_url = "https://www.espn.com/soccer/"
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Connection": "keep-alive",
        })
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ AI –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
        self.gemini_api_key = os.getenv("GEMINI_API_KEY")
        self.model = None
        self._init_translator()
    
    def _init_translator(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç AI –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞"""
        if self.gemini_api_key:
            try:
                genai.configure(api_key=self.gemini_api_key)
                self.model = genai.GenerativeModel("gemini-2.5-flash")
                print("‚úÖ AI –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ AI –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞: {e}")
                self.model = None
        else:
            print("‚ö†Ô∏è GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω - –ø–µ—Ä–µ–≤–æ–¥ –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω")
    
    def get_page_content(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            return BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None
    
    def find_top_headlines_section(self, soup):
        """–ù–∞—Ö–æ–¥–∏—Ç –±–ª–æ–∫ 'Top Headlines' –Ω–∞ ESPN Soccer"""
        
        # –°–ø–æ—Å–æ–± 1: –ü–æ–∏—Å–∫ –ø–æ —Ç–æ—á–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
        header_texts = [
            "Top Headlines",
            "top headlines",
            "Headlines"
        ]
        
        for header_text in header_texts:
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å —Ç–∞–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º
            header_element = soup.find(text=re.compile(header_text, re.I))
            if header_element:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫: '{header_text}'")
                
                # –ù–∞—Ö–æ–¥–∏–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å–µ–∫—Ü–∏–∏
                parent = header_element.parent
                while parent and parent.name not in ['section', 'div', 'article']:
                    parent = parent.parent
                
                if parent:
                    # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å–æ —Å–ø–∏—Å–∫–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π
                    news_container = parent.find_next(['div', 'ul', 'section'])
                    if news_container:
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
                        return news_container
                    else:
                        return parent
        
        # –°–ø–æ—Å–æ–± 2: –ü–æ–∏—Å–∫ –ø–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º –¥–ª—è ESPN –∫–ª–∞—Å—Å–∞–º
        possible_selectors = [
            # ESPN —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '.contentItem',
            '.headlines',
            '.top-headlines',
            '.news-feed',
            '.contentItem__content',
            '.story-feed',
            
            # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤
            '[class*="headline"]',
            '[class*="story"]',
            '[class*="news"]',
            '.col-a',  # ESPN –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–ª–æ–Ω–æ—á–Ω—É—é —Å–µ—Ç–∫—É
            '.col-one'
        ]
        
        for selector in possible_selectors:
            elements = soup.select(selector)
            for element in elements:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —ç–ª–µ–º–µ–Ω—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ—É—Ç–±–æ–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
                links = element.find_all('a', href=True)
                soccer_links = [link for link in links if self.is_soccer_news_link(link.get('href', ''))]
                
                if len(soccer_links) >= 3:  # –ï—Å–ª–∏ –µ—Å—Ç—å –º–∏–Ω–∏–º—É–º 3 —Ñ—É—Ç–±–æ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ —Å —Ñ—É—Ç–±–æ–ª—å–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                    return element
        
        # –°–ø–æ—Å–æ–± 3: –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        print("‚ö†Ô∏è –ò—â–µ–º —Ñ—É—Ç–±–æ–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ...")
        
        main_content_selectors = [
            'main',
            '#main-container',
            '.main-content',
            '.page-container',
            'body'
        ]
        
        for selector in main_content_selectors:
            main_content = soup.select_one(selector)
            if main_content:
                # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ—É—Ç–±–æ–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
                all_links = main_content.find_all('a', href=True)
                soccer_links = []
                
                for link in all_links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    if self.is_soccer_news_link(href) and len(text) > 10:
                        soccer_links.append(link)
                
                if len(soccer_links) >= 5:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ñ—É—Ç–±–æ–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(soccer_links)} —Ñ—É—Ç–±–æ–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –≤ {selector}")
                    # –°–æ–∑–¥–∞–µ–º –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
                    virtual_container = soup.new_tag('div')
                    for link in soccer_links[:10]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10
                        virtual_container.append(link.parent or link)
                    return virtual_container
        
        print("‚ùå –ë–ª–æ–∫ —Å —Ñ—É—Ç–±–æ–ª—å–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None
    
    def is_soccer_news_link(self, href):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ —Ñ—É—Ç–±–æ–ª—å–Ω–æ–π –Ω–æ–≤–æ—Å—Ç—å—é ESPN"""
        if not href:
            return False
        
        # ESPN —Ñ—É—Ç–±–æ–ª—å–Ω—ã–µ URL –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        soccer_patterns = [
            r'/soccer/',
            r'/football/',  # –ï–≤—Ä–æ–ø–µ–π—Å–∫–∏–π —Ñ—É—Ç–±–æ–ª –Ω–∞ ESPN
            r'/story/_/id/\d+',  # –û–±—â–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω —Å—Ç–∞—Ç–µ–π ESPN
            r'/news/story/',
            r'espn.com/soccer',
            r'premier-league',
            r'champions-league',
            r'la-liga',
            r'serie-a',
            r'bundesliga',
            r'ligue-1',
            r'mls',
            r'uefa',
            r'fifa'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        exclude_patterns = [
            r'/video/',
            r'/watch/',
            r'/fantasy/',
            r'/betting/',
            r'/schedule/',
            r'/standings/',
            r'/stats/',
            r'/teams/',
            r'/players/',
            r'/scores/',
            r'#',
            r'javascript:',
            r'mailto:',
            r'/podcast/'
        ]
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        if any(re.search(pattern, href, re.I) for pattern in exclude_patterns):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ—É—Ç–±–æ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        return any(re.search(pattern, href, re.I) for pattern in soccer_patterns)
    
    def extract_news_from_section(self, section, since_time: Optional[datetime] = None):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏"""
        if not section:
            return []
        
        news_links = []
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ —Å–µ–∫—Ü–∏–∏
        all_links = section.find_all('a', href=True)
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –≤ —Å–µ–∫—Ü–∏–∏")
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ñ—É—Ç–±–æ–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            if self.is_soccer_news_link(href) and len(text) > 15:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                if href.startswith('/'):
                    full_url = f"https://www.espn.com{href}"
                elif href.startswith('http'):
                    full_url = href
                else:
                    full_url = urljoin(self.base_url, href)
                
                news_links.append({
                    'title': text,
                    'url': full_url,
                    'href': href
                })
                
                print(f"üì∞ –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {text[:50]}...")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        seen_urls = set()
        unique_news = []
        
        for news in news_links:
            if news['url'] not in seen_urls:
                unique_news.append(news)
                seen_urls.add(news['url'])
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ (–ø—Ä–∏ —É–∫–∞–∑–∞–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ - –≤—Å–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏, –∏–Ω–∞—á–µ - –ø–µ—Ä–≤—ã–µ 8)
        if since_time:
            return unique_news
        else:
            return unique_news[:8]
    
    def translate_to_ukrainian(self, text: str, context: str = "—Ñ—É—Ç–±–æ–ª—å–Ω–∞ –Ω–æ–≤–∏–Ω–∞") -> str:
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π —Å –ø–æ–º–æ—â—å—é AI"""
        if not self.model or not text:
            return text
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        clean_text = re.sub(r'\s+', ' ', text).strip()
        
        if len(clean_text) < 5:
            return text
        
        prompt = f"""–ü–µ—Ä–µ–∫–ª–∞–¥–∏ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ç–µ–∫—Å—Ç –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É. –¶–µ {context}.

–í–ê–ñ–õ–ò–í–Ü –ü–†–ê–í–ò–õ–ê:
1. –ó–±–µ—Ä—ñ–≥–∞–π —Ñ—É—Ç–±–æ–ª—å–Ω—É —Ç–µ—Ä–º—ñ–Ω–æ–ª–æ–≥—ñ—é (–Ω–∞–∑–≤–∏ –∫–æ–º–∞–Ω–¥, –ª—ñ–≥, —Ç—É—Ä–Ω—ñ—Ä—ñ–≤)
2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –ø—Ä–∏—Ä–æ–¥–Ω—É —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É
3. –ê–¥–∞–ø—Ç—É–π –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ–≥–æ —á–∏—Ç–∞—á–∞
4. –ó–±–µ—Ä—ñ–≥–∞–π –µ–º–æ—Ü—ñ–π–Ω–∏–π —Ç–æ–Ω –æ—Ä–∏–≥—ñ–Ω–∞–ª—É
5. –ù–∞–∑–≤–∏ –∫–æ–º–∞–Ω–¥ –∑–∞–ª–∏—à–∞–π –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∑–∞–≥–∞–ª—å–Ω–æ–ø—Ä–∏–π–Ω—è—Ç—ñ —É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ –Ω–∞–∑–≤–∏
6. –ù–ï –¥–æ–¥–∞–≤–∞–π –∑–∞–π–≤–∏—Ö –ø–æ—è—Å–Ω–µ–Ω—å - —Ç—ñ–ª—å–∫–∏ –ø–µ—Ä–µ–∫–ª–∞–¥

–¢–ï–ö–°–¢ –î–õ–Ø –ü–ï–†–ï–ö–õ–ê–î–£:
{clean_text}

–ü–ï–†–ï–ö–õ–ê–î:"""

        try:
            response = self.model.generate_content(prompt)
            translated = response.text.strip()
            
            # –û—á–∏—â–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥ –æ—Ç –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            translated = re.sub(r'^(–ü–ï–†–ï–ö–õ–ê–î:|–ü–µ—Ä–µ–∫–ª–∞–¥:)\s*', '', translated)
            translated = translated.strip()
            
            print(f"üåê –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ: {clean_text[:30]}... ‚Üí {translated[:30]}...")
            return translated
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
            return text
    
    def parse_espn_date(self, date_text: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É ESPN (–æ–±—ã—á–Ω–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ '4h ago', 'Yesterday', etc.)"""
        try:
            if not date_text:
                return None
            
            current_time = datetime.now(KIEV_TZ)
            date_text = date_text.lower().strip()
            
            # "X hours ago"
            hours_match = re.search(r'(\d+)h\s*ago', date_text)
            if hours_match:
                hours = int(hours_match.group(1))
                return current_time - timedelta(hours=hours)
            
            # "X minutes ago"
            minutes_match = re.search(r'(\d+)m\s*ago', date_text)
            if minutes_match:
                minutes = int(minutes_match.group(1))
                return current_time - timedelta(minutes=minutes)
            
            # "X days ago"
            days_match = re.search(r'(\d+)d\s*ago', date_text)
            if days_match:
                days = int(days_match.group(1))
                return current_time - timedelta(days=days)
            
            # "Yesterday"
            if 'yesterday' in date_text:
                return current_time - timedelta(days=1)
            
            # "Today"
            if 'today' in date_text:
                return current_time.replace(hour=12, minute=0, second=0, microsecond=0)
            
            # –ï—Å–ª–∏ –Ω–µ —Å–º–æ–≥–ª–∏ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã ESPN '{date_text}': {e}")
            return None
    
    def estimate_article_publish_time(self, soup, url: str) -> Optional[datetime]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏ ESPN"""
        try:
            print(f"üïí –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ ESPN: {url}")
            
            # ESPN —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –≤—Ä–µ–º–µ–Ω–∏
            time_selectors = [
                'time[datetime]',
                '.timestamp',
                '.article-meta time',
                '.byline time',
                '[data-date]',
                '.publish-date',
                '.article-date',
                '[class*="time"]',
                '[class*="date"]'
            ]
            
            for selector in time_selectors:
                time_elem = soup.select_one(selector)
                if time_elem:
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å datetime –∞—Ç—Ä–∏–±—É—Ç
                    datetime_attr = time_elem.get('datetime') or time_elem.get('data-date')
                    if datetime_attr:
                        try:
                            parsed_date = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00'))
                            parsed_date_kiev = parsed_date.astimezone(KIEV_TZ)
                            print(f"‚úÖ –í—Ä–µ–º—è –∏–∑ –∞—Ç—Ä–∏–±—É—Ç–∞: {parsed_date_kiev}")
                            return parsed_date_kiev
                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ datetime –∞—Ç—Ä–∏–±—É—Ç–∞: {e}")
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –≤—Ä–µ–º–µ–Ω–∏
                    time_text = time_elem.get_text(strip=True)
                    if time_text:
                        parsed_time = self.parse_espn_date(time_text)
                        if parsed_time:
                            print(f"‚úÖ –í—Ä–µ–º—è –∏–∑ —Ç–µ–∫—Å—Ç–∞: {parsed_time}")
                            return parsed_time
            
            # –ò—â–µ–º –≤—Ä–µ–º—è –≤ –º–µ—Ç–∞-—Ç–µ–≥–∞—Ö
            meta_selectors = [
                'meta[property="article:published_time"]',
                'meta[name="publish_date"]',
                'meta[property="og:published_time"]'
            ]
            
            for selector in meta_selectors:
                meta_tag = soup.select_one(selector)
                if meta_tag:
                    content = meta_tag.get('content', '')
                    if content:
                        try:
                            parsed_date = datetime.fromisoformat(content.replace('Z', '+00:00'))
                            parsed_date_kiev = parsed_date.astimezone(KIEV_TZ)
                            print(f"‚úÖ –í—Ä–µ–º—è –∏–∑ –º–µ—Ç–∞-—Ç–µ–≥–∞: {parsed_date_kiev}")
                            return parsed_date_kiev
                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –º–µ—Ç–∞-–¥–∞—Ç—ã: {e}")
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤—Ä–µ–º—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None (–Ω–µ —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è!)
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ ESPN")
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ ESPN: {e}")
            return None
    
    def extract_article_content(self, soup):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ ESPN"""
        content_selectors = [
            '.article-body',
            '.story-body',
            '.RichTextStoryBody',
            '.Story__Body',
            '.ArticleBody',
            '[data-module="ArticleBody"]',
            '.story-text',
            '.article-content'
        ]
        
        main_content = ""
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # –£–±–∏—Ä–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                for unwanted in content_elem.find_all(['script', 'style', 'iframe', 'aside', '[class*="ad"]']):
                    unwanted.decompose()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                paragraphs = content_elem.find_all('p')
                if paragraphs:
                    main_content = '\n'.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
                    break
        
        # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—â–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        if not main_content:
            all_paragraphs = soup.find_all('p')
            meaningful_paragraphs = []
            
            for p in all_paragraphs:
                text = p.get_text(strip=True)
                # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                if (len(text) > 30 and 
                    not any(skip in text.lower() for skip in ['cookie', 'advertisement', 'subscribe', 'follow us'])):
                    meaningful_paragraphs.append(text)
            
            main_content = '\n'.join(meaningful_paragraphs[:5])  # –ü–µ—Ä–≤—ã–µ 5 –∑–Ω–∞—á–∏–º—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        
        print(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(main_content)} —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ESPN")
        return main_content
    
    def extract_main_image(self, soup, base_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ ESPN"""
        image_selectors = [
            'meta[property="og:image"]',
            '.article-figure img',
            '.story-header img',
            '.media-wrapper img',
            '.hero-image img',
            '.featured-image img',
            'figure img:first-of-type',
            '.article-body img:first-of-type'
        ]
        
        for selector in image_selectors:
            if 'meta' in selector:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('content', '')
            else:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('src', '') or img_elem.get('data-src', '') or img_elem.get('data-original', '')
            
            if image_url:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url
                elif image_url.startswith('/'):
                    image_url = 'https://www.espn.com' + image_url
                elif not image_url.startswith('http'):
                    image_url = urljoin(base_url, image_url)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –º–∞–ª–µ–Ω—å–∫–∞—è –∏–∫–æ–Ω–∫–∞
                if not any(small in image_url.lower() for small in ['icon', 'logo', 'thumb', 'avatar', '/16x', '/32x']):
                    return image_url
        
        return ''
    
    def get_full_article_data(self, news_item, since_time: Optional[datetime] = None):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ ESPN —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º"""
        url = news_item['url']
        soup = self.get_page_content(url)
        
        if not soup:
            return None
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            publish_time = self.estimate_article_publish_time(soup, url)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
            if since_time and publish_time:
                if publish_time <= since_time:
                    print(f"‚è∞ ESPN —Å—Ç–∞—Ç—å—è –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ {publish_time.strftime('%H:%M %d.%m')} - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    return None
                else:
                    print(f"‚úÖ ESPN —Å—Ç–∞—Ç—å—è –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ {publish_time.strftime('%H:%M %d.%m')} - –Ω–æ–≤–∞—è!")
            elif since_time and not publish_time:
                print(f"‚ö†Ô∏è –í—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ ESPN –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ - —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç—å—é –Ω–æ–≤–æ–π")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            original_content = self.extract_article_content(soup)
            original_title = news_item['title']
            
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç
            print(f"üåê –ü–µ—Ä–µ–≤–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫...")
            translated_title = self.translate_to_ukrainian(original_title, "–∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ñ—É—Ç–±–æ–ª—å–Ω–æ—ó –Ω–æ–≤–∏–Ω–∏")
            
            print(f"üåê –ü–µ—Ä–µ–≤–æ–¥–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç...")
            translated_content = self.translate_to_ukrainian(original_content, "—Ç–µ–∫—Å—Ç —Ñ—É—Ç–±–æ–ª—å–Ω–æ—ó –Ω–æ–≤–∏–Ω–∏")
            
            # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–º
            summary = self.create_ukrainian_summary(translated_content, translated_title)
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_url = self.extract_main_image(soup, url)
            
            return {
                'title': translated_title,
                'original_title': original_title,
                'url': url,
                'content': translated_content,
                'original_content': original_content,
                'summary': summary,
                'image_url': image_url,
                'publish_time': publish_time,
                'source': 'ESPN Soccer'
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ ESPN {url}: {e}")
            return None
    
    def create_ukrainian_summary(self, content, title):
        """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–º —è–∑—ã–∫–µ"""
        if not content:
            return title
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', content)
        meaningful_sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        
        if meaningful_sentences:
            summary = '. '.join(meaningful_sentences[:2])
            return summary + '.' if not summary.endswith('.') else summary
        
        return content[:200] + '...' if len(content) > 200 else content
    
    def get_latest_news(self, since_time: Optional[datetime] = None):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ - –ø–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ ESPN Soccer —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º"""
        print("üîç –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É ESPN Soccer...")
        
        if since_time:
            print(f"üïí –ò—â–µ–º ESPN –Ω–æ–≤–æ—Å—Ç–∏ —Å {since_time.strftime('%H:%M %d.%m.%Y')}")
        
        soup = self.get_page_content(self.base_url)
        if not soup:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É ESPN Soccer")
            return []
        
        print("üéØ –ò—â–µ–º –±–ª–æ–∫ 'Top Headlines' –Ω–∞ ESPN...")
        headlines_section = self.find_top_headlines_section(soup)
        
        if not headlines_section:
            print("‚ùå –ë–ª–æ–∫ 'Top Headlines' –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ ESPN")
            return []
        
        print("üì∞ –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞ ESPN...")
        news_items = self.extract_news_from_section(headlines_section, since_time)
        
        if not news_items:
            print("‚ùå –ù–æ–≤–æ—Å—Ç–∏ ESPN –≤ –±–ª–æ–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π ESPN")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –∫–∞–∂–¥—É—é –Ω–æ–≤–æ—Å—Ç—å
        full_articles = []
        
        for i, news_item in enumerate(news_items, 1):
            print(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º ESPN –Ω–æ–≤–æ—Å—Ç—å {i}/{len(news_items)}: {news_item['title'][:50]}...")
            
            article_data = self.get_full_article_data(news_item, since_time)
            
            if since_time and article_data is None:
                print(f"üõë –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è ESPN –Ω–æ–≤–æ—Å—Ç—å, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É")
                break
            
            if article_data:
                full_articles.append(article_data)
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ ESPN
            time.sleep(2)
        
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(full_articles)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π ESPN —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º")
        return full_articles


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π
def get_espn_news(since_time: Optional[datetime] = None):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π ESPN Soccer —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º"""
    parser = ESPNSoccerParser()
    articles = parser.get_latest_news(since_time)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç, –æ–∂–∏–¥–∞–µ–º—ã–π –æ—Å–Ω–æ–≤–Ω—ã–º –∫–æ–¥–æ–º
    result = []
    for article in articles:
        result.append({
            'title': article['title'],
            'link': article['url'],
            'url': article['url'],
            'summary': article['summary'],
            'image_url': article['image_url'],
            'content': article['content'],
            'publish_time': article.get('publish_time'),
            'source': 'ESPN Soccer',
            'original_title': article.get('original_title', ''),
            'original_content': article.get('original_content', '')
        })
    
    return result


def test_espn_parser():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ESPN –ø–∞—Ä—Å–µ—Ä–∞"""
    print("üß™ –¢–ï–°–¢–ò–†–£–ï–ú ESPN SOCCER PARSER")
    print("=" * 60)
    
    parser = ESPNSoccerParser()
    
    # –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π
    print("\nüìã –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π ESPN")
    articles = parser.get_latest_news()
    
    if articles:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π ESPN")
        for i, article in enumerate(articles, 1):
            publish_time = article.get('publish_time')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            print(f"   üì∞ {i}. {article['title'][:60]}... ({time_str})")
            print(f"      üåê –û—Ä–∏–≥–∏–Ω–∞–ª: {article.get('original_title', '')[:60]}...")
            print(f"      üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {'‚úÖ' if article.get('image_url') else '‚ùå'}")
    else:
        print("‚ùå –ù–æ–≤–æ—Å—Ç–∏ ESPN –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    # –¢–µ—Å—Ç 2: –ü–µ—Ä–µ–≤–æ–¥
    if articles:
        print(f"\nüìã –¢–µ—Å—Ç 2: –ö–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–∞")
        test_article = articles[0]
        print(f"üî§ –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: {test_article.get('original_title', '')}")
        print(f"üåê –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: {test_article['title']}")
        
        original_summary = test_article.get('original_content', '')[:200]
        translated_summary = test_article.get('content', '')[:200]
        print(f"üî§ –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: {original_summary}...")
        print(f"üåê –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {translated_summary}...")


if __name__ == "__main__":
    test_espn_parser()