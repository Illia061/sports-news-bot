import re
import os
import requests
import time
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import google.generativeai as genai

from db import cursor

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_AVAILABLE = False
model = None


def init_gemini():
    global GEMINI_AVAILABLE, model
    if not GEMINI_API_KEY:
        print("‚ö†Ô∏è GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        return
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel("gemini-2.5-flash")
        GEMINI_AVAILABLE = True
        print("‚úÖ Gemini –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Gemini: {e}")


def has_gemini_key() -> bool:
    if not GEMINI_AVAILABLE:
        init_gemini()
    return GEMINI_AVAILABLE


class AIContentSimilarityChecker:
    def __init__(self, similarity_threshold: float = 0.75):
        self.similarity_threshold = similarity_threshold
        if not has_gemini_key():
            print("‚ö†Ô∏è AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –±–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞")

    def clean_text_for_ai(self, text: str) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
        if not text:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏
        text = re.sub(r'<[^>]+>', '', text)
        
        # –£–±–∏—Ä–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —ç–º–æ–¥–∑–∏, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –≤–∞–∂–Ω—ã–µ
        text = re.sub(r'[üì∞üìäüî•üí™üéØ‚≠ê]', '', text)  # –£–±–∏—Ä–∞–µ–º –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ
        # –û—Å—Ç–∞–≤–ª—è–µ–º –≤–∞–∂–Ω—ã–µ: ‚öΩüèÜü•Ö‚úÖ‚ùåüåçüö´üëë
        
        # –ù–ï —É–±–∏—Ä–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ - –æ–Ω–∏ –ø–æ–º–æ–≥–∞—é—Ç —Ä–∞–∑–ª–∏—á–∞—Ç—å —Å—Ç–∞—Ç—å–∏
        # text = re.sub(r'(ESPN Soccer|Football\.ua|OneFootball)', '', text)  # –ó–ê–ö–û–ú–ú–ï–ù–¢–ò–†–û–í–ê–ù–û
        
        # –£–±–∏—Ä–∞–µ–º —Ö—ç—à—Ç–µ–≥–∏ –≤ –∫–æ–Ω—Ü–µ, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞
        text = re.sub(r'\s*#\w+\s*$', '', text)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def ai_compare_texts(self, new_text: str, existing_texts: List[str]) -> Dict[str, Any]:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è AI-–ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º."""
        if not has_gemini_key() or not model:
            return {"ai_available": False, "similarities": [], "is_duplicate": False}

        clean_new_text = self.clean_text_for_ai(new_text)
        clean_existing_texts = [self.clean_text_for_ai(text) for text in existing_texts]

        if not clean_new_text or not any(clean_existing_texts):
            return {"ai_available": True, "similarities": [], "is_duplicate": False}

        # –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–†–û–ú–ü–¢ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏
        prompt = f"""–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ –∞–Ω–∞–ª—ñ–∑—É —Ñ—É—Ç–±–æ–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω. –ü–æ—Ä—ñ–≤–Ω—è–π –ù–û–í–£ –Ω–æ–≤–∏–Ω—É –∑ –Ü–°–ù–£–Æ–ß–ò–ú–ò —Ç–∞ –≤–∏–∑–Ω–∞—á, —á–∏ —î –≤–æ–Ω–∞ –¥—É–±–ª—ñ–∫–∞—Ç–æ–º.

–ö–†–ò–¢–ï–†–Ü–á –î–£–ë–õ–Ü–ö–ê–¢–Ü–í:
- –¢–∞ —Å–∞–º–∞ –ø–æ–¥—ñ—è (–º–∞—Ç—á, —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä, –Ω–∞–≥–æ—Ä–æ–¥–∞) 
- –¢–æ–π —Å–∞–º–∏–π –≥—Ä–∞–≤–µ—Ü—å/–∫–æ–º–∞–Ω–¥–∞ –≤ —Ç—ñ–π —Å–∞–º—ñ–π —Å–∏—Ç—É–∞—Ü—ñ—ó
- –¢–æ–π —Å–∞–º–∏–π —á–∞—Å–æ–≤–∏–π –ø–µ—Ä—ñ–æ–¥ –ø–æ–¥—ñ—ó

–ù–ï –í–í–ê–ñ–ê–ô –î–£–ë–õ–Ü–ö–ê–¢–ê–ú–ò:
- –†—ñ–∑–Ω—ñ –∫–æ–º–∞–Ω–¥–∏/–≥—Ä–∞–≤—Ü—ñ (–Ω–∞–≤—ñ—Ç—å –≤ —Å—Ö–æ–∂–∏—Ö —Å–∏—Ç—É–∞—Ü—ñ—è—Ö)
- –†—ñ–∑–Ω—ñ –º–∞—Ç—á—ñ/—Ç—É—Ä–Ω—ñ—Ä–∏
- –†—ñ–∑–Ω—ñ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∏/–Ω–∞–≥–æ—Ä–æ–¥–∏
- –ó–∞–≥–∞–ª—å–Ω—ñ —Ñ—É—Ç–±–æ–ª—å–Ω—ñ —Ç–µ—Ä–º—ñ–Ω–∏ –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏

–ù–û–í–ê –ù–û–í–ò–ù–ê:
{clean_new_text}

–Ü–°–ù–£–Æ–ß–Ü –ù–û–í–ò–ù–ò:
{' | '.join([f"[{i+1}] {text}" for i, text in enumerate(clean_existing_texts)])}

–î–∞–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É —Ñ–æ—Ä–º–∞—Ç—ñ:
–î–£–ë–õ–Ü–ö–ê–¢: –¢–ê–ö/–ù–Ü
–ü–û–Ø–°–ù–ï–ù–ù–Ø: [–∫–æ—Ä–æ—Ç–∫–µ –æ–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è]
–°–•–û–ñ–Ü–°–¢–¨ –ó: [–Ω–æ–º–µ—Ä —ñ—Å–Ω—É—é—á–æ—ó –Ω–æ–≤–∏–Ω–∏ –∞–±–æ "–ñ–û–î–ù–ê"]"""

        try:
            response = model.generate_content(prompt)
            ai_response = response.text.strip()
            
            # –ü–∞—Ä—Å–∏–º –≤—ñ–¥–ø–æ–≤—ñ–¥—å AI
            is_duplicate = False
            explanation = ""
            similar_to = "–ñ–û–î–ù–ê"
            
            lines = ai_response.split('\n')
            for line in lines:
                if line.startswith('–î–£–ë–õ–Ü–ö–ê–¢:'):
                    is_duplicate = '–¢–ê–ö' in line.upper()
                elif line.startswith('–ü–û–Ø–°–ù–ï–ù–ù–Ø:'):
                    explanation = line.replace('–ü–û–Ø–°–ù–ï–ù–ù–Ø:', '').strip()
                elif line.startswith('–°–•–û–ñ–Ü–°–¢–¨ –ó:'):
                    similar_to = line.replace('–°–•–û–ñ–Ü–°–¢–¨ –ó:', '').strip()
            
            return {
                "ai_available": True,
                "ai_response": ai_response,
                "explanation": explanation,
                "similar_to": similar_to,
                "similarities": [],
                "is_duplicate": is_duplicate,
            }
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ AI –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {"ai_available": False, "error": str(e), "similarities": [], "is_duplicate": False}

    def fallback_similarity_check(self, text1: str, text2: str) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è fallback-–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å —É—á–µ—Ç–æ–º —Ñ—É—Ç–±–æ–ª—å–Ω–æ–π —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏."""
        if not text1 or not text2:
            return 0.0
        
        clean_text1 = self.clean_text_for_ai(text1).lower()
        clean_text2 = self.clean_text_for_ai(text2).lower()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        def extract_key_entities(text):
            # –ö–æ–º–∞–Ω–¥—ã (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫)
            teams = re.findall(r'\b(?:—Ä–µ–∞–ª|–±–∞—Ä—Å–µ–ª–æ–Ω–∞|–ª—ñ–≤–µ—Ä–ø—É–ª—å|–º–∞–Ω—á–µ—Å—Ç–µ—Ä|–∞—Ä—Å–µ–Ω–∞–ª|—á–µ–ª—Å—ñ|—Ä–µ–π–Ω–¥–∂–µ—Ä—Å|–Ω–∞–ø–æ–ª—ñ|–≥–µ–Ω–∫|–∞—Å—Ç–æ–Ω –≤—ñ–ª–ª–∞|–∫–ª—É–± –±—Ä—é–≥–≥–µ|–±–∞–≤–∞—Ä—ñ—è|—é–≤–µ–Ω—Ç—É—Å|–ø—Å–∂|–∞—Ç–ª–µ—Ç—ñ–∫–æ|—Å–µ–≤—ñ–ª—å—è|–≤–∞–ª–µ–Ω—Å—ñ—è|—ñ–Ω—Ç–µ—Ä|–º—ñ–ª–∞–Ω|—Ä–æ–º–∞|–ª–∞—Ü—ñ–æ|–∞—Ç–∞–ª–∞–Ω—Ç–∞|—Ñ—ñ–æ—Ä–µ–Ω—Ç–∏–Ω–∞|—Ä–µ–∞–ª –º–∞–¥—Ä–∏–¥|–±–∞—Ä—Å–µ–ª–æ–Ω–∞|–º–∞–Ω—á–µ—Å—Ç–µ—Ä —é–Ω–∞–π—Ç–µ–¥|–º–∞–Ω—á–µ—Å—Ç–µ—Ä —Å—ñ—Ç—ñ|—Ç–æ—Ç—Ç–µ–Ω–≥–µ–º|–Ω—å—é–∫–∞—Å–ª|–≤–µ—Å—Ç –≥–µ–º|–ª–µ—Å—Ç–µ—Ä|–µ–≤–µ—Ä—Ç–æ–Ω|—Å–∞—É—Ç–≥–µ–º–ø—Ç–æ–Ω|–±–µ—Ä–Ω–ª—ñ|—Ñ—É–ª–≥–µ–º|–≤–æ–ª—å–≤–∑|–±—Ä–∞–π—Ç–æ–Ω|–∫—Ä–∏—Å—Ç–∞–ª –ø–∞–ª–∞—Å|–∞–π–ø—Å–≤—ñ—á|–±–æ—Ä–Ω–º—É—Ç)\b', text)
            
            # –ò–≥—Ä–æ–∫–∏ (–∏–º–µ–Ω–∞ —Å —Ñ–∞–º–∏–ª–∏—è–º–∏ –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–º/–∞–Ω–≥–ª–∏–π—Å–∫–æ–º)
            players_ua = re.findall(r'\b[–ê-–Ø–Ü–á–Ñ][–∞-—è—ñ—ó—î“ë]+\s+[–ê-–Ø–Ü–á–Ñ][–∞-—è—ñ—ó—î“ë]+\b', text)
            players_en = re.findall(r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b', text)
            players = players_ua + players_en
            
            # –¢—É—Ä–Ω–∏—Ä—ã/—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è
            competitions = re.findall(r'\b(?:–ª—ñ–≥–∞ —á–µ–º–ø—ñ–æ–Ω—ñ–≤|–ø—Ä–µ–º\'—î—Ä-–ª—ñ–≥–∞|–ª–∞ –ª—ñ–≥–∞|—Å–µ—Ä—ñ—è –∞|–±—É–Ω–¥–µ—Å–ª—ñ–≥–∞|pfa|uefa|ucl|champions league|premier league|la liga|serie a|bundesliga|europa league|conference league|fa cup|carabao cup|copa del rey|coppa italia|dfb pokal|–∫—É–±–æ–∫ –∞–Ω–≥–ª—ñ—ó|–∫—É–±–æ–∫ –Ω—ñ–º–µ—á—á–∏–Ω–∏|–∫—É–±–æ–∫ —ñ—Ç–∞–ª—ñ—ó|–∫—É–±–æ–∫ —ñ—Å–ø–∞–Ω—ñ—ó)\b', text)
            
            # –ß–∏—Å–ª–∞ –∏ —Å—É–º–º—ã
            numbers = re.findall(r'\b\d+[\d\s]*(?:–º—ñ–ª—å–π–æ–Ω—ñ–≤?|–≥–æ–ª—ñ–≤?|–∞—Å–∏—Å—Ç—ñ–≤?|—Ä–æ–∫—ñ–≤?|—Ö–≤–∏–ª–∏–Ω?|million|goal|assist|year|minute)\b', text)
            
            return {
                'teams': set([team.lower() for team in teams]),
                'players': set([player.lower() for player in players]), 
                'competitions': set([comp.lower() for comp in competitions]),
                'numbers': set([num.lower() for num in numbers])
            }
        
        entities1 = extract_key_entities(clean_text1)
        entities2 = extract_key_entities(clean_text2)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å—É—â–Ω–æ—Å—Ç—è–º - –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–∞
        teams_overlap = len(entities1['teams'].intersection(entities2['teams']))
        players_overlap = len(entities1['players'].intersection(entities2['players']))
        competitions_overlap = len(entities1['competitions'].intersection(entities2['competitions']))
        
        # –ï—Å–ª–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç –∫–æ–º–∞–Ω–¥—ã –ò –∏–≥—Ä–æ–∫–∏ –ò —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è - –≤–µ—Ä–æ—è—Ç–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç
        if teams_overlap > 0 and players_overlap > 0 and competitions_overlap > 0:
            return 0.9
        
        # –ï—Å–ª–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç 2 –∏–∑ 3 –∫–ª—é—á–µ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π - —Å—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫
        if (teams_overlap > 0 and players_overlap > 0) or \
           (teams_overlap > 0 and competitions_overlap > 0) or \
           (players_overlap > 0 and competitions_overlap > 0):
            return 0.7
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º (—Å –ø–æ–Ω–∏–∂–µ–Ω–Ω—ã–º –≤–µ—Å–æ–º)
        words1 = set(clean_text1.split())
        words2 = set(clean_text2.split())
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —Ñ—É—Ç–±–æ–ª–∞
        stop_words = {
            '–≤','–Ω–∞','–∑–∞','–¥–æ','–≤—ñ–¥','–¥–ª—è','–ø—Ä–æ','–ø—ñ–¥','–Ω–∞–¥','–ø—Ä–∏','–∑','—É','—ñ','—Ç–∞','–∞–±–æ','–∞–ª–µ',
            '—Ñ—É—Ç–±–æ–ª','—Ñ—É—Ç–±–æ–ª—å–Ω–∏–π','–≥—Ä–∞','–º–∞—Ç—á','–∫–æ–º–∞–Ω–¥–∞','–≥—Ä–∞–≤–µ—Ü—å','—Ç—Ä–µ–Ω–µ—Ä','–∫–ª—É–±','—Å–µ–∑–æ–Ω',
            '–≥–æ–ª','–º\'—è—á','–ø–æ–ª–µ','—Å—Ç–∞–¥—ñ–æ–Ω','–≤–±–æ–ª—ñ–≤–∞–ª—å–Ω–∏–∫–∏','–ø–µ—Ä–µ–º–æ–≥–∞','–ø–æ—Ä–∞–∑–∫–∞', '–Ω–æ–≤–∏–Ω–∏', '—Å–ø–æ—Ä—Ç',
            'football', 'soccer', 'game', 'match', 'team', 'player', 'coach', 'club', 'season',
            'goal', 'ball', 'field', 'stadium', 'fans', 'win', 'loss', 'news', 'sport',
            'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'
        }
        
        words1 = {w for w in words1 if len(w) > 2 and w not in stop_words}
        words2 = {w for w in words2 if len(w) > 2 and w not in stop_words}
        
        if not words1 or not words2:
            return 0.0
        
        common_words = words1.intersection(words2)
        base_similarity = len(common_words) / max(len(words1), len(words2))
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —Å—Ö–æ–∂–µ—Å—Ç—å –∏–∑ entity-–∞–Ω–∞–ª–∏–∑–∞ –∏ word-–∞–Ω–∞–ª–∏–∑–∞
        return min(0.6, base_similarity)  # –ú–∞–∫—Å–∏–º—É–º 0.6 –¥–ª—è –æ–±—ã—á–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å–ª–æ–≤


class TelegramChannelChecker:
    def __init__(self):
        self.bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
        self.channel_id = os.getenv('TELEGRAM_CHANNEL_ID')

    def get_recent_posts(self, limit: int = 5, since_time: Optional[datetime] = None) -> List[Dict[str, Any]]:
        if not self.bot_token or not self.channel_id:
            return []
        try:
            url = f"https://api.telegram.org/bot{self.bot_token}/getUpdates"
            response = requests.get(url, timeout=30)
            result = response.json()
            if not result.get('ok'):
                return []
            channel_posts = []
            for update in result.get('result', []):
                if 'channel_post' in update:
                    post = update['channel_post']
                    if str(post.get('chat', {}).get('id')) == str(self.channel_id):
                        channel_posts.append(post)
            channel_posts.sort(key=lambda x: x.get('date', 0), reverse=True)
            recent_posts = channel_posts[:limit]
            formatted_posts = []
            for post in recent_posts:
                text = post.get('text') or post.get('caption', '') or ''
                post_date = datetime.fromtimestamp(post.get('date', 0))
                if text and (not since_time or post_date >= since_time):
                    formatted_posts.append({
                        'text': text,
                        'date': post_date,
                        'message_id': post.get('message_id')
                    })
            return formatted_posts
        except Exception:
            return []


def get_recent_posts_from_db():
    query = "SELECT title, post_text, posted_at FROM posted_news ORDER BY posted_at DESC LIMIT 4"
    cursor.execute(query)
    rows = cursor.fetchall()
    
    posts = []
    for row in rows:
        title, post_text, posted_at = row
        text = post_text or title
        if text:
            try:
                dt = datetime.fromisoformat(posted_at)
            except Exception:
                dt = None
            posts.append({'text': text, 'date': dt})
    
    print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(posts)} –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–æ–≤ –∏–∑ –±–∞–∑—ã")
    return posts


def check_content_similarity(new_article: Dict[str, Any], threshold: float = 0.75, since_time: Optional[datetime] = None) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    title = new_article.get('title', '')
    print(f"üîç –î–ï–¢–ê–õ–¨–ù–ê–Ø AI –ø—Ä–æ–≤–µ—Ä–∫–∞: {title[:60]}...")
    
    ai_checker = AIContentSimilarityChecker(threshold)
    channel_checker = TelegramChannelChecker()
    
    new_text = new_article.get('post_text') or new_article.get('title', '')
    if not new_text:
        return False
    
    db_posts = get_recent_posts_from_db()
    channel_posts = channel_checker.get_recent_posts(limit=10, since_time=since_time)
    recent_posts = db_posts + channel_posts
    
    if not recent_posts:
        print("   ‚úÖ –ù–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        return False
    
    existing_texts = [post['text'] for post in recent_posts]
    print(f"   üìä –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å {len(existing_texts)} –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –ø–æ—Å—Ç–∞–º–∏")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º
    print(f"   üìù –ù–û–í–´–ô –¢–ï–ö–°–¢: {new_text[:100]}...")
    for i, text in enumerate(existing_texts[:3], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
        print(f"   üìù –°–£–©–ï–°–¢–í–£–Æ–©–ò–ô {i}: {text[:100]}...")
    
    if has_gemini_key():
        print("   ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ–º AI-–∞–Ω–∞–ª–∏–∑...")
        ai_result = ai_checker.ai_compare_texts(new_text, existing_texts)
        
        if ai_result.get("ai_available"):
            is_duplicate = ai_result.get("is_duplicate", False)
            explanation = ai_result.get("explanation", "")
            similar_to = ai_result.get("similar_to", "–ñ–û–î–ù–ê")
            
            print(f"   ü§ñ AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {'–î–£–ë–õ–ò–ö–ê–¢' if is_duplicate else '–£–ù–ò–ö–ê–õ–¨–ù–ê–Ø'}")
            print(f"   üìÑ –ü–æ—è—Å–Ω–µ–Ω–∏–µ: {explanation}")
            print(f"   üîó –ü–æ—Ö–æ–∂–∞ –Ω–∞: {similar_to}")
            
            return is_duplicate
    
    print("   üîÑ AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback...")
    max_similarity = 0.0
    most_similar_text = ""
    
    for i, existing_text in enumerate(existing_texts, 1):
        similarity = ai_checker.fallback_similarity_check(new_text, existing_text)
        if similarity > max_similarity:
            max_similarity = similarity
            most_similar_text = existing_text
        print(f"   üìä –°—Ö–æ–∂–µ—Å—Ç—å —Å #{i}: {similarity:.3f}")
    
    is_duplicate = max_similarity >= threshold
    
    print(f"   üìä –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø —Å—Ö–æ–∂–µ—Å—Ç—å: {max_similarity:.3f} (–ø–æ—Ä–æ–≥: {threshold})")
    print(f"   üîç –ù–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–π: {most_similar_text[:60]}...")
    print(f"   üéØ –†–ï–ó–£–õ–¨–¢–ê–¢: {'–î–£–ë–õ–ò–ö–ê–¢' if is_duplicate else '–£–ù–ò–ö–ê–õ–¨–ù–ê–Ø'}")
    
    return is_duplicate


def check_articles_similarity(articles: List[Dict[str, Any]], threshold: float = 0.75) -> List[Dict[str, Any]]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—å–∏ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –º–µ–∂–¥—É —Å–æ–±–æ–π (–≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø—Ä–æ–≤–µ—Ä–∫–∞).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.
    """
    if not articles:
        return []
    
    print(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º {len(articles)} —Å—Ç–∞—Ç–µ–π –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç—ã...")
    ai_checker = AIContentSimilarityChecker(threshold)
    unique_articles = []
    
    for i, article in enumerate(articles):
        article_text = article.get('post_text') or article.get('title', '')
        if not article_text:
            continue
        
        is_duplicate = False
        duplicate_explanation = ""
        
        if unique_articles:
            existing_texts = [art.get('post_text', art.get('title', '')) for art in unique_articles]
            
            if has_gemini_key():
                print(f"   üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—å—é {i+1}: {article.get('title', '')[:50]}...")
                ai_result = ai_checker.ai_compare_texts(article_text, existing_texts)
                
                if ai_result.get("ai_available"):
                    is_duplicate = ai_result.get("is_duplicate", False)
                    duplicate_explanation = ai_result.get("explanation", "")
                    similar_to = ai_result.get("similar_to", "–ñ–û–î–ù–ê")
                    
                    if is_duplicate:
                        print(f"      üö´ –î–£–ë–õ–ò–ö–ê–¢: {duplicate_explanation} (–ø–æ—Ö–æ–∂–∞ –Ω–∞ #{similar_to})")
                    else:
                        print(f"      ‚úÖ –£–ù–ò–ö–ê–õ–¨–ù–ê–Ø: {duplicate_explanation}")
            else:
                max_similarity = 0.0
                for existing_text in existing_texts:
                    similarity = ai_checker.fallback_similarity_check(article_text, existing_text)
                    max_similarity = max(max_similarity, similarity)
                
                is_duplicate = max_similarity >= threshold
                
                if is_duplicate:
                    print(f"   üö´ –î—É–±–ª–∏–∫–∞—Ç (—Å—Ö–æ–∂–µ—Å—Ç—å: {max_similarity:.3f}): {article.get('title', '')[:50]}...")
                else:
                    print(f"   ‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω–∞—è (—Å—Ö–æ–∂–µ—Å—Ç—å: {max_similarity:.3f}): {article.get('title', '')[:50]}...")
        
        if not is_duplicate:
            unique_articles.append(article)
    
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(unique_articles)}/{len(articles)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
    return unique_articles

