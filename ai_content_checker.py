import re
import os
import requests
import time
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import google.generativeai as genai

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ AI –∏–∑ ai_processor
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_AVAILABLE = False
model = None

def init_gemini():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç–∞ Gemini"""
    global GEMINI_AVAILABLE, model
    if not GEMINI_API_KEY:
        print("‚ö†Ô∏è GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        return
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel("gemini-2.5-flash")
        GEMINI_AVAILABLE = True
        print("‚úÖ Gemini –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Gemini: {e}")

def has_gemini_key() -> bool:
    if not GEMINI_AVAILABLE:
        init_gemini()
    return GEMINI_AVAILABLE

class AIContentSimilarityChecker:
    """AI-–ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Gemini"""
    
    def __init__(self, similarity_threshold: float = 0.7):
        """
        :param similarity_threshold: –ü–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ (0.0-1.0)
        """
        self.similarity_threshold = similarity_threshold
        if not has_gemini_key():
            print("‚ö†Ô∏è AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –±–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
    
    def clean_text_for_ai(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞"""
        if not text:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏
        text = re.sub(r'<[^>]+>', '', text)
        
        # –£–±–∏—Ä–∞–µ–º —Ö–µ—à—Ç–µ–≥–∏ (–Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç)
        text = re.sub(r'\s*#\w+\s*', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ —ç–º–æ–¥–∑–∏
        text = re.sub(r'[‚öΩüèÜü•Öüì∞üìäüî•üí™üëëüéØ‚≠êüö´‚úÖ‚ùåüåç]', '', text)
        
        # –£–±–∏—Ä–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        text = re.sub(r'(ESPN Soccer|Football\.ua|OneFootball)', '', text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def ai_compare_texts(self, new_text: str, existing_texts: List[str]) -> Dict[str, Any]:
        """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç AI –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤"""
        if not has_gemini_key() or not model:
            return {"ai_available": False, "similarities": [], "is_duplicate": False}
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç—ã
        clean_new_text = self.clean_text_for_ai(new_text)
        clean_existing_texts = [self.clean_text_for_ai(text) for text in existing_texts]
        
        if not clean_new_text or not any(clean_existing_texts):
            return {"ai_available": True, "similarities": [], "is_duplicate": False}
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è AI
        existing_texts_formatted = ""
        for i, text in enumerate(clean_existing_texts, 1):
            if text:  # –¢–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã
                existing_texts_formatted += f"\n–¢–µ–∫—Å—Ç {i}: {text}\n"
        
        if not existing_texts_formatted:
            return {"ai_available": True, "similarities": [], "is_duplicate": False}
        
        prompt = f"""–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ –∞–Ω–∞–ª—ñ–∑—É —Ñ—É—Ç–±–æ–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω. –¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è - –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —á–∏ —î –Ω–æ–≤–∞ –Ω–æ–≤–∏–Ω–∞ –¥—É–±–ª—ñ–∫–∞—Ç–æ–º —ñ—Å–Ω—É—é—á–∏—Ö.

–ù–û–í–ê –ù–û–í–ò–ù–ê:
{clean_new_text}

–Ü–°–ù–£–Æ–ß–Ü –ù–û–í–ò–ù–ò:{existing_texts_formatted}

–ó–ê–í–î–ê–ù–ù–Ø:
1. –ü–æ—Ä—ñ–≤–Ω—è–π –Ω–æ–≤—É –Ω–æ–≤–∏–Ω—É –∑ –∫–æ–∂–Ω–æ—é —ñ—Å–Ω—É—é—á–æ—é
2. –í–∏–∑–Ω–∞—á —Å–µ–º–∞–Ω—Ç–∏—á–Ω—É —Å—Ö–æ–∂—ñ—Å—Ç—å (–Ω–µ —Ç—ñ–ª—å–∫–∏ –¥–æ—Å–ª—ñ–≤–Ω—É)
3. –í—Ä–∞—Ö–æ–≤—É–π —â–æ –æ–¥–Ω–∞–∫–æ–≤—ñ –ø–æ–¥—ñ—ó –º–æ–∂—É—Ç—å –±—É—Ç–∏ –æ–ø–∏—Å–∞–Ω—ñ –ø–æ-—Ä—ñ–∑–Ω–æ–º—É
4. –í—Ä–∞—Ö–æ–≤—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ñ—É—Ç–±–æ–ª—É - —Ä—ñ–∑–Ω—ñ –º–∞—Ç—á—ñ, —Ä—ñ–∑–Ω—ñ –∫–æ–º–∞–Ω–¥–∏ —Ü–µ –†–Ü–ó–ù–Ü –Ω–æ–≤–∏–Ω–∏
5. –û–¥–Ω–∞–∫ –æ–¥–Ω–∞ —ñ —Ç–∞ –∂ –ø–æ–¥—ñ—è (—Ç–æ–π —Å–∞–º–∏–π –º–∞—Ç—á, —Ç–æ–π —Å–∞–º–∏–π —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä) —Ü–µ –î–£–ë–õ–Ü–ö–ê–¢

–ö–†–ò–¢–ï–†–Ü–á –î–£–ë–õ–Ü–ö–ê–¢–£:
- –¢–∞ –∂ —Ñ—É—Ç–±–æ–ª—å–Ω–∞ –ø–æ–¥—ñ—è (–º–∞—Ç—á, —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä, —Ç—Ä–∞–≤–º–∞ –≥—Ä–∞–≤—Ü—è —Ç–æ—â–æ)
- –¢—ñ –∂ –æ—Å–Ω–æ–≤–Ω—ñ —Ñ–∞–∫—Ç–∏, –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –ø–æ-—Ä—ñ–∑–Ω–æ–º—É —Å—Ñ–æ—Ä–º—É–ª—å–æ–≤–∞–Ω—ñ
- –¢–æ–π —Å–∞–º–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–∞—Ç—á—É –º—ñ–∂ —Ç–∏–º–∏ –∂ –∫–æ–º–∞–Ω–¥–∞–º–∏

–ù–ï –î–£–ë–õ–Ü–ö–ê–¢:
- –†—ñ–∑–Ω—ñ –º–∞—Ç—á—ñ (–Ω–∞–≤—ñ—Ç—å —Ç–∏—Ö —Å–∞–º–∏—Ö –∫–æ–º–∞–Ω–¥ –≤ —Ä—ñ–∑–Ω–∏–π —á–∞—Å)
- –†—ñ–∑–Ω—ñ –≥—Ä–∞–≤—Ü—ñ –∞–±–æ –∫–æ–º–∞–Ω–¥–∏
- –†—ñ–∑–Ω—ñ –ø–æ–¥—ñ—ó –Ω–∞–≤—ñ—Ç—å –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ –º–∞—Ç—á—É

–§–û–†–ú–ê–¢ –í–Ü–î–ü–û–í–Ü–î–Ü (–¥—É–∂–µ –≤–∞–∂–ª–∏–≤–æ –¥–æ—Ç—Ä–∏–º—É–≤–∞—Ç–∏—Å—è):
–ê–ù–ê–õ–Ü–ó:
–¢–µ–∫—Å—Ç 1: [—Å—Ö–æ–∂—ñ—Å—Ç—å 0-100%] - [–ø–æ—è—Å–Ω–µ–Ω–Ω—è]
–¢–µ–∫—Å—Ç 2: [—Å—Ö–æ–∂—ñ—Å—Ç—å 0-100%] - [–ø–æ—è—Å–Ω–µ–Ω–Ω—è]
–¢–µ–∫—Å—Ç 3: [—Å—Ö–æ–∂—ñ—Å—Ç—å 0-100%] - [–ø–æ—è—Å–Ω–µ–Ω–Ω—è]

–í–ò–°–ù–û–í–û–ö: [–¢–ê–ö/–ù–Ü] - [–æ–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è]

–ë—É–¥—å —Ç–æ—á–Ω–∏–º —Ç–∞ –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–∏–º —É —Å–≤–æ—î–º—É –∞–Ω–∞–ª—ñ–∑—ñ."""

        try:
            print(f"ü§ñ –í—ñ–¥–ø—Ä–∞–≤–ª—è—é {len(clean_new_text)} —Å–∏–º–≤–æ–ª—ñ–≤ –Ω–∞ AI –∞–Ω–∞–ª—ñ–∑ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤...")
            response = model.generate_content(prompt)
            ai_response = response.text.strip()
            
            print(f"ü§ñ AI –≤—ñ–¥–ø–æ–≤—ñ–¥—å –æ—Ç—Ä–∏–º–∞–Ω–∞: {len(ai_response)} —Å–∏–º–≤–æ–ª—ñ–≤")
            
            # –ü–∞—Ä—Å–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å AI
            similarities = []
            is_duplicate = False
            
            # –®—É–∫–∞—î–º–æ —Å—Ö–æ–∂–æ—Å—Ç—ñ –≤ —Ç–µ–∫—Å—Ç—ñ
            similarity_pattern = r'–¢–µ–∫—Å—Ç (\d+): (\d+)%'
            matches = re.findall(similarity_pattern, ai_response)
            
            for match in matches:
                text_num = int(match[0])
                similarity_percent = int(match[1])
                similarities.append({
                    'text_index': text_num - 1,
                    'similarity_percent': similarity_percent,
                    'similarity_ratio': similarity_percent / 100.0
                })
            
            # –®—É–∫–∞—î–º–æ –≤–∏—Å–Ω–æ–≤–æ–∫
            if '–í–ò–°–ù–û–í–û–ö: –¢–ê–ö' in ai_response.upper():
                is_duplicate = True
            elif '–í–ò–°–ù–û–í–û–ö: –ù–Ü' in ai_response.upper():
                is_duplicate = False
            else:
                # –†–µ–∑–µ—Ä–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞ - —è–∫—â–æ —Å—Ö–æ–∂—ñ—Å—Ç—å > 70%
                max_similarity = max([s['similarity_percent'] for s in similarities]) if similarities else 0
                is_duplicate = max_similarity >= (self.similarity_threshold * 100)
            
            return {
                "ai_available": True,
                "ai_response": ai_response,
                "similarities": similarities,
                "is_duplicate": is_duplicate,
                "max_similarity": max([s['similarity_percent'] for s in similarities]) if similarities else 0
            }
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ AI –∞–Ω–∞–ª—ñ–∑—É: {e}")
            return {"ai_available": False, "error": str(e), "similarities": [], "is_duplicate": False}
    
    def fallback_similarity_check(self, text1: str, text2: str) -> float:
        """–†–µ–∑–µ—Ä–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –±–µ–∑ AI"""
        if not text1 or not text2:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        words1 = set(self.clean_text_for_ai(text1).lower().split())
        words2 = set(self.clean_text_for_ai(text2).lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {'–≤', '–Ω–∞', '–∑–∞', '–¥–æ', '–≤—ñ–¥', '–¥–ª—è', '–ø—Ä–æ', '–ø—ñ–¥', '–Ω–∞–¥', '–ø—Ä–∏', '–∑', '—É', '—ñ', '—Ç–∞', '–∞–±–æ', '–∞–ª–µ'}
        words1 = {w for w in words1 if len(w) > 2 and w not in stop_words}
        words2 = {w for w in words2 if len(w) > 2 and w not in stop_words}
        
        if not words1 or not words2:
            return 0.0
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
        common_words = words1.intersection(words2)
        similarity = len(common_words) / max(len(words1), len(words2))
        
        return similarity


class TelegramChannelChecker:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ Telegram –∫–∞–Ω–∞–ª–∞"""
    
    def __init__(self):
        self.bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
        self.channel_id = os.getenv('TELEGRAM_CHANNEL_ID')
    
    def get_recent_posts(self, limit: int = 5, since_time: Optional[datetime] = None) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ—Å—Ç—ã –∏–∑ –∫–∞–Ω–∞–ª–∞"""
        if not self.bot_token or not self.channel_id:
            print("‚ùå Telegram –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []
    
        try:
            url = f"https://api.telegram.org/bot{self.bot_token}/getUpdates"
            params = {
                'limit': 100,
                'offset': -100
            }
            
            response = requests.get(url, params=params, timeout=30)
            result = response.json()
            
            if not result.get('ok'):
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π: {result.get('description')}")
                return []
            
            channel_posts = []
            for update in result.get('result', []):
                if 'channel_post' in update:
                    post = update['channel_post']
                    if str(post.get('chat', {}).get('id')) == str(self.channel_id):
                        channel_posts.append(post)
            
            channel_posts.sort(key=lambda x: x.get('date', 0), reverse=True)
            recent_posts = channel_posts[:limit]
            
            formatted_posts = []
            for post in recent_posts:
                text = post.get('text') or post.get('caption', '') or ''
                post_date = datetime.fromtimestamp(post.get('date', 0))
                if text and (not since_time or post_date >= since_time):
                    formatted_posts.append({
                        'text': text,
                        'date': post_date,
                        'message_id': post.get('message_id')
                    })
        
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(formatted_posts)} –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–æ–≤ –∏–∑ –∫–∞–Ω–∞–ª–∞")
            return formatted_posts
    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å—Ç–æ–≤ –∏–∑ –∫–∞–Ω–∞–ª–∞: {e}")
            return []

def check_content_similarity(new_article: Dict[str, Any], threshold: float = 0.7, since_time: Optional[datetime] = None) -> bool:
    """
    AI-–ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞–Ω–∞–ª–æ–≤
    
    :param new_article: –ù–æ–≤–∞—è —Å—Ç–∞—Ç—å—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    :param threshold: –ü–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ (0.0-1.0)
    :param since_time: –í—Ä–µ–º—è, –Ω–∞—á–∏–Ω–∞—è —Å –∫–æ—Ç–æ—Ä–æ–≥–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    :return: True –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ—Ö–æ–∂ (–Ω—É–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å), False –µ—Å–ª–∏ —É–Ω–∏–∫–∞–ª–µ–Ω (–º–æ–∂–Ω–æ –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å)
    """
    print(f"üîç AI –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {new_article.get('title', '')[:50]}...")
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≤–µ—Ä—è–ª—å—â–∏–∫–∏
    ai_checker = AIContentSimilarityChecker(threshold)
    channel_checker = TelegramChannelChecker()
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ–π —Å—Ç–∞—Ç—å–∏
    new_text = new_article.get('post_text') or new_article.get('title', '')
    if not new_text:
        print("‚ö†Ô∏è –ù–æ–≤–∞—è —Å—Ç–∞—Ç—å—è –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞")
        return False
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ—Å—Ç—ã –∏–∑ –∫–∞–Ω–∞–ª–∞ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    recent_posts = channel_checker.get_recent_posts(limit=10, since_time=since_time)
    
    if not recent_posts:
        print("‚úÖ –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ –ø–æ—Å—Ç—ã - –ø—É–±–ª–∏–∫—É–µ–º")
        return False
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    existing_texts = [post['text'] for post in recent_posts]
    
    print(f"üìä –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å {len(existing_texts)} –Ω–µ–¥–∞–≤–Ω–∏–º–∏ –ø–æ—Å—Ç–∞–º–∏...")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    if has_gemini_key():
        print("ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ–º AI –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏...")
        ai_result = ai_checker.ai_compare_texts(new_text, existing_texts)
        
        if ai_result.get("ai_available"):
            print("‚úÖ AI –∞–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω:")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            for similarity in ai_result.get("similarities", []):
                text_idx = similarity['text_index']
                percent = similarity['similarity_percent']
                if text_idx < len(recent_posts):
                    post_preview = recent_posts[text_idx]['text'][:50]
                    date_str = recent_posts[text_idx]['date'].strftime('%H:%M %d.%m')
                    print(f"   üìä –ü–æ—Å—Ç {text_idx + 1} ({date_str}): {percent}% —Å—Ö–æ–∂–æ—Å—Ç—ñ")
                    print(f"      üìÑ {post_preview}...")
            
            is_duplicate = ai_result.get("is_duplicate", False)
            max_similarity = ai_result.get("max_similarity", 0)
            
            if is_duplicate:
                print(f"üö´ AI –í–ò–°–ù–û–í–û–ö: –î–£–ë–õ–Ü–ö–ê–¢! (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å: {max_similarity}%)")
                print(f"üìù AI –ø–æ—è—Å–Ω–µ–Ω–Ω—è:")
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á–∞—Å—Ç—å AI –æ—Ç–≤–µ—Ç–∞ —Å –≤—ã–≤–æ–¥–æ–º
                ai_response = ai_result.get("ai_response", "")
                if "–í–ò–°–ù–û–í–û–ö:" in ai_response:
                    conclusion_part = ai_response.split("–í–ò–°–ù–û–í–û–ö:")[1][:200]
                    print(f"   {conclusion_part}...")
            else:
                print(f"‚úÖ AI –í–ò–°–ù–û–í–û–ö: –£–ù–Ü–ö–ê–õ–¨–ù–ò–ô –ö–û–ù–¢–ï–ù–¢ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å: {max_similarity}%)")
            
            return is_duplicate
        else:
            print("‚ö†Ô∏è AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É")
    
    # –†–µ–∑–µ—Ä–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ AI
    print("üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –ø–æ—Ö–æ–∂–µ—Å—Ç–∏...")
    max_similarity = 0.0
    
    for i, existing_text in enumerate(existing_texts):
        similarity = ai_checker.fallback_similarity_check(new_text, existing_text)
        similarity_percent = similarity * 100
        
        if i < len(recent_posts):
            post_date = recent_posts[i]['date'].strftime('%H:%M %d.%m')
            print(f"üìä –ü–æ—Å—Ç {i + 1} ({post_date}): {similarity_percent:.1f}% —Å—Ö–æ–∂–æ—Å—Ç—ñ")
        
        if similarity > max_similarity:
            max_similarity = similarity
        
        if similarity >= threshold:
            print(f"üö´ –î–£–ë–õ–Ü–ö–ê–¢! –°—Ö–æ–∂—ñ—Å—Ç—å {similarity_percent:.1f}% –ø–µ—Ä–µ–≤–∏—â—É—î –ø–æ—Ä—ñ–≥ {threshold * 100}%")
            return True
    
    print(f"‚úÖ –ö–æ–Ω—Ç–µ–Ω—Ç —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å: {max_similarity * 100:.1f}%)")
    return False


def check_articles_similarity(articles: List[Dict[str, Any]], threshold: float = 0.7) -> List[Dict[str, Any]]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—å–∏ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –º–µ–∂–¥—É —Å–æ–±–æ–π (–≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    """
    if not articles:
        return []
    
    print(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º {len(articles)} —Å—Ç–∞—Ç–µ–π –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç—ã...")
    
    ai_checker = AIContentSimilarityChecker(threshold)
    unique_articles = []
    
    for i, article in enumerate(articles):
        print(f"üì∞ –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—å—é {i+1}/{len(articles)}: {article.get('title', '')[:50]}...")
        
        article_text = article.get('post_text') or article.get('title', '')
        
        if not article_text:
            print("‚ö†Ô∏è –°—Ç–∞—Ç—å—è –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue
        
        is_duplicate = False
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ —Å—Ç–∞—Ç—å—è–º–∏
        if unique_articles:
            existing_texts = [art.get('post_text', art.get('title', '')) for art in unique_articles]
            
            if has_gemini_key():
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                ai_result = ai_checker.ai_compare_texts(article_text, existing_texts)
                
                if ai_result.get("ai_available"):
                    is_duplicate = ai_result.get("is_duplicate", False)
                    max_similarity = ai_result.get("max_similarity", 0)
                    
                    if is_duplicate:
                        print(f"üö´ AI: –î—É–±–ª—ñ–∫–∞—Ç! (—Å—Ö–æ–∂—ñ—Å—Ç—å: {max_similarity}%)")
                        # –ù–∞–π–¥–µ–º —Å –∫–∞–∫–æ–π –∏–º–µ–Ω–Ω–æ —Å—Ç–∞—Ç—å–µ–π –¥—É–±–ª–∏–∫–∞—Ç
                        for similarity in ai_result.get("similarities", []):
                            if similarity['similarity_percent'] >= threshold * 100:
                                idx = similarity['text_index']
                                if idx < len(unique_articles):
                                    similar_title = unique_articles[idx].get('title', '')[:50]
                                    print(f"   üìÑ –°—Ö–æ–∂–∞ –∑: {similar_title}...")
                                break
                    else:
                        print(f"‚úÖ AI: –£–Ω—ñ–∫–∞–ª—å–Ω–∞ (–º–∞–∫—Å. —Å—Ö–æ–∂—ñ—Å—Ç—å: {max_similarity}%)")
                else:
                    print("‚ö†Ô∏è AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏")
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
                max_similarity = 0.0
                for existing_text in existing_texts:
                    similarity = ai_checker.fallback_similarity_check(article_text, existing_text)
                    max_similarity = max(max_similarity, similarity)
                
                is_duplicate = max_similarity >= threshold
                print(f"üîß –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: {'–î—É–±–ª—ñ–∫–∞—Ç' if is_duplicate else '–£–Ω—ñ–∫–∞–ª—å–Ω–∞'} (—Å—Ö–æ–∂—ñ—Å—Ç—å: {max_similarity * 100:.1f}%)")
        
        # –ï—Å–ª–∏ –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö
        if not is_duplicate:
            unique_articles.append(article)
            print(f"‚úÖ –°—Ç–∞—Ç—å—è –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö")
        
        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –¥–ª—è AI
        if has_gemini_key():
            time.sleep(0.5)
    
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏: {len(unique_articles)}/{len(articles)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
    return unique_articles


def test_ai_similarity_checker():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç AI –ø—Ä–æ–≤–µ—Ä–∫—É –ø–æ—Ö–æ–∂–µ—Å—Ç–∏"""
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï AI –ü–†–û–í–ï–†–ö–ò –ü–û–•–û–ñ–ï–°–¢–ò")
    print("=" * 60)
    
    if not has_gemini_key():
        print("‚ùå AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —Ç–µ—Å—Ç–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É")
    else:
        print("‚úÖ AI –¥–æ—Å—Ç—É–ø–µ–Ω - —Ç–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é —Å–∏—Å—Ç–µ–º—É")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_articles = [
        {
            'title': '–®–∞—Ö—Ç–∞—Ä –ø–µ—Ä–µ–º—ñ–≥ –î–∏–Ω–∞–º–æ –∑ —Ä–∞—Ö—É–Ω–∫–æ–º 2:1',
            'post_text': '<b>‚öΩ –®–∞—Ö—Ç–∞—Ä –ø–µ—Ä–µ–º—ñ–≥ –î–∏–Ω–∞–º–æ –∑ —Ä–∞—Ö—É–Ω–∫–æ–º 2:1</b>\n\n–í —á–µ–º–ø—ñ–æ–Ω–∞—Ç—ñ –£–∫—Ä–∞—ó–Ω–∏ –≤—ñ–¥–±—É–≤—Å—è –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤–∏–π –º–∞—Ç—á –º—ñ–∂ –®–∞—Ö—Ç–∞—Ä —Ç–∞ –î–∏–Ω–∞–º–æ. –ü–µ—Ä–µ–º–æ–≥—É –∑–¥–æ–±—É–≤ –®–∞—Ö—Ç–∞—Ä –∑–∞–≤–¥—è–∫–∏ –≥–æ–ª–∞–º —É –¥—Ä—É–≥–æ–º—É —Ç–∞–π–º–µ.\n\n#—Ñ—É—Ç–±–æ–ª #–Ω–æ–≤–∏–Ω–∏'
        },
        {
            'title': '–î–∏–Ω–∞–º–æ –ø—Ä–æ–≥—Ä–∞–ª–æ –®–∞—Ö—Ç–∞—Ä—é 1:2 –≤ —á–µ–º–ø—ñ–æ–Ω–∞—Ç—ñ',
            'post_text': '<b>‚öΩ –î–∏–Ω–∞–º–æ –ø—Ä–æ–≥—Ä–∞–ª–æ –®–∞—Ö—Ç–∞—Ä—é</b>\n\n–£ –≤—á–æ—Ä–∞—à–Ω—å–æ–º—É –º–∞—Ç—á—ñ –£–ü–õ –î–∏–Ω–∞–º–æ –ø–æ—Å—Ç—É–ø–∏–ª–æ—Å—è –®–∞—Ö—Ç–∞—Ä—é –∑ —Ä–∞—Ö—É–Ω–∫–æ–º 1:2. –ú–∞—Ç—á –ø—Ä–æ–π—à–æ–≤ –≤ –Ω–∞–ø—Ä—É–∂–µ–Ω—ñ–π –±–æ—Ä–æ—Ç—å–±—ñ.\n\n#—Ñ—É—Ç–±–æ–ª #–£–ü–õ'
        },
        {
            'title': '–ú–±–∞–ø–ø–µ –∑–∞–±–∏–≤ –¥–≤–∞ –≥–æ–ª–∏ –∑–∞ –†–µ–∞–ª –ú–∞–¥—Ä–∏–¥',
            'post_text': '<b>‚öΩ –ú–±–∞–ø–ø–µ - –≥–µ—Ä–æ–π –º–∞—Ç—á—É</b>\n\n–§—Ä–∞–Ω—Ü—É–∑—å–∫–∏–π —Ñ–æ—Ä–≤–∞—Ä–¥ –≤—ñ–¥–∑–Ω–∞—á–∏–≤—Å—è –¥—É–±–ª–µ–º —É –º–∞—Ç—á—ñ –õ–∞ –õ—ñ–≥–∏ –ø—Ä–æ—Ç–∏ –ë–∞—Ä—Å–µ–ª–æ–Ω–∏. –†–µ–∞–ª –ø–µ—Ä–µ–º—ñ–≥ 3:1.\n\n#—Ñ—É—Ç–±–æ–ª #–†–µ–∞–ª'
        }
    ]
    
    # –¢–µ—Å—Ç: –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    print(f"\nüîç –¢–µ—Å—Ç: –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    print("-" * 50)
    
    unique_articles = check_articles_similarity(test_articles, 0.7)
    
    print(f"‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(unique_articles)}/{len(test_articles)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")


if __name__ == "__main__":
    test_ai_similarity_checker()
