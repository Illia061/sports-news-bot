
import re
import os
import requests
import time
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import google.generativeai as genai
from cachetools import TTLCache
import difflib

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_AVAILABLE = False
model = None

def init_gemini():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç–∞ Gemini."""
    global GEMINI_AVAILABLE, model
    if not GEMINI_API_KEY:
        print("‚ö†Ô∏è GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        return
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel("gemini-2.5-flash")
        GEMINI_AVAILABLE = True
        print("‚úÖ Gemini –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Gemini: {e}")

def has_gemini_key() -> bool:
    if not GEMINI_AVAILABLE:
        init_gemini()
    return GEMINI_AVAILABLE

def clean_text_for_ai(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞."""
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s*#\w+\s*', ' ', text)
    text = re.sub(r'[‚öΩüèÜü•Öüì∞üìäüî•üí™üëëüéØ‚≠êüö´‚úÖ‚ùåüåç]', '', text)
    text = re.sub(r'(ESPN Soccer|Football\.ua|OneFootball)', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.replace('**', '')
    return text

class AIContentSimilarityChecker:
    """AI-–ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Gemini."""
    
    def __init__(self, similarity_threshold: float = 0.7):
        self.similarity_threshold = similarity_threshold
        if not has_gemini_key():
            print("‚ö†Ô∏è AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –±–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
    
    def ai_compare_texts(self, new_text: str, existing_texts: List[str], batch_size: int = 5) -> Dict[str, Any]:
        """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç AI –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –≤ –ø–∞–∫–µ—Ç–Ω–æ–º —Ä–µ–∂–∏–º–µ."""
        if not has_gemini_key() or not model:
            return {"ai_available": False, "similarities": [], "is_duplicate": False}
        
        clean_new_text = clean_text_for_ai(new_text)
        clean_existing_texts = [clean_text_for_ai(text) for text in existing_texts]
        
        if not clean_new_text or not any(clean_existing_texts):
            return {"ai_available": True, "similarities": [], "is_duplicate": False}
        
        similarities = []
        is_duplicate = False
        max_similarity = 0
        
        for i in range(0, len(clean_existing_texts), batch_size):
            batch_texts = clean_existing_texts[i:i + batch_size]
            if not batch_texts:
                continue
            
            existing_texts_formatted = "\n".join(f"–¢–µ–∫—Å—Ç {j+1}: {text}" for j, text in enumerate(batch_texts) if text)
            if not existing_texts_formatted:
                continue
            
            prompt = f"""–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ –∞–Ω–∞–ª—ñ–∑—É —Ñ—É—Ç–±–æ–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω. –¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è - –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —á–∏ —î –Ω–æ–≤–∞ –Ω–æ–≤–∏–Ω–∞ –¥—É–±–ª—ñ–∫–∞—Ç–æ–º —ñ—Å–Ω—É—é—á–∏—Ö.

–ù–û–í–ê –ù–û–í–ò–ù–ê:
{clean_new_text}

–Ü–°–ù–£–Æ–ß–Ü –ù–û–í–ò–ù–ò:{existing_texts_formatted}

–ó–ê–í–î–ê–ù–ù–Ø:
1. –ü–æ—Ä—ñ–≤–Ω—è–π –Ω–æ–≤—É –Ω–æ–≤–∏–Ω—É –∑ –∫–æ–∂–Ω–æ—é —ñ—Å–Ω—É—é—á–æ—é
2. –í–∏–∑–Ω–∞—á —Å–µ–º–∞–Ω—Ç–∏—á–Ω—É —Å—Ö–æ–∂—ñ—Å—Ç—å (–Ω–µ —Ç—ñ–ª—å–∫–∏ –¥–æ—Å–ª—ñ–≤–Ω—É)
3. –í—Ä–∞—Ö–æ–≤—É–π —â–æ –æ–¥–Ω–∞–∫–æ–≤—ñ –ø–æ–¥—ñ—ó –º–æ–∂—É—Ç—å –±—É—Ç–∏ –æ–ø–∏—Å–∞–Ω—ñ –ø–æ-—Ä—ñ–∑–Ω–æ–º—É
4. –í—Ä–∞—Ö–æ–≤—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ñ—É—Ç–±–æ–ª—É - —Ä—ñ–∑–Ω—ñ –º–∞—Ç—á—ñ, —Ä—ñ–∑–Ω—ñ –∫–æ–º–∞–Ω–¥–∏ —Ü–µ –†–Ü–ó–ù–Ü –Ω–æ–≤–∏–Ω–∏
5. –û–¥–Ω–∞–∫ –æ–¥–Ω–∞ —ñ —Ç–∞ –∂ –ø–æ–¥—ñ—è (—Ç–æ–π —Å–∞–º–∏–π –º–∞—Ç—á, —Ç–æ–π —Å–∞–º–∏–π —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä) —Ü–µ –î–£–ë–õ–Ü–ö–ê–¢

–ö–†–ò–¢–ï–†–Ü–á –î–£–ë–õ–Ü–ö–ê–¢–£:
- –¢–∞ –∂ —Ñ—É—Ç–±–æ–ª—å–Ω–∞ –ø–æ–¥—ñ—è (–º–∞—Ç—á, —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä, —Ç—Ä–∞–≤–º–∞ –≥—Ä–∞–≤—Ü—è —Ç–æ—â–æ)
- –¢—ñ –∂ –æ—Å–Ω–æ–≤–Ω—ñ —Ñ–∞–∫—Ç–∏, –Ω–∞–≤—ñ—Ç—å —è–∫—â–æ –ø–æ-—Ä—ñ–∑–Ω–æ–º—É —Å—Ñ–æ—Ä–º—É–ª—å–æ–≤–∞–Ω—ñ
- –¢–æ–π —Å–∞–º–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–∞—Ç—á—É –º—ñ–∂ —Ç–∏–º–∏ –∂ –∫–æ–º–∞–Ω–¥–∞–º–∏

–ù–ï –î–£–ë–õ–Ü–ö–ê–¢:
- –†—ñ–∑–Ω—ñ –º–∞—Ç—á—ñ (–Ω–∞–≤—ñ—Ç—å —Ç–∏—Ö —Å–∞–º–∏—Ö –∫–æ–º–∞–Ω–¥ –≤ —Ä—ñ–∑–Ω–∏–π —á–∞—Å)
- –†—ñ–∑–Ω—ñ –≥—Ä–∞–≤—Ü—ñ –∞–±–æ –∫–æ–º–∞–Ω–¥–∏
- –†—ñ–∑–Ω—ñ –ø–æ–¥—ñ—ó –Ω–∞–≤—ñ—Ç—å –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ –º–∞—Ç—á—É

–§–û–†–ú–ê–¢ –í–Ü–î–ü–û–í–Ü–î–Ü (–¥—É–∂–µ –≤–∞–∂–ª–∏–≤–æ –¥–æ—Ç—Ä–∏–º—É–≤–∞—Ç–∏—Å—è):
–ê–ù–ê–õ–Ü–ó:
–¢–µ–∫—Å—Ç 1: [—Å—Ö–æ–∂—ñ—Å—Ç—å 0-100%] - [–ø–æ—è—Å–Ω–µ–Ω–Ω—è]
–¢–µ–∫—Å—Ç 2: [—Å—Ö–æ–∂—ñ—Å—Ç—å 0-100%] - [–ø–æ—è—Å–Ω–µ–Ω–Ω—è]
–¢–µ–∫—Å—Ç 3: [—Å—Ö–æ–∂—ñ—Å—Ç—å 0-100%] - [–ø–æ—è—Å–Ω–µ–Ω–Ω—è]

–í–ò–°–ù–û–í–û–ö: [–¢–ê–ö/–ù–Ü] - [–æ–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è]

–ë—É–¥—å —Ç–æ—á–Ω–∏–º —Ç–∞ –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–∏–º —É —Å–≤–æ—î–º—É –∞–Ω–∞–ª—ñ–∑—ñ."""
            
            try:
                print(f"ü§ñ –í—ñ–¥–ø—Ä–∞–≤–ª—è—é {len(clean_new_text)} —Å–∏–º–≤–æ–ª—ñ–≤ –Ω–∞ AI –∞–Ω–∞–ª—ñ–∑ –¥—É–±–ª–∏–∫–∞—Ç—ñ–≤...")
                response = model.generate_content(prompt)
                ai_response = response.text.strip()
                
                print(f"ü§ñ AI –≤—ñ–¥–ø–æ–≤—ñ–¥—å –æ—Ç—Ä–∏–º–∞–Ω–∞: {len(ai_response)} —Å–∏–º–≤–æ–ª—ñ–≤")
                
                batch_matches = re.findall(r'–¢–µ–∫—Å—Ç (\d+): (\d+)%', ai_response)
                for match in batch_matches:
                    text_num = int(match[0]) - 1 + i
                    similarity_percent = int(match[1])
                    similarities.append({
                        'text_index': text_num,
                        'similarity_percent': similarity_percent,
                        'similarity_ratio': similarity_percent / 100.0
                    })
                    max_similarity = max(max_similarity, similarity_percent)
                
                if '–í–ò–°–ù–û–í–û–ö: –¢–ê–ö' in ai_response.upper():
                    is_duplicate = True
                    break
            except Exception as e:
                print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ AI –∞–Ω–∞–ª—ñ–∑—É –¥–ª—è –ø–∞–∫–µ—Ç–∞ {i//batch_size + 1}: {e}")
                continue
        
        return {
            "ai_available": True,
            "ai_response": ai_response if 'ai_response' in locals() else "",
            "similarities": similarities,
            "is_duplicate": is_duplicate,
            "max_similarity": max_similarity
        }
    
    def fallback_similarity_check(self, text1: str, text2: str) -> float:
        """–†–µ–∑–µ—Ä–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –±–µ–∑ AI."""
        if not text1 or not text2:
            return 0.0
        
        clean_text1 = clean_text_for_ai(text1).lower()
        clean_text2 = clean_text_for_ai(text2).lower()
        
        matcher = difflib.SequenceMatcher(None, clean_text1, clean_text2)
        similarity = matcher.ratio()
        
        key_terms = {'—à–∞—Ö—Ç–∞—Ä', '–¥–∏–Ω–∞–º–æ', '—Ä–µ–∞–ª', '–±–∞—Ä—Å–µ–ª–æ–Ω–∞', '–º–±–∞–ø–ø–µ', '—Ä–æ–Ω–∞–ª–¥—É'}
        words1 = set(clean_text1.split())
        words2 = set(clean_text2.split())
        common_key_terms = key_terms.intersection(words1).intersection(words2)
        
        if common_key_terms:
            similarity = min(1.0, similarity + 0.2)
        
        return similarity

class TelegramChannelChecker:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ Telegram –∫–∞–Ω–∞–ª–∞."""
    
    def __init__(self):
        self.bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
        self.channel_id = os.getenv('TELEGRAM_CHANNEL_ID')
        self.cache = TTLCache(maxsize=10, ttl=300)  # –ö—ç—à –Ω–∞ 5 –º–∏–Ω—É—Ç
    
    def get_recent_posts(self, limit: int = 5, since_time: Optional[datetime] = None) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ—Å—Ç—ã –∏–∑ –∫–∞–Ω–∞–ª–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
        cache_key = (limit, since_time.isoformat() if since_time else None)
        if cache_key in self.cache:
            print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã")
            return self.cache[cache_key]
        
        if not self.bot_token or not self.channel_id:
            print("‚ùå Telegram –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []
    
        try:
            url = f"https://api.telegram.org/bot{self.bot_token}/getUpdates"
            params = {'limit': 100, 'offset': -100}
            response = requests.get(url, params=params, timeout=30)
            result = response.json()
            
            if not result.get('ok'):
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π: {result.get('description')}")
                return []
            
            channel_posts = []
            for update in result.get('result', []):
                if 'channel_post' in update:
                    post = update['channel_post']
                    if str(post.get('chat', {}).get('id')) == str(self.channel_id):
                        channel_posts.append(post)
            
            channel_posts.sort(key=lambda x: x.get('date', 0), reverse=True)
            recent_posts = channel_posts[:limit]
            
            formatted_posts = []
            for post in recent_posts:
                text = post.get('text') or post.get('caption', '') or ''
                post_date = datetime.fromtimestamp(post.get('date', 0))
                if text and (not since_time or post_date >= since_time):
                    formatted_posts.append({
                        'text': text,
                        'date': post_date,
                        'message_id': post.get('message_id')
                    })
        
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(formatted_posts)} –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–æ–≤ –∏–∑ –∫–∞–Ω–∞–ª–∞")
            self.cache[cache_key] = formatted_posts
            return formatted_posts
    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å—Ç–æ–≤ –∏–∑ –∫–∞–Ω–∞–ª–∞: {e}")
            return []

def check_content_similarity(new_article: Dict[str, Any], threshold: float = 0.7, since_time: Optional[datetime] = None) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Ö–æ–∂–µ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ—Å—Ç–∞–º–∏ –≤ –∫–∞–Ω–∞–ª–µ."""
    print(f"üîç AI –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {new_article.get('title', '')[:50]}...")
    
    ai_checker = AIContentSimilarityChecker(threshold)
    channel_checker = TelegramChannelChecker()
    
    new_text = new_article.get('post_text') or new_article.get('title', '')
    if not new_text:
        print("‚ö†Ô∏è –ù–æ–≤–∞—è —Å—Ç–∞—Ç—å—è –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞")
        return False
    
    recent_posts = channel_checker.get_recent_posts(limit=10, since_time=since_time)
    
    if not recent_posts:
        print("‚úÖ –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ –ø–æ—Å—Ç—ã - –ø—É–±–ª–∏–∫—É–µ–º")
        return False
    
    existing_texts = [post['text'] for post in recent_posts]
    
    print(f"üìä –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å {len(existing_texts)} –Ω–µ–¥–∞–≤–Ω–∏–º–∏ –ø–æ—Å—Ç–∞–º–∏...")
    
    if has_gemini_key():
        print("ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ–º AI –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏...")
        ai_result = ai_checker.ai_compare_texts(new_text, existing_texts)
        
        if ai_result.get("ai_available"):
            print("‚úÖ AI –∞–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω:")
            
            for similarity in ai_result.get("similarities", []):
                text_idx = similarity['text_index']
                percent = similarity['similarity_percent']
                if text_idx < len(recent_posts):
                    post_preview = recent_posts[text_idx]['text'][:50]
                    date_str = recent_posts[text_idx]['date'].strftime('%H:%M %d.%m')
                    print(f"   üìä –ü–æ—Å—Ç {text_idx + 1} ({date_str}): {percent}% —Å—Ö–æ–∂–æ—Å—Ç—ñ")
                    print(f"      üìÑ {post_preview}...")
            
            is_duplicate = ai_result.get("is_duplicate", False)
            max_similarity = ai_result.get("max_similarity", 0)
            
            if is_duplicate:
                print(f"üö´ AI –í–ò–°–ù–û–í–û–ö: –î–£–ë–õ–Ü–ö–ê–¢! (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å: {max_similarity}%)")
                print(f"üìù AI –ø–æ—è—Å–Ω–µ–Ω–Ω—è:")
                ai_response = ai_result.get("ai_response", "")
                if "–í–ò–°–ù–û–í–û–ö:" in ai_response:
                    conclusion_part = ai_response.split("–í–ò–°–ù–û–í–û–ö:")[1][:200]
                    print(f"   {conclusion_part}...")
            else:
                print(f"‚úÖ AI –í–ò–°–ù–û–í–û–ö: –£–ù–Ü–ö–ê–õ–¨–ù–ò–ô –ö–û–ù–¢–ï–ù–¢ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å: {max_similarity}%)")
            
            return is_duplicate
        else:
            print("‚ö†Ô∏è AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É")
    
    print("üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –ø–æ—Ö–æ–∂–µ—Å—Ç–∏...")
    max_similarity = 0.0
    
    for i, existing_text in enumerate(existing_texts):
        similarity = ai_checker.fallback_similarity_check(new_text, existing_text)
        similarity_percent = similarity * 100
        
        if i < len(recent_posts):
            post_date = recent_posts[i]['date'].strftime('%H:%M %d.%m')
            print(f"üìä –ü–æ—Å—Ç {i + 1} ({post_date}): {similarity_percent:.1f}% —Å—Ö–æ–∂–æ—Å—Ç—ñ")
        
        if similarity > max_similarity:
            max_similarity = similarity
        
        if similarity >= threshold:
            print(f"üö´ –î–£–ë–õ–Ü–ö–ê–¢! –°—Ö–æ–∂—ñ—Å—Ç—å {similarity_percent:.1f}% –ø–µ—Ä–µ–≤–∏—â—É—î –ø–æ—Ä—ñ–≥ {threshold * 100}%")
            return True
    
    print(f"‚úÖ –ö–æ–Ω—Ç–µ–Ω—Ç —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å: {max_similarity * 100:.1f}%)")
    return False

def check_articles_similarity(articles: List[Dict[str, Any]], threshold: float = 0.7) -> List[Dict[str, Any]]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—å–∏ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –º–µ–∂–¥—É —Å–æ–±–æ–π."""
    if not articles:
        return []
    
    print(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º {len(articles)} —Å—Ç–∞—Ç–µ–π –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç—ã...")
    
    ai_checker = AIContentSimilarityChecker(threshold)
    unique_articles = []
    
    for i, article in enumerate(articles):
        print(f"üì∞ –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—å—é {i+1}/{len(articles)}: {article.get('title', '')[:50]}...")
        
        article_text = article.get('post_text') or article.get('title', '')
        
        if not article_text:
            print("‚ö†Ô∏è –°—Ç–∞—Ç—å—è –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue
        
        is_duplicate = False
        
        if unique_articles:
            existing_texts = [art.get('post_text', art.get('title', '')) for art in unique_articles]
            
            if has_gemini_key():
                ai_result = ai_checker.ai_compare_texts(article_text, existing_texts)
                
                if ai_result.get("ai_available"):
                    is_duplicate = ai_result.get("is_duplicate", False)
                    max_similarity = ai_result.get("max_similarity", 0)
                    
                    if is_duplicate:
                        print(f"üö´ AI: –î—É–±–ª—ñ–∫–∞—Ç! (—Å—Ö–æ–∂—ñ—Å—Ç—å: {max_similarity}%)")
                        for similarity in ai_result.get("similarities", []):
                            if similarity['similarity_percent'] >= threshold * 100:
                                idx = similarity['text_index']
                                if idx < len(unique_articles):
                                    similar_title = unique_articles[idx].get('title', '')[:50]
                                    print(f"   üìÑ –°—Ö–æ–∂–∞ –∑: {similar_title}...")
                                break
                    else:
                        print(f"‚úÖ AI: –£–Ω—ñ–∫–∞–ª—å–Ω–∞ (–º–∞–∫—Å. —Å—Ö–æ–∂—ñ—Å—Ç—å: {max_similarity}%)")
                else:
                    print("‚ö†Ô∏è AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏")
            else:
                max_similarity = 0.0
                for existing_text in existing_texts:
                    similarity = ai_checker.fallback_similarity_check(article_text, existing_text)
                    max_similarity = max(max_similarity, similarity)
                
                is_duplicate = max_similarity >= threshold
                print(f"üîß –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: {'–î—É–±–ª—ñ–∫–∞—Ç' if is_duplicate else '–£–Ω—ñ–∫–∞–ª—å–Ω–∞'} (—Å—Ö–æ–∂—ñ—Å—Ç—å: {max_similarity * 100:.1f}%)")
        
        if not is_duplicate:
            unique_articles.append(article)
            print(f"‚úÖ –°—Ç–∞—Ç—å—è –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö")
        
        if has_gemini_key():
            time.sleep(0.5)
    
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏: {len(unique_articles)}/{len(articles)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
    return unique_articles

def test_ai_similarity_checker():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç AI –ø—Ä–æ–≤–µ—Ä–∫—É –ø–æ—Ö–æ–∂–µ—Å—Ç–∏."""
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï AI –ü–†–û–í–ï–†–ö–ò –ü–û–•–û–ñ–ï–°–¢–ò")
    print("=" * 60)
    
    if not has_gemini_key():
        print("‚ùå AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —Ç–µ—Å—Ç–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É")
    else:
        print("‚úÖ AI –¥–æ—Å—Ç—É–ø–µ–Ω - —Ç–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é —Å–∏—Å—Ç–µ–º—É")
    
    test_articles = [
        {
            'title': '–®–∞—Ö—Ç–∞—Ä –ø–µ—Ä–µ–º—ñ–≥ –î–∏–Ω–∞–º–æ –∑ —Ä–∞—Ö—É–Ω–∫–æ–º 2:1',
            'post_text': '<b>‚öΩ –®–∞—Ö—Ç–∞—Ä –ø–µ—Ä–µ–º—ñ–≥ –î–∏–Ω–∞–º–æ –∑ —Ä–∞—Ö—É–Ω–∫–æ–º 2:1</b>\n\n–í —á–µ–º–ø—ñ–æ–Ω–∞—Ç—ñ –£–∫—Ä–∞—ó–Ω–∏ –≤—ñ–¥–±—É–≤—Å—è –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤–∏–π –º–∞—Ç—á –º—ñ–∂ –®–∞—Ö—Ç–∞—Ä —Ç–∞ –î–∏–Ω–∞–º–æ. –ü–µ—Ä–µ–º–æ–≥—É –∑–¥–æ–±—É–≤ –®–∞—Ö—Ç–∞—Ä –∑–∞–≤–¥—è–∫–∏ –≥–æ–ª–∞–º —É –¥—Ä—É–≥–æ–º—É —Ç–∞–π–º–µ.\n\n#—Ñ—É—Ç–±–æ–ª #–Ω–æ–≤–∏–Ω–∏'
        },
        {
            'title': '–î–∏–Ω–∞–º–æ –ø—Ä–æ–≥—Ä–∞–ª–æ –®–∞—Ö—Ç–∞—Ä—é 1:2 –≤ —á–µ–º–ø—ñ–æ–Ω–∞—Ç—ñ',
            'post_text': '<b>‚öΩ –î–∏–Ω–∞–º–æ –ø—Ä–æ–≥—Ä–∞–ª–æ –®–∞—Ö—Ç–∞—Ä—é</b>\n\n–£ –≤—á–æ—Ä–∞—à–Ω—å–æ–º—É –º–∞—Ç—á—ñ –£–ü–õ –î–∏–Ω–∞–º–æ –ø–æ—Å—Ç—É–ø–∏–ª–æ—Å—è –®–∞—Ö—Ç–∞—Ä—é –∑ —Ä–∞—Ö—É–Ω–∫–æ–º 1:2. –ú–∞—Ç—á –ø—Ä–æ–π—à–æ–≤ –≤ –Ω–∞–ø—Ä—É–∂–µ–Ω—ñ–π –±–æ—Ä–æ—Ç—å–±—ñ.\n\n#—Ñ—É—Ç–±–æ–ª #–£–ü–õ'
        },
        {
            'title': '–ú–±–∞–ø–ø–µ –∑–∞–±–∏–≤ –¥–≤–∞ –≥–æ–ª–∏ –∑–∞ –†–µ–∞–ª –ú–∞–¥—Ä–∏–¥',
            'post_text': '<b>‚öΩ –ú–±–∞–ø–ø–µ - –≥–µ—Ä–æ–π –º–∞—Ç—á—É</b>\n\n–§—Ä–∞–Ω—Ü—É–∑—å–∫–∏–π —Ñ–æ—Ä–≤–∞—Ä–¥ –≤—ñ–¥–∑–Ω–∞—á–∏–≤—Å—è –¥—É–±–ª–µ–º —É –º–∞—Ç—á—ñ –õ–∞ –õ—ñ–≥–∏ –ø—Ä–æ—Ç–∏ –ë–∞—Ä—Å–µ–ª–æ–Ω–∏. –†–µ–∞–ª –ø–µ—Ä–µ–º—ñ–≥ 3:1.\n\n#—Ñ—É—Ç–±–æ–ª #–†–µ–∞–ª'
        }
    ]
    
    print(f"\nüîç –¢–µ—Å—Ç: –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    print("-" * 50)
    
    unique_articles = check_articles_similarity(test_articles, 0.7)
    
    print(f"‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(unique_articles)}/{len(test_articles)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")

if __name__ == "__main__":
    test_ai_similarity_checker()
