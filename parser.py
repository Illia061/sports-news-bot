import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from zoneinfo import ZoneInfo

KIEV_TZ = ZoneInfo("Europe/Kiev")

class FootballUATargetedParser:
    def __init__(self, max_consecutive_old=2):
        self.base_url = "https://football.ua/"
        self.max_consecutive_old = max_consecutive_old
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "uk-UA,uk;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
        })
    
    def get_page_content(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            return BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None
    
    def find_golovne_za_dobu_section(self, soup):
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –±–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'"""
        header_texts = [
            "–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£",
            "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É", 
            "–ì–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
        ]
        
        for header_text in header_texts:
            header_element = soup.find(text=re.compile(header_text, re.I))
            if header_element:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫: '{header_text}'")
                parent = header_element.parent
                while parent and parent.name not in ['section', 'div', 'article']:
                    parent = parent.parent
                if parent:
                    news_container = parent.find_next(['div', 'ul', 'section'])
                    if news_container:
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
                        return news_container
                    else:
                        return parent
        
        possible_selectors = [
            '.sidebar', '.right-column', '.side-block', '.news-sidebar',
            '.daily-news', '.main-today', '.today-block', '.golovne',
            '[class*="today"]', '[class*="daily"]', '[class*="golovne"]'
        ]
        
        for selector in possible_selectors:
            elements = soup.select(selector)
            for element in elements:
                if re.search(r'–≥–æ–ª–æ–≤–Ω–µ.*–∑–∞.*–¥–æ–±—É', element.get_text(), re.I):
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                    return element
        
        print("‚ö†Ô∏è –ò—â–µ–º –±–ª–æ–∫ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
        all_divs = soup.find_all(['div', 'section'], class_=True)
        for div in all_divs:
            div_text = div.get_text().lower()
            if '–≥–æ–ª–æ–≤–Ω–µ' in div_text and '–¥–æ–±—É' in div_text:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ —Å —Ç–µ–∫—Å—Ç–æ–º '–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É'")
                return div
        
        print("‚ùå –ë–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None
    
    def extract_news_from_section(self, section, since_time: Optional[datetime] = None):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        if not section:
            return []
        
        news_links = []
        all_links = section.find_all('a', href=True)
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –≤ —Å–µ–∫—Ü–∏–∏")
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            if self.is_news_link(href) and len(text) > 10:
                full_url = urljoin(self.base_url, href)
                news_links.append({
                    'title': text,
                    'url': full_url,
                    'href': href
                })
                print(f"üì∞ –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {text[:50]}...")
        
        seen_urls = set()
        unique_news = []
        for news in news_links:
            if news['url'] not in seen_urls:
                unique_news.append(news)
                seen_urls.add(news['url'])
        
        if since_time:
            print(f"üïí –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å {since_time.strftime('%H:%M %d.%m.%Y')}")
            return unique_news
        else:
            return unique_news[:5]
    
    def is_news_link(self, href):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π"""
        if not href:
            return False
        news_patterns = [
            r'/news/', r'/ukraine/', r'/world/', r'/europe/', r'/england/',
            r'/spain/', r'/italy/', r'/germany/', r'/france/', r'/poland/',
            r'/\d+[^/]*\.html'
        ]
        return any(re.search(pattern, href) for pattern in news_patterns)
    
    def parse_ukrainian_date(self, date_text: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏—Ç —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã"""
        try:
            ukrainian_months = {
                '—Å—ñ—á–Ω—è': 1, '–ª—é—Ç–æ–≥–æ': 2, '–±–µ—Ä–µ–∑–Ω—è': 3, '–∫–≤—ñ—Ç–Ω—è': 4, '—Ç—Ä–∞–≤–Ω—è': 5, '—á–µ—Ä–≤–Ω—è': 6,
                '–ª–∏–ø–Ω—è': 7, '—Å–µ—Ä–ø–Ω—è': 8, '–≤–µ—Ä–µ—Å–Ω—è': 9, '–∂–æ–≤—Ç–Ω—è': 10, '–ª–∏—Å—Ç–æ–ø–∞–¥–∞': 11, '–≥—Ä—É–¥–Ω—è': 12,
                '—Å—ñ—á': 1, '–ª—é—Ç': 2, '–±–µ—Ä': 3, '–∫–≤—ñ': 4, '—Ç—Ä–∞': 5, '—á–µ—Ä': 6,
                '–ª–∏–ø': 7, '—Å–µ—Ä': 8, '–≤–µ—Ä': 9, '–∂–æ–≤': 10, '–ª–∏—Å': 11, '–≥—Ä—É': 12
            }
            cleaned_text = re.sub(r'[,.]', '', date_text.lower().strip())
            pattern1 = r'(\d{1,2})\s+(\w+)\s+(\d{4})[\s,]+(\d{1,2}):(\d{2})'
            match1 = re.search(pattern1, cleaned_text)
            if match1:
                day = int(match1.group(1))
                month_name = match1.group(2)
                year = int(match1.group(3))
                hour = int(match1.group(4))
                minute = int(match1.group(5))
                if month_name in ukrainian_months:
                    month = ukrainian_months[month_name]
                    return datetime(year, month, day, hour, minute, tzinfo=KIEV_TZ)
            pattern2 = r'(\d{1,2})\.(\d{1,2})\.(\d{4})[\s,]+(\d{1,2}):(\d{2})'
            match2 = re.search(pattern2, cleaned_text)
            if match2:
                day = int(match2.group(1))
                month = int(match2.group(2))
                year = int(match2.group(3))
                hour = int(match2.group(4))
                minute = int(match2.group(5))
                return datetime(year, month, day, hour, minute, tzinfo=KIEV_TZ)
            pattern3 = r'^(\d{1,2}):(\d{2})$'
            match3 = re.search(pattern3, cleaned_text)
            if match3:
                hour = int(match3.group(1))
                minute = int(match3.group(2))
                today = datetime.now(KIEV_TZ).replace(hour=hour, minute=minute, second=0, microsecond=0)
                return today
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–π –¥–∞—Ç—ã '{date_text}': {e}")
        return None
    
    def estimate_article_publish_time(self, soup, url: str) -> Optional[datetime]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏"""
        try:
            print(f"üïí –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –¥–ª—è: {url}")
            meta_selectors = [
                'meta[property="article:published_time"]',
                'meta[name="publish_date"]', 'meta[name="date"]',
                'meta[property="og:published_time"]',
                'meta[name="DC.date"]', 'meta[itemprop="datePublished"]'
            ]
            for selector in meta_selectors:
                meta_tag = soup.select_one(selector)
                if meta_tag:
                    content = meta_tag.get('content', '')
                    if content:
                        print(f"üìÖ –ù–∞–π–¥–µ–Ω –º–µ—Ç–∞-—Ç–µ–≥ {selector}: {content}")
                        try:
                            if 'T' in content:
                                parsed_date = datetime.fromisoformat(content.replace('Z', '+00:00').replace('+00:00', ''))
                                parsed_date_kiev = parsed_date.astimezone(KIEV_TZ)
                                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω –º–µ—Ç–∞-—Ç–µ–≥: {parsed_date_kiev}")
                                return parsed_date_kiev
                        except Exception as e:
                            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –º–µ—Ç–∞-—Ç–µ–≥: {e}")
                            continue
            date_selectors = [
                '.article-date', '.publish-date', '.news-date',
                '.date', '.timestamp', 'time[datetime]',
                '.article-time', '.post-date', '.entry-date',
                '[class*="date"]', '[class*="time"]'
            ]
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    datetime_attr = date_elem.get('datetime')
                    if datetime_attr:
                        print(f"üìÖ –ù–∞–π–¥–µ–Ω datetime –∞—Ç—Ä–∏–±—É—Ç: {datetime_attr}")
                        try:
                            parsed_date = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00').replace('+00:00', ''))
                            parsed_date_kiev = parsed_date.astimezone(KIEV_TZ)
                            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω datetime: {parsed_date_kiev}")
                            return parsed_date_kiev
                        except Exception as e:
                            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å datetime: {e}")
                    date_text = date_elem.get_text(strip=True)
                    if date_text:
                        print(f"üìÖ –ù–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç –¥–∞—Ç—ã –≤ {selector}: '{date_text}'")
                        parsed_date = self.parse_ukrainian_date(date_text)
                        if parsed_date:
                            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω —Ç–µ–∫—Å—Ç –¥–∞—Ç—ã: {parsed_date}")
                            return parsed_date
            all_text = soup.get_text()
            date_patterns = [
                r'(\d{1,2})\s+(—Å—ñ—á–Ω—è|–ª—é—Ç–æ–≥–æ|–±–µ—Ä–µ–∑–Ω—è|–∫–≤—ñ—Ç–Ω—è|—Ç—Ä–∞–≤–Ω—è|—á–µ—Ä–≤–Ω—è|–ª–∏–ø–Ω—è|—Å–µ—Ä–ø–Ω—è|–≤–µ—Ä–µ—Å–Ω—è|–∂–æ–≤—Ç–Ω—è|–ª–∏—Å—Ç–æ–ø–∞–¥–∞|–≥—Ä—É–¥–Ω—è)\s+(\d{4})[\s,]+(\d{1,2}):(\d{2})',
                r'(\d{1,2})\.(\d{1,2})\.(\d{4})[\s,]+(\d{1,2}):(\d{2})',
                r'(\d{1,2})/(\d{1,2})/(\d{4})[\s,]+(\d{1,2}):(\d{2})'
            ]
            for pattern in date_patterns:
                matches = re.findall(pattern, all_text, re.IGNORECASE)
                for match in matches[:3]:
                    if len(match) >= 5:
                        try:
                            if '—Å—ñ—á–Ω—è' in pattern or '–ª—é—Ç–æ–≥–æ' in pattern:
                                parsed_date = self.parse_ukrainian_date(' '.join(match))
                            else:
                                day, month, year, hour, minute = map(int, match)
                                parsed_date = datetime(year, month, day, hour, minute, tzinfo=KIEV_TZ)
                            if parsed_date:
                                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –¥–∞—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ: {parsed_date}")
                                return parsed_date
                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –¥–∞—Ç—ã: {e}")
                            continue
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")
            return None
    
    def count_words(self, text: str) -> int:
        """–¢–û–ß–ù–´–ô –ø–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤ –∫–∞–∫ –¥–µ–ª–∞–µ—Ç —á–µ–ª–æ–≤–µ–∫ - —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞"""
        if not text:
            return 0
        clean_text = re.sub(r'<[^>]+>', '', text)
        service_patterns = [
            r'\d{1,2}\s+\w+\s+\d{4},\s+\d{1,2}:\d{2}',  # –¥–∞—Ç—ã –∏ –≤—Ä–µ–º—è
            r'getty images', r'—Ñ–æ—Ç–æ:.*', r'–¥–∂–µ—Ä–µ–ª–æ:.*',
            r'—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–æ–∂:.*', r'\([^)]*\)'
        ]
        for pattern in service_patterns:
            clean_text = re.sub(pattern, '', clean_text, flags=re.IGNORECASE)
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        word_text = re.sub(r'[^\w\s]', ' ', clean_text, flags=re.UNICODE)
        words = []
        for word in word_text.split():
            word = word.strip()
            if len(word) >= 2 and not word.isdigit():
                words.append(word)
        return len(words)
    
    def extract_clean_article_content(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¢–û–õ–¨–ö–û –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ –±–µ–∑ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, —Å–ø–∏—Å–∫–æ–≤ –∏ —Å–ª—É–∂–µ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        soup_copy = BeautifulSoup(str(soup), 'html.parser')
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        unwanted_selectors = [
            'script', 'style', 'iframe', 'noscript', 'svg',
            'header', 'nav', 'footer', 'aside',
            '.header', '.footer', '.navigation', '.nav',
            '[class*="ad"]', '[class*="banner"]', '[class*="advertisement"]',
            '[class*="social"]', '[class*="share"]', '[class*="related"]',
            '[class*="comment"]', '[class*="sidebar"]', '[class*="widget"]',
            '.breadcrumb', '.tags', '.meta', '.author', '.date', '.time',
            '.article-date', '.publish-date', '.news-date',
            '[class*="date"]', '[class*="time"]', '[class*="meta"]',
            '.social-buttons', '.article-info', '.news-info',
            '.photo-credit', '.image-caption', '.getty-images',
            'h1', 'h2', 'h3', 'h4', 'h5', 'h6',  # –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∏
            'ul', 'ol', 'li',  # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ø–∏—Å–∫–∏
            'blockquote',  # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–∏—Ç–∞—Ç—ã
        ]
        for selector in unwanted_selectors:
            for element in soup_copy.select(selector):
                element.decompose()
    
        # –°—Ç—Ä–æ–≥–æ –≤—ã–±–∏—Ä–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        main_selectors = [
            '.article-body p',  # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
            '.news-content p', 
            '.post-content p',
            '.main-content p',
            '.content p:not(.caption):not([class*="meta"]):not([class*="info"])',  # –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ–¥–ø–∏—Å–∏ –∏ –º–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            'article p'
        ]
    
        article_paragraphs = []
        for selector in main_selectors:
            paragraphs = soup_copy.select(selector)
            if paragraphs:
                print(f"üéØ –ù–∞–π–¥–µ–Ω—ã –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                article_paragraphs = paragraphs
                break
    
        if not article_paragraphs:
            print("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤–Ω—É—Ç—Ä–∏ article –∏–ª–∏ .content")
            article_container = soup_copy.select_one('article, .content, .article-body, .news-content')
            if article_container:
                article_paragraphs = article_container.find_all('p', recursive=False)
    
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ
        meaningful_paragraphs = []
        for p in article_paragraphs:
            p_text = p.get_text(strip=True)
            if (len(p_text) > 30 and  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑
                not p.find_parent(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'blockquote']) and
                not any(skip_phrase in p_text.lower() for skip_phrase in [
                    'getty images', '—Ñ–æ—Ç–æ:', '–¥–∂–µ—Ä–µ–ª–æ:', '—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–æ–∂',
                    '–ø—ñ–¥–ø–∏—Å—É–π—Ç–µ—Å—å', '—Å—Ç–µ–∂–∏—Ç–µ', 'telegram', 'facebook', 'twitter',
                    '–ø—Ä–æ —Ü–µ –ø–æ–≤—ñ–¥–æ–º–ª—è—î', 'football.ua', '—Ñ—É—Ç–±–æ–ª.ua',
                    'cookie', '—Ä–µ–∫–ª–∞–º', '–∫–æ–º–µ–Ω—Ç–∞—Ä', '–∞–≤—Ç–æ—Ä:', '—Ç–µ–≥–∏:'
                ]) and
                not re.match(r'^\d{1,2}\s+\w+\s+\d{4},\s+\d{1,2}:\d{2}$', p_text) and
                not re.match(r'^[–ê-–Ø–Ü–Ñ][–∞-—è—ñ—î—ó]+\s+[–ê-–Ø–Ü–Ñ][–∞-—è—ñ—î—ó]+,\s*getty images$', p_text, re.IGNORECASE)
            ):
                meaningful_paragraphs.append(p_text)
    
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        main_content = ' '.join(meaningful_paragraphs)
        print(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(main_content)} —Å–∏–º–≤–æ–ª–æ–≤ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")
        print(f"üìä –ò–∑ {len(meaningful_paragraphs)} —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
        if main_content:
            print(f"üîç –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤: {main_content[:200]}...")
        return main_content
    
    def get_full_article_data(self, news_item, since_time: Optional[datetime] = None):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å–∏ —Å –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤—Ä–µ–º–µ–Ω–∏"""
        url = news_item['url']
        soup = self.get_page_content(url)
        if not soup:
            return None
        try:
            if since_time:
                publish_time = self.estimate_article_publish_time(soup, url)
                if publish_time and publish_time <= since_time:
                    print(f"‚è∞ –°—Ç–∞—Ç—å—è –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ {publish_time.strftime('%H:%M %d.%m')} - —Å—Ç–∞—Ä–∞—è")
                    return None
            else:
                publish_time = self.estimate_article_publish_time(soup, url)
            print("üìÑ –ò–∑–≤–ª–µ–∫–∞–µ–º –¢–û–õ–¨–ö–û –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏...")
            clean_content = self.extract_clean_article_content(soup)
            word_count = self.count_words(clean_content)
            print(f"üìä –¢–û–ß–ù–û–ï –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {word_count}")
            preview = clean_content[:200] + "..." if len(clean_content) > 200 else clean_content
            print(f"üîç –ü–µ—Ä–≤—ã–µ —Å–∏–º–≤–æ–ª—ã: {preview}")
            if word_count > 500:
                print(f"üìè –°—Ç–∞—Ç—å—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è ({word_count} —Å–ª–æ–≤ > 500) - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return None
            print(f"‚úÖ –°—Ç–∞—Ç—å—è –ø–æ–¥—Ö–æ–¥–∏—Ç ({word_count} —Å–ª–æ–≤ ‚â§ 500)")
            summary = self.create_summary(clean_content, news_item['title'])
            image_url = self.extract_main_image(soup, url)
            return {
                'title': news_item['title'],
                'url': url,
                'content': clean_content,
                'summary': summary,
                'image_url': image_url,
                'publish_time': publish_time,
                'word_count': word_count
            }
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {url}: {e}")
            return None
    
    def extract_article_content(self, soup):
        """–£–°–¢–ê–†–ï–í–®–ò–ô –º–µ—Ç–æ–¥ - —Ç–µ–ø–µ—Ä—å –≤—ã–∑—ã–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –º–µ—Ç–æ–¥"""
        return self.extract_clean_article_content(soup)
    
    def create_summary(self, content, title):
        """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É"""
        if not content:
            return title
        sentences = re.split(r'[.!?]+', content)
        meaningful_sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        if meaningful_sentences:
            summary = '. '.join(meaningful_sentences[:2])
            return summary + '.' if not summary.endswith('.') else summary
        return content[:200] + '...' if len(content) > 200 else content
    
    def extract_main_image(self, soup, base_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏"""
        image_selectors = [
            'meta[property="og:image"]',
            '.article-image img',
            '.news-image img',
            'article img',
            '.content img:first-of-type',
            '.main-image img',
            '.post-image img'
        ]
        for selector in image_selectors:
            if 'meta' in selector:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('content', '')
            else:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('src', '') or img_elem.get('data-src', '')
            if image_url:
                full_image_url = urljoin(base_url, image_url)
                if not any(small in image_url.lower() for small in ['icon', 'logo', 'thumb', 'avatar']):
                    return full_image_url
        return ''
    
    def get_latest_news(self, since_time: Optional[datetime] = None):
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' —Å —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        print("üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É Football.ua...")
        if since_time:
            print(f"üïí –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å {since_time.strftime('%H:%M %d.%m.%Y')}")
        soup = self.get_page_content(self.base_url)
        if not soup:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return []
        print("üéØ –ò—â–µ–º –±–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'...")
        golovne_section = self.find_golovne_za_dobu_section(soup)
        if not golovne_section:
            print("‚ùå –ë–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        print("üì∞ –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞...")
        news_items = self.extract_news_from_section(golovne_section, since_time)
        if not news_items:
            print("‚ùå –ù–æ–≤–æ—Å—Ç–∏ –≤ –±–ª–æ–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –±–ª–æ–∫–µ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'")
        full_articles = []
        consecutive_old_articles = 0
        for i, news_item in enumerate(news_items, 1):
            print(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å {i}/{len(news_items)}: {news_item['title'][:50]}...")
            article_data = self.get_full_article_data(news_item, since_time)
            if article_data is None:
                if since_time:
                    soup_temp = self.get_page_content(news_item['url'])
                    if soup_temp:
                        publish_time = self.estimate_article_publish_time(soup_temp, news_item['url'])
                        if publish_time and publish_time <= since_time:
                            consecutive_old_articles += 1
                            print(f"‚è∞ –°—Ç–∞—Ä–∞—è —Å—Ç–∞—Ç—å—è #{consecutive_old_articles} –ø–æ–¥—Ä—è–¥ (–≤—Ä–µ–º—è: {publish_time.strftime('%H:%M %d.%m')})")
                            if consecutive_old_articles >= self.max_consecutive_old:
                                print(f"üö´ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: {self.max_consecutive_old} —Å—Ç–∞—Ç—å–∏ –ø–æ–¥—Ä—è–¥ –æ–∫–∞–∑–∞–ª–∏—Å—å —Å—Ç–∞—Ä—ã–º–∏ - –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—Å—Ç–∞–ª—å–Ω—ã—Ö")
                                print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {len(news_items) - i} –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å—Ç–∞—Ç–µ–π")
                                break
                            continue
                        else:
                            consecutive_old_articles = 0
                            print(f"‚è≠Ô∏è –°—Ç–∞—Ç—å—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ –¥–ª–∏–Ω–µ/—Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                            continue
                    else:
                        consecutive_old_articles = 0
                        print(f"‚è≠Ô∏è –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                        continue
                else:
                    print(f"‚è≠Ô∏è –°—Ç–∞—Ç—å—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
            consecutive_old_articles = 0
            full_articles.append(article_data)
            print(f"‚úÖ –°—Ç–∞—Ç—å—è –¥–æ–±–∞–≤–ª–µ–Ω–∞: {article_data['title'][:50]}...")
            time.sleep(1)
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(full_articles)} –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å—Ç–∞—Ç–µ–π")
        return full_articles

def get_latest_news(since_time: Optional[datetime] = None):
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    parser = FootballUATargetedParser()
    if since_time:
        since_time_buffered = since_time - timedelta(minutes=1)
        articles = parser.get_latest_news(since_time_buffered)
    else:
        articles = parser.get_latest_news()
    result = []
    for article in articles:
        result.append({
            'title': article['title'],
            'link': article['url'],
            'url': article['url'],
            'summary': article['summary'],
            'image_url': article['image_url'],
            'content': article['content'],
            'publish_time': article.get('publish_time'),
            'word_count': article.get('word_count'),
            'source': 'Football.ua'
        })
    return result

def test_targeted_parser():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞"""
    print("üéØ –¢–ï–°–¢–ò–†–£–ï–ú –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–ê–†–°–ï–† –î–õ–Ø –ë–õ–û–ö–ê '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'")
    print("=" * 60)
    print("\nüìã –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
    parser = FootballUATargetedParser(max_consecutive_old=2)
    articles = parser.get_latest_news()
    if articles:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        for i, article in enumerate(articles, 1):
            publish_time = article.get('publish_time')
            word_count = article.get('word_count', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            print(f"   üì∞ {i}. {article['title'][:50]}... ({time_str}, {word_count} —Å–ª–æ–≤)")
    print(f"\nüìã –¢–µ—Å—Ç 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –º–∏–Ω—É—Ç (—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π)")
    since_time = datetime.now(KIEV_TZ) - timedelta(minutes=30)
    recent_articles = parser.get_latest_news(since_time)
    if recent_articles:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(recent_articles)} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        for i, article in enumerate(recent_articles, 1):
            publish_time = article.get('publish_time')
            word_count = article.get('word_count', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            print(f"   üì∞ {i}. {article['title'][:50]}... ({time_str}, {word_count} —Å–ª–æ–≤)")
    else:
        print("üî≠ –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –º–∏–Ω—É—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    print(f"\nüìã –¢–µ—Å—Ç 3: –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (max_consecutive_old=1)")
    aggressive_parser = FootballUATargetedParser(max_consecutive_old=1)
    since_time_old = datetime.now(KIEV_TZ) - timedelta(hours=2)
    aggressive_articles = aggressive_parser.get_latest_news(since_time_old)
    print(f"üìä –° –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –Ω–∞–π–¥–µ–Ω–æ: {len(aggressive_articles)} —Å—Ç–∞—Ç–µ–π")
    print(f"\nüìã –¢–µ—Å—Ç 4: –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ —Å–ª–æ–≤")
    test_texts = [
        "–≠—Ç–æ –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –∏–∑ –ø—è—Ç–∏ —Å–ª–æ–≤.",
        "–¢–µ–∫—Å—Ç —Å    –ª–∏—à–Ω–∏–º–∏   –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏ –∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è!!!",
        "<p>HTML —Ç–µ–∫—Å—Ç</p> —Å <strong>—Ç–µ–≥–∞–º–∏</strong> –∏ –æ–±—ã—á–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º.",
        ""
    ]
    for test_text in test_texts:
        word_count = parser.count_words(test_text)
        print(f"   üìù \"{test_text[:30]}...\" ‚Üí {word_count} —Å–ª–æ–≤")
    print("\nüöÄ –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:")
    print("   ‚úÖ –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –≤ –Ω–∞—á–∞–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    print("   ‚úÖ –ü—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ N —Å—Ç–∞—Ä—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ–¥—Ä—è–¥")
    print("   ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä max_consecutive_old")
    print("   ‚úÖ –≠–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤")
    print("   ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤ –≤ —á–∏—Å—Ç–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ")

if __name__ == "__main__":
    test_targeted_parser()
