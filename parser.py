import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from zoneinfo import ZoneInfo

KIEV_TZ = ZoneInfo("Europe/Kiev")

class FootballUATargetedParser:
    def __init__(self):
        self.base_url = "https://football.ua/"
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "uk-UA,uk;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
        })
    
    def get_page_content(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            return BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None
    
    def find_golovne_za_dobu_section(self, soup):
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –±–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'"""
        
        # –°–ø–æ—Å–æ–± 1: –ü–æ–∏—Å–∫ –ø–æ —Ç–æ—á–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
        header_texts = [
            "–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£",
            "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É", 
            "–ì–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
        ]
        
        for header_text in header_texts:
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å —Ç–∞–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º
            header_element = soup.find(text=re.compile(header_text, re.I))
            if header_element:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫: '{header_text}'")
                
                # –ù–∞—Ö–æ–¥–∏–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å–µ–∫—Ü–∏–∏
                parent = header_element.parent
                while parent and parent.name not in ['section', 'div', 'article']:
                    parent = parent.parent
                
                if parent:
                    # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å–æ —Å–ø–∏—Å–∫–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π
                    news_container = parent.find_next(['div', 'ul', 'section'])
                    if news_container:
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
                        return news_container
                    else:
                        return parent
        
        # –°–ø–æ—Å–æ–± 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —ç—Ç–æ—Ç –±–ª–æ–∫
        possible_selectors = [
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏/–∫–æ–ª–æ–Ω–∫–∏
            '.sidebar',
            '.right-column', 
            '.side-block',
            '.news-sidebar',
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –±–ª–æ–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
            '.daily-news',
            '.main-today',
            '.today-block',
            '.golovne',
            
            # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '[class*="today"]',
            '[class*="daily"]',
            '[class*="golovne"]'
        ]
        
        for selector in possible_selectors:
            elements = soup.select(selector)
            for element in elements:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —ç–ª–µ–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
                if re.search(r'–≥–æ–ª–æ–≤–Ω–µ.*–∑–∞.*–¥–æ–±—É', element.get_text(), re.I):
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                    return element
        
        # –°–ø–æ—Å–æ–± 3: –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–∏
        print("‚ö†Ô∏è  –ò—â–µ–º –±–ª–æ–∫ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
        
        # –ò—â–µ–º –≤—Å–µ –±–ª–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
        all_divs = soup.find_all(['div', 'section'], class_=True)
        
        for div in all_divs:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –±–ª–æ–∫–µ —Ç–µ–∫—Å—Ç "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
            div_text = div.get_text().lower()
            if '–≥–æ–ª–æ–≤–Ω–µ' in div_text and '–¥–æ–±—É' in div_text:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ —Å —Ç–µ–∫—Å—Ç–æ–º '–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É'")
                return div
        
        print("‚ùå –ë–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None
    
    def extract_news_from_section(self, section, since_time: Optional[datetime] = None):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        if not section:
            return []
        
        news_links = []
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ —Å–µ–∫—Ü–∏–∏
        all_links = section.find_all('a', href=True)
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –≤ —Å–µ–∫—Ü–∏–∏")
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            if self.is_news_link(href) and len(text) > 10:
                full_url = urljoin(self.base_url, href)
                news_links.append({
                    'title': text,
                    'url': full_url,
                    'href': href
                })
                
                print(f"üì∞ –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {text[:50]}...")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        seen_urls = set()
        unique_news = []
        
        for news in news_links:
            if news['url'] not in seen_urls:
                unique_news.append(news)
                seen_urls.add(news['url'])
        
        return unique_news
    
    def is_news_link(self, href: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–æ–≤–æ—Å—Ç—å—é"""
        return bool(re.match(r'/(?:[\w-]+/)?\d{6}-[\w-]+\.html$', href))
    
    def parse_date_string(self, date_str: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É –¥–∞—Ç—ã —Å —É—á–µ—Ç–æ–º —É–∫—Ä–∞–∏–Ω—Å–∫–∏—Ö –º–µ—Å—è—Ü–µ–≤"""
        if not date_str:
            return None
        month_map = {
            '—Å—ñ—á–Ω—è': '01', '–ª—é—Ç–æ–≥–æ': '02', '–±–µ—Ä–µ–∑–Ω—è': '03',
            '–∫–≤—ñ—Ç–Ω—è': '04', '—Ç—Ä–∞–≤–Ω—è': '05', '—á–µ—Ä–≤–Ω—è': '06',
            '–ª–∏–ø–Ω—è': '07', '—Å–µ—Ä–ø–Ω—è': '08', '–≤–µ—Ä–µ—Å–Ω—è': '09',
            '–∂–æ–≤—Ç–Ω—è': '10', '–ª–∏—Å—Ç–æ–ø–∞–¥–∞': '11', '–≥—Ä—É–¥–Ω—è': '12'
        }
        for uk_month, num in month_map.items():
            if uk_month in date_str.lower():
                date_str = re.sub(uk_month, num, date_str, flags=re.I)
                break
        date_formats = [
            '%d %m %Y %H:%M', '%d.%m.%Y %H:%M', '%d %m %Y',
            '%Y-%m-%d %H:%M', '%Y-%m-%dT%H:%M:%S', '%d.%m.%Y'
        ]
        for fmt in date_formats:
            try:
                dt = datetime.strptime(date_str.strip(), fmt)
                return dt.replace(tzinfo=KIEV_TZ)
            except ValueError:
                continue
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: '{date_str}'")
        return None
    
    def get_article_publish_time(self, soup, url) -> Optional[datetime]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏"""
        print(f"üïí –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –¥–ª—è: {url}")
        
        # –ù–æ–≤—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã: meta-—Ç–µ–≥–∏ —Å–Ω–∞—á–∞–ª–∞
        meta_selectors = [
            ('meta[property="og:published_time"]', 'content'),
            ('meta[name="published"]', 'content'),
            ('meta[itemprop="datePublished"]', 'content'),
            ('time[datetime]', 'datetime'),  # –î–ª—è <time datetime="...">
        ]
        for selector, attr in meta_selectors:
            elem = soup.select_one(selector)
            if elem:
                date_str = elem.get(attr, '').strip()
                if date_str:
                    print(f"üìÖ –ù–∞–π–¥–µ–Ω meta/—Ç–µ–≥: '{date_str}'")
                    parsed_date = self.parse_date_string(date_str)
                    if parsed_date:
                        return parsed_date
        
        # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –∫–∞–∫ fallback
        date_selectors = [
            '.news-date',
            '.article-date',
            '.publish-date',
            '[class*="date"]',
        ]
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_str = date_elem.get_text(strip=True)
                print(f"üìÖ –ù–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç –¥–∞—Ç—ã –≤ {selector}: '{date_str}'")
                parsed_date = self.parse_date_string(date_str)
                if parsed_date:
                    return parsed_date
        
        # Fallback: –¢–µ–∫—É—â–∞—è –¥–∞—Ç–∞ –º–∏–Ω—É—Å 5 –º–∏–Ω—É—Ç, –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
        print("‚ö†Ô∏è –î–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
        return datetime.now(KIEV_TZ) - timedelta(minutes=5)
    
    def extract_article_content(self, soup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏"""
        content_selectors = [
            '.news-full-content',
            '.article-body',
            '.post-content',
            '[itemprop="articleBody"]',
            '.news-text'
        ]
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                for unwanted in content_div.find_all(['script', 'style', 'ads', 'aside']):
                    unwanted.decompose()
                return ' '.join(content_div.get_text(strip=True).split())
        paragraphs = soup.find_all('p')
        return ' '.join(p.get_text(strip=True) for p in paragraphs)
    
    def extract_article_summary(self, soup, content: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ"""
        meta_summary = soup.select_one('meta[name="description"]')
        if meta_summary:
            return meta_summary.get('content', '')[:300]
        intro = soup.select_one('.news-intro, .lead, p:first-of-type')
        if intro:
            return intro.get_text(strip=True)[:300]
        return content[:300] + '...' if content else ''
    
    def extract_article_image(self, soup, base_url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏"""
        image_selectors = [
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
            '.news-image img',
            '.article-content img:first-of-type',
            '.main-image img',
            '.post-image img'
        ]
        
        for selector in image_selectors:
            if 'meta' in selector:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('content', '')
            else:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('src', '') or img_elem.get('data-src', '')
            
            if image_url:
                full_image_url = urljoin(base_url, image_url)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –º–∞–ª–µ–Ω—å–∫–∞—è –∏–∫–æ–Ω–∫–∞
                if not any(small in image_url.lower() for small in ['icon', 'logo', 'thumb', 'avatar']):
                    return full_image_url
        
        return ''
    
    def get_full_article_data(self, news_item: Dict[str, str], since_time: Optional[datetime] = None) -> Optional[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏"""
        url = news_item['url']
        soup = self.get_page_content(url)
        if not soup:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ç—å—é: {url}")
            return None
        
        publish_time = self.get_article_publish_time(soup, url)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        if since_time and publish_time < since_time:
            print(f"‚è∞ –°—Ç–∞—Ç—å—è —Å—Ç–∞—Ä–∞—è ({publish_time.strftime('%H:%M %d.%m')}), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return None
        
        content = self.extract_article_content(soup)
        
        # –î–æ—Ä–∞–±–æ—Ç–∫–∞: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
        word_count = len(content.split())
        if word_count > 450:
            print(f"üìè –°—Ç–∞—Ç—å—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è ({word_count} —Å–ª–æ–≤), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return None
        
        summary = self.extract_article_summary(soup, content)
        image_url = self.extract_article_image(soup, url)
        
        return {
            'title': news_item['title'],
            'url': url,
            'content': content,
            'summary': summary,
            'image_url': image_url,
            'publish_time': publish_time
        }
    
    def get_latest_news(self, since_time: Optional[datetime] = None):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ - –ø–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        print("üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É Football.ua...")
        
        if since_time:
            print(f"üïí –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å {since_time.strftime('%H:%M %d.%m.%Y')}")
        
        soup = self.get_page_content(self.base_url)
        if not soup:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return []
        
        print("üéØ –ò—â–µ–º –±–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'...")
        golovne_section = self.find_golovne_za_dobu_section(soup)
        
        if not golovne_section:
            print("‚ùå –ë–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        
        print("üì∞ –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞...")
        news_items = self.extract_news_from_section(golovne_section, since_time)
        
        if not news_items:
            print("‚ùå –ù–æ–≤–æ—Å—Ç–∏ –≤ –±–ª–æ–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –±–ª–æ–∫–µ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏
        full_articles = []
        
        for i, news_item in enumerate(news_items, 1):
            print(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å {i}/{len(news_items)}: {news_item['title'][:50]}...")
            
            article_data = self.get_full_article_data(news_item, since_time)
            
            # –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–ª–∏ –¥–ª–∏–Ω–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
            if article_data is None:
                print(f"üõë –ü—Ä–æ–ø—É—Å–∫–∞–µ–º (—Å—Ç–∞—Ä–∞—è/–¥–ª–∏–Ω–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å): {news_item['title'][:50]}...")
                continue
            
            # –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë
            if article_data:
                full_articles.append(article_data)
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(1)
        
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(full_articles)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π")
        return full_articles

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
def get_latest_news(since_time: Optional[datetime] = None):
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    parser = FootballUATargetedParser()
    articles = parser.get_latest_news(since_time)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç, –æ–∂–∏–¥–∞–µ–º—ã–π –æ—Å–Ω–æ–≤–Ω—ã–º –∫–æ–¥–æ–º
    result = []
    for article in articles:
        result.append({
            'title': article['title'],
            'link': article['url'],  # main.py –æ–∂–∏–¥–∞–µ—Ç 'link', –∞ –Ω–µ 'url'
            'url': article['url'],   # –¥–æ–±–∞–≤–ª—è–µ–º –∏ 'url' –¥–ª—è ai_processor
            'summary': article['summary'],
            'image_url': article['image_url'],
            'content': article['content'],  # –í–ê–ñ–ù–û: –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è AI
            'publish_time': article.get('publish_time')  # –ù–û–í–û–ï: –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        })
    
    return result

def test_targeted_parser():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞"""
    print("üéØ –¢–ï–°–¢–ò–†–£–ï–ú –ü–ê–†–°–ï–† –î–õ–Ø –ë–õ–û–ö–ê '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'")
    print("=" * 60)
    
    # –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π (—Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ)
    print("\nüìã –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
    parser = FootballUATargetedParser()
    articles = parser.get_latest_news()
    
    if articles:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        for i, article in enumerate(articles, 1):
            publish_time = article.get('publish_time')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            print(f"   üì∞ {i}. {article['title'][:50]}... ({time_str})")
    
    # –¢–µ—Å—Ç 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    print(f"\nüìã –¢–µ—Å—Ç 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –º–∏–Ω—É—Ç")
    since_time = datetime.now(KIEV_TZ) - timedelta(minutes=30)
    recent_articles = parser.get_latest_news(since_time)
    
    if recent_articles:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(recent_articles)} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        for i, article in enumerate(recent_articles, 1):
            publish_time = article.get('publish_time')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            print(f"   üì∞ {i}. {article['title'][:50]}... ({time_str})")
    else:
        print("üì≠ –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –º–∏–Ω—É—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

if __name__ == "__main__":
    test_targeted_parser()
