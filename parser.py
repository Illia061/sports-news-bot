import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

class FootballUATargetedParser:
    def __init__(self):
        self.base_url = "https://football.ua/"
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "uk-UA,uk;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
        })
    
    def get_page_content(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            return BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None
    
    def find_golovne_za_dobu_section(self, soup):
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –±–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'"""
        
        # –°–ø–æ—Å–æ–± 1: –ü–æ–∏—Å–∫ –ø–æ —Ç–æ—á–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
        header_texts = [
            "–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£",
            "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É", 
            "–ì–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
        ]
        
        for header_text in header_texts:
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å —Ç–∞–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º
            header_element = soup.find(text=re.compile(header_text, re.I))
            if header_element:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫: '{header_text}'")
                
                # –ù–∞—Ö–æ–¥–∏–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å–µ–∫—Ü–∏–∏
                parent = header_element.parent
                while parent and parent.name not in ['section', 'div', 'article']:
                    parent = parent.parent
                
                if parent:
                    # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å–æ —Å–ø–∏—Å–∫–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π
                    news_container = parent.find_next(['div', 'ul', 'section'])
                    if news_container:
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
                        return news_container
                    else:
                        return parent
        
        # –°–ø–æ—Å–æ–± 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —ç—Ç–æ—Ç –±–ª–æ–∫
        possible_selectors = [
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏/–∫–æ–ª–æ–Ω–∫–∏
            '.sidebar',
            '.right-column', 
            '.side-block',
            '.news-sidebar',
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –±–ª–æ–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
            '.daily-news',
            '.main-today',
            '.today-block',
            '.golovne',
            
            # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '[class*="today"]',
            '[class*="daily"]',
            '[class*="golovne"]'
        ]
        
        for selector in possible_selectors:
            elements = soup.select(selector)
            for element in elements:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —ç–ª–µ–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
                if re.search(r'–≥–æ–ª–æ–≤–Ω–µ.*–∑–∞.*–¥–æ–±—É', element.get_text(), re.I):
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                    return element
        
        # –°–ø–æ—Å–æ–± 3: –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–∏
        print("‚ö†Ô∏è  –ò—â–µ–º –±–ª–æ–∫ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
        
        # –ò—â–µ–º –≤—Å–µ –±–ª–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
        all_divs = soup.find_all(['div', 'section'], class_=True)
        
        for div in all_divs:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –±–ª–æ–∫–µ —Ç–µ–∫—Å—Ç "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
            div_text = div.get_text().lower()
            if '–≥–æ–ª–æ–≤–Ω–µ' in div_text and '–¥–æ–±—É' in div_text:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ —Å —Ç–µ–∫—Å—Ç–æ–º '–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É'")
                return div
        
        print("‚ùå –ë–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None
    
    def extract_news_from_section(self, section, since_time: Optional[datetime] = None):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        if not section:
            return []
        
        news_links = []
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ —Å–µ–∫—Ü–∏–∏
        all_links = section.find_all('a', href=True)
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –≤ —Å–µ–∫—Ü–∏–∏")
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            if self.is_news_link(href) and len(text) > 10:
                full_url = urljoin(self.base_url, href)
                news_links.append({
                    'title': text,
                    'url': full_url,
                    'href': href
                })
                
                print(f"üì∞ –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {text[:50]}...")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        seen_urls = set()
        unique_news = []
        
        for news in news_links:
            if news['url'] not in seen_urls:
                unique_news.append(news)
                seen_urls.add(news['url'])
        
        # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –≤—Ä–µ–º—è, –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 (—Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ)
        if since_time:
            print(f"üïí –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å {since_time.strftime('%H:%M %d.%m.%Y')}")
            return unique_news  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        else:
            return unique_news[:5]  # –°—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
    
    def is_news_link(self, href):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π"""
        if not href:
            return False
        
        # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –Ω–∞ football.ua
        news_patterns = [
            r'/news/',
            r'/ukraine/',
            r'/world/',
            r'/europe/',
            r'/england/',
            r'/spain/',
            r'/italy/',
            r'/germany/',
            r'/france/',
            r'/poland/',
            r'/\d+[^/]*\.html'  # –°—Å—ã–ª–∫–∏ —Å ID –Ω–æ–≤–æ—Å—Ç–µ–π
        ]
        
        return any(re.search(pattern, href) for pattern in news_patterns)
    
    def estimate_article_publish_time(self, soup, url: str) -> Optional[datetime]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏"""
        try:
            # –ò—â–µ–º –º–µ—Ç–∞-—Ç–µ–≥–∏ —Å –¥–∞—Ç–æ–π
            meta_selectors = [
                'meta[property="article:published_time"]',
                'meta[name="publish_date"]',
                'meta[name="date"]',
                'meta[property="og:published_time"]'
            ]
            
            for selector in meta_selectors:
                meta_tag = soup.select_one(selector)
                if meta_tag:
                    content = meta_tag.get('content', '')
                    if content:
                        try:
                            # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å ISO —Ñ–æ—Ä–º–∞—Ç
                            return datetime.fromisoformat(content.replace('Z', '+00:00'))
                        except:
                            continue
            
            # –ò—â–µ–º –¥–∞—Ç—É –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            date_selectors = [
                '.article-date',
                '.publish-date',
                '.news-date',
                '.date',
                '.timestamp',
                'time[datetime]'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    date_text = date_elem.get('datetime') or date_elem.get_text(strip=True)
                    if date_text:
                        try:
                            # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
                            if 'T' in date_text:
                                return datetime.fromisoformat(date_text.replace('Z', '+00:00'))
                            # –î–æ–±–∞–≤—å—Ç–µ –∑–¥–µ—Å—å –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                        except:
                            continue
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è
            # (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –Ω–æ–≤–æ—Å—Ç—å —Å–≤–µ–∂–∞—è)
            return datetime.now()
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")
            return datetime.now()
    
    def get_full_article_data(self, news_item, since_time: Optional[datetime] = None):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤—Ä–µ–º–µ–Ω–∏"""
        url = news_item['url']
        soup = self.get_page_content(url)
        
        if not soup:
            return None
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            publish_time = self.estimate_article_publish_time(soup, url)
            
            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –≤—Ä–µ–º—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º
            if since_time and publish_time:
                if publish_time <= since_time:
                    print(f"‚è∞ –°—Ç–∞—Ç—å—è –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ {publish_time.strftime('%H:%M %d.%m')} - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–¥–æ {since_time.strftime('%H:%M %d.%m')})")
                    return None
                else:
                    print(f"‚úÖ –°—Ç–∞—Ç—å—è –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ {publish_time.strftime('%H:%M %d.%m')} - –Ω–æ–≤–∞—è!")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content = self.extract_article_content(soup)
            
            # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É
            summary = self.create_summary(content, news_item['title'])
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_url = self.extract_main_image(soup, url)
            
            return {
                'title': news_item['title'],
                'url': url,
                'content': content,
                'summary': summary,
                'image_url': image_url,
                'publish_time': publish_time
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {url}: {e}")
            return None
    
    def extract_article_content(self, soup):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ (–†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –¥–ª—è AI)"""
        content_selectors = [
            '.article-content',
            '.news-content',
            '.post-content',
            '.content',
            'article',
            '.main-text',
            '.article-body',
            '.news-body'
        ]
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å—Ç–∞—Ç—å–∏
        main_content = ""
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # –£–±–∏—Ä–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                for unwanted in content_elem.find_all(['script', 'style', 'iframe', 'div[class*="ad"]', 'div[class*="banner"]']):
                    unwanted.decompose()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã (–Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è AI)
                paragraphs = content_elem.find_all('p')
                if paragraphs:
                    main_content = '\n'.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
                    break
        
        # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—â–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        if not main_content:
            all_paragraphs = soup.find_all('p')
            meaningful_paragraphs = []
            
            for p in all_paragraphs:
                text = p.get_text(strip=True)
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                if (len(text) > 30 and 
                    not any(skip in text.lower() for skip in ['cookie', '—Ä–µ–∫–ª–∞–º–∞', '–ø—ñ–¥–ø–∏—Å', '—Ñ–æ—Ç–æ', '–¥–∂–µ—Ä–µ–ª–æ'])):
                    meaningful_paragraphs.append(text)
            
            # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è AI (–¥–æ 1500 —Å–∏–º–≤–æ–ª–æ–≤)
            main_content = '\n'.join(meaningful_paragraphs)
            if len(main_content) > 1500:
                sentences = re.split(r'[.!?]+', main_content)
                trimmed_content = ""
                for sentence in sentences:
                    if len(trimmed_content + sentence) < 1500:
                        trimmed_content += sentence + ". "
                    else:
                        break
                main_content = trimmed_content.strip()
        
        print(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(main_content)} —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
        return main_content
    
    def create_summary(self, content, title):
        """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É"""
        if not content:
            return title
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', content)
        meaningful_sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        
        if meaningful_sentences:
            summary = '. '.join(meaningful_sentences[:2])
            return summary + '.' if not summary.endswith('.') else summary
        
        return content[:200] + '...' if len(content) > 200 else content
    
    def extract_main_image(self, soup, base_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏"""
        image_selectors = [
            'meta[property="og:image"]',
            '.article-image img',
            '.news-image img',
            'article img',
            '.content img:first-of-type',
            '.main-image img',
            '.post-image img'
        ]
        
        for selector in image_selectors:
            if 'meta' in selector:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('content', '')
            else:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('src', '') or img_elem.get('data-src', '')
            
            if image_url:
                full_image_url = urljoin(base_url, image_url)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –º–∞–ª–µ–Ω—å–∫–∞—è –∏–∫–æ–Ω–∫–∞
                if not any(small in image_url.lower() for small in ['icon', 'logo', 'thumb', 'avatar']):
                    return full_image_url
        
        return ''
    
    def get_latest_news(self, since_time: Optional[datetime] = None):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ - –ø–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        print("üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É Football.ua...")
        
        if since_time:
            print(f"üïí –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å {since_time.strftime('%H:%M %d.%m.%Y')}")
        
        soup = self.get_page_content(self.base_url)
        if not soup:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return []
        
        print("üéØ –ò—â–µ–º –±–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'...")
        golovne_section = self.find_golovne_za_dobu_section(soup)
        
        if not golovne_section:
            print("‚ùå –ë–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        
        print("üì∞ –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞...")
        news_items = self.extract_news_from_section(golovne_section, since_time)
        
        if not news_items:
            print("‚ùå –ù–æ–≤–æ—Å—Ç–∏ –≤ –±–ª–æ–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –±–ª–æ–∫–µ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏
        full_articles = []
        
        for i, news_item in enumerate(news_items, 1):
            print(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å {i}/{len(news_items)}: {news_item['title'][:50]}...")
            
            article_data = self.get_full_article_data(news_item, since_time)
            
            # –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë
            if article_data:
                full_articles.append(article_data)
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(1)
        
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(full_articles)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π")
        return full_articles

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
def get_latest_news(since_time: Optional[datetime] = None):
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    parser = FootballUATargetedParser()
    articles = parser.get_latest_news(since_time)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç, –æ–∂–∏–¥–∞–µ–º—ã–π –æ—Å–Ω–æ–≤–Ω—ã–º –∫–æ–¥–æ–º
    result = []
    for article in articles:
        result.append({
            'title': article['title'],
            'link': article['url'],  # main.py –æ–∂–∏–¥–∞–µ—Ç 'link', –∞ –Ω–µ 'url'
            'url': article['url'],   # –¥–æ–±–∞–≤–ª—è–µ–º –∏ 'url' –¥–ª—è ai_processor
            'summary': article['summary'],
            'image_url': article['image_url'],
            'content': article['content'],  # –í–ê–ñ–ù–û: –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è AI
            'publish_time': article.get('publish_time')  # –ù–û–í–û–ï: –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        })
    
    return result

def test_targeted_parser():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞"""
    print("üéØ –¢–ï–°–¢–ò–†–£–ï–ú –ü–ê–†–°–ï–† –î–õ–Ø –ë–õ–û–ö–ê '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'")
    print("=" * 60)
    
    # –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π (—Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ)
    print("\nüìã –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
    parser = FootballUATargetedParser()
    articles = parser.get_latest_news()
    
    if articles:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        for i, article in enumerate(articles, 1):
            print(f"   üì∞ {i}. {article['title'][:50]}...")
    
    # –¢–µ—Å—Ç 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    print(f"\nüìã –¢–µ—Å—Ç 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –º–∏–Ω—É—Ç")
    since_time = datetime.now() - timedelta(minutes=30)
    recent_articles = parser.get_latest_news(since_time)
    
    if recent_articles:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(recent_articles)} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        for i, article in enumerate(recent_articles, 1):
            publish_time = article.get('publish_time', datetime.now())
            print(f"   üì∞ {i}. {article['title'][:50]}... ({publish_time.strftime('%H:%M')})")
    else:
        print("üì≠ –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –º–∏–Ω—É—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

if __name__ == "__main__":
    test_targeted_parser()
