
import requests
from bs4 import BeautifulSoup
import re
import time
from urllib.parse import urljoin, urlparse

class FootballUAParser:
    def __init__(self):
        self.base_url = "https://football.ua/"
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "uk-UA,uk;q=0.9,en;q=0.8,ru;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Cache-Control": "max-age=0"
        })
    
    def get_page_content(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return BeautifulSoup(response.text, "html.parser")
        except requests.RequestException as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
            return None
    
    def find_main_today_section(self, soup):
        """–ò—â–µ—Ç —Ä–∞–∑–¥–µ–ª '–ì–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É' —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏"""
        
        # –°–ø–æ—Å–æ–± 1: –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
        possible_headers = [
            "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É", "–≥–æ–ª–æ–≤–Ω–µ", "–∑–∞ –¥–æ–±—É", 
            "–≥–æ–ª–æ–≤–Ω—ñ –Ω–æ–≤–∏–Ω–∏", "—Ç–æ–ø –Ω–æ–≤–∏–Ω–∏", "–Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ"
        ]
        
        for header_text in possible_headers:
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏
            header = soup.find(text=re.compile(header_text, re.I))
            if header:
                # –ò—â–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
                section = header.find_parent(['section', 'div', 'article'])
                if section:
                    print(f"–ù–∞–π–¥–µ–Ω–∞ —Å–µ–∫—Ü–∏—è –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É: '{header_text}'")
                    return section
        
        # –°–ø–æ—Å–æ–± 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º –∏ –∞—Ç—Ä–∏–±—É—Ç–∞–º
        section_selectors = [
            {'class': re.compile(r'main.*today|today.*main|–≥–æ–ª–æ–≤–Ω–µ|main.*news', re.I)},
            {'data-section': re.compile(r'main|today|–≥–æ–ª–æ–≤–Ω–µ', re.I)},
            {'id': re.compile(r'main.*today|today.*main|–≥–æ–ª–æ–≤–Ω–µ', re.I)}
        ]
        
        for selector in section_selectors:
            section = soup.find(['section', 'div'], attrs=selector)
            if section:
                print(f"–ù–∞–π–¥–µ–Ω–∞ —Å–µ–∫—Ü–∏—è –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É: {selector}")
                return section
        
        # –°–ø–æ—Å–æ–± 3: –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –±–ª–æ–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
        main_selectors = [
            'section.main-news-feed',
            'div.main-news-feed', 
            '.main-news',
            '.news-main',
            'main .news',
            '.main-content',
            '#main-content'
        ]
        
        for selector in main_selectors:
            section = soup.select_one(selector)
            if section:
                print(f"–ù–∞–π–¥–µ–Ω–∞ –æ—Å–Ω–æ–≤–Ω–∞—è —Å–µ–∫—Ü–∏—è: {selector}")
                return section
        
        print("–°–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è —Å–µ–∫—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç")
        return soup
    
    def extract_news_links(self, section):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ —Å–µ–∫—Ü–∏–∏"""
        articles = []
        
        # –†–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Å—Å—ã–ª–æ–∫
        link_patterns = [
            'a[href*="/news/"]',
            'a[href*="/ukraine/"]',
            'a[href*="/world/"]',
            'a.main-news-feed__link',
            'a.news-link',
            'a.article-link',
            '.news-item a',
            '.post-title a',
            'article a',
            'h1 a, h2 a, h3 a, h4 a'  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å —Å—Å—ã–ª–∫–∞–º–∏
        ]
        
        found_links = set()  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        
        for pattern in link_patterns:
            links = section.select(pattern)
            print(f"–ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É: {pattern}")
            
            for link in links:
                href = link.get('href')
                if not href or href in found_links:
                    continue
                    
                # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title = self.extract_title(link)
                if not title or len(title) < 10:
                    continue
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é —Å—Å—ã–ª–∫—É
                full_url = urljoin(self.base_url, href)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–æ–≤–æ—Å—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞
                if self.is_news_link(full_url):
                    articles.append({
                        'title': title.strip(),
                        'link': full_url,
                        'pattern': pattern
                    })
                    found_links.add(href)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ /news/ –ø–µ—Ä–≤—ã–º–∏)
        articles.sort(key=lambda x: (
            0 if '/news/' in x['link'] else 1,
            len(x['title'])  # –ë–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤–ø–µ—Ä–µ–¥
        ))
        
        return articles[:5]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5
    
    def extract_title(self, link_element):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞ —Å—Å—ã–ª–∫–∏"""
        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –∏–∑ —Å–∞–º–æ–π —Å—Å—ã–ª–∫–∏
        title = link_element.get_text(strip=True)
        
        if title:
            return title
        
        # –ï—Å–ª–∏ –≤ —Å—Å—ã–ª–∫–µ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞, –∏—â–µ–º –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
        for parent in [link_element.parent, link_element.parent.parent if link_element.parent else None]:
            if parent:
                title = parent.get_text(strip=True)
                if title and title != link_element.get_text(strip=True):
                    return title
        
        # –ò—â–µ–º title –∏–ª–∏ alt –∞—Ç—Ä–∏–±—É—Ç—ã
        title = link_element.get('title') or link_element.get('alt')
        if title:
            return title
        
        return ""
    
    def is_news_link(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π"""
        if not url:
            return False
            
        parsed = urlparse(url)
        path = parsed.path.lower()
        
        # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        news_sections = ['/news/', '/ukraine/', '/world/', '/europe/', '/championships/']
        
        return any(section in path for section in news_sections)
    
    def get_latest_news(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π"""
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É Football.ua...")
        
        soup = self.get_page_content(self.base_url)
        if not soup:
            return []
        
        print("–ò—â–µ–º —Ä–∞–∑–¥–µ–ª '–ì–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É'...")
        main_section = self.find_main_today_section(soup)
        
        if not main_section:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω—É—é —Å–µ–∫—Ü–∏—é –Ω–æ–≤–æ—Å—Ç–µ–π")
            return []
        
        print("–ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏...")
        articles = self.extract_news_links(main_section)
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
        return articles

def main():
    parser = FootballUAParser()
    
    print("=" * 60)
    print("–ü–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π Football.ua - —Ä–∞–∑–¥–µ–ª '–ì–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É'")
    print("=" * 60)
    
    try:
        news = parser.get_latest_news()
        
        if news:
            print(f"\nüì∞ –ù–∞–π–¥–µ–Ω–æ {len(news)} –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π:")
            print("=" * 60)
            
            for i, article in enumerate(news, 1):
                print(f"{i}. {article['title']}")
                print(f"   üîó {article['link']}")
                print(f"   üìç –ù–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑: {article['pattern']}")
                print("-" * 60)
        else:
            print("\n‚ùå –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
            print("–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
            print("- –ò–∑–º–µ–Ω–∏–ª–∞—Å—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–π—Ç–∞")
            print("- –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º")
            print("- –°–∞–π—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\nüí• –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()
        articles.append({"title": title, "link": link})

    return articles

