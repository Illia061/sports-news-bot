import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time

def clean_text(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    if not text or not isinstance(text, str):
        return ""
    
    # –£–¥–∞–ª—è–µ–º —Ä–∞–∑–±–∏—Ç—ã–π —Ç–µ–∫—Å—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ò. –Ω. —à. –µ.)
    text = re.sub(r'(\w)\.\s*', r'\1', text)
    text = re.sub(r'\s*\.\s*', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'[^\x20-\x7E–∞-—è–ê-–Ø—ë–Å0-9.,!?:;\-]', '', text)
    return text

class FootballUATargetedParser:
    def __init__(self):
        self.base_url = "https://football.ua/"
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html floats and doubles,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "uk-UA,uk;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
        })
    
    def get_page_content(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'  # –Ø–≤–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8
            return BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None
    
    def find_golovne_za_dobu_section(self, soup):
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –±–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'"""
        header_texts = [
            "–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£",
            "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É", 
            "–ì–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
        ]
        
        for header_text in header_texts:
            header_element = soup.find(text=re.compile(header_text, re.I))
            if header_element:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫: '{header_text}'")
                parent = header_element.parent
                while parent and parent.name not in ['section', 'div', 'article']:
                    parent = parent.parent
                
                if parent:
                    news_container = parent.find_next(['div', 'ul', 'section'])
                    if news_container:
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
                        return news_container
                    else:
                        return parent
        
        possible_selectors = [
            '.sidebar',
            '.right-column', 
            '.side-block',
            '.news-sidebar',
            '.daily-news',
            '.main-today',
            '.today-block',
            '.golovne',
            '[class*="today"]',
            '[class*="daily"]',
            '[class*="golovne"]'
        ]
        
        for selector in possible_selectors:
            elements = soup.select(selector)
            for element in elements:
                if re.search(r'–≥–æ–ª–æ–≤–Ω–µ.*–∑–∞.*–¥–æ–±—É', element.get_text(), re.I):
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                    return element
        
        print("‚ö†Ô∏è  –ò—â–µ–º –±–ª–æ–∫ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
        all_divs = soup.find_all(['div', 'section'], class_=True)
        
        for div in all_divs:
            div_text = div.get_text().lower()
            if '–≥–æ–ª–æ–≤–Ω–µ' in div_text and '–¥–æ–±—É' in div_text:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ —Å —Ç–µ–∫—Å—Ç–æ–º '–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É'")
                return div
        
        print("‚ùå –ë–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None
    
    def extract_news_from_section(self, section):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏"""
        if not section:
            return []
        
        news_links = []
        all_links = section.find_all('a', href=True)
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –≤ —Å–µ–∫—Ü–∏–∏")
        
        for link in all_links:
            href = link.get('href', '')
            text = clean_text(link.get_text(strip=True))
            
            if self.is_news_link(href) and len(text) > 10:
                full_url = urljoin(self.base_url, href)
                news_links.append({
                    'title': text,
                    'url': full_url,
                    'href': href
                })
                print(f"üì∞ –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {text[:50]}...")
        
        seen_urls = set()
        unique_news = []
        
        for news in news_links:
            if news['url'] not in seen_urls:
                unique_news.append(news)
                seen_urls.add(news['url'])
        
        return unique_news[:5]
    
    def is_news_link(self, href):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π"""
        if not href:
            return False
        
        news_patterns = [
            r'/news/',
            r'/ukraine/',
            r'/world/',
            r'/europe/',
            r'/england/',
            r'/spain/',
            r'/italy/',
            r'/germany/',
            r'/france/',
            r'/poland/',
            r'/\d+[^/]*\.html'
        ]
        
        return any(re.search(pattern, href) for pattern in news_patterns)
    
    def get_full_article_data(self, news_item):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏"""
        url = news_item['url']
        soup = self.get_page_content(url)
        
        if not soup:
            return {
                'title': news_item['title'],
                'url': url,
                'content': '',
                'summary': news_item['title'],
                'image_url': ''
            }
        
        try:
            content = self.extract_article_content(soup)
            summary = self.create_summary(content, news_item['title'])
            image_url = self.extract_main_image(soup, url)
            
            return {
                'title': news_item['title'],
                'url': url,
                'content': clean_text(content),
                'summary': clean_text(summary),
                'image_url': image_url
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {url}: {e}")
            return {
                'title': news_item['title'],
                'url': url,
                'content': '',
                'summary': news_item['title'],
                'image_url': ''
            }
    
    def extract_article_content(self, soup):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏"""
        content_selectors = [
            '.article-content',
            '.news-content',
            '.post-content',
            '.content',
            'article',
            '.main-text'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                paragraphs = content_elem.find_all('p')
                if paragraphs:
                    return '\n'.join([clean_text(p.get_text(strip=True)) for p in paragraphs[:4]])
        
        paragraphs = soup.find_all('p')
        meaningful_paragraphs = [
            clean_text(p.get_text(strip=True)) for p in paragraphs 
            if len(p.get_text(strip=True)) > 30
        ]
        
        return '\n'.join(meaningful_paragraphs[:3]) if meaningful_paragraphs else ''
    
    def create_summary(self, content, title):
        """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É"""
        if not content:
            return clean_text(title)
        
        content = clean_text(content)
        sentences = [s.strip() for s in content.split('. ') if s.strip()]
        summary = '. '.join(sentences[:2]) + '.' if sentences else content[:200] + '...'
        return clean_text(summary)
    
    def extract_main_image(self, soup, base_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏"""
        image_selectors = [
            'meta[property="og:image"]',
            '.article-image img',
            '.news-image img',
            'article img',
            '.content img:first-of-type'
        ]
        
        for selector in image_selectors:
            if 'meta' in selector:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('content', '')
            else:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('src', '') or img_elem.get('data-src', '')
            
            if image_url:
                full_image_url = urljoin(base_url, image_url)
                if not any(small in image_url.lower() for small in ['icon', 'logo', 'thumb', 'avatar']):
                    return full_image_url
        
        return ''
    
    def get_latest_news(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ - –ø–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'"""
        print("üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É Football.ua...")
        
        soup = self.get_page_content(self.base_url)
        if not soup:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return []
        
        print("üéØ –ò—â–µ–º –±–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'...")
        golovne_section = self.find_golovne_za_dobu_section(soup)
        
        if not golovne_section:
            print("‚ùå –ë–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        
        print("üì∞ –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞...")
        news_items = self.extract_news_from_section(golovne_section)
        
        if not news_items:
            print("‚ùå –ù–æ–≤–æ—Å—Ç–∏ –≤ –±–ª–æ–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –±–ª–æ–∫–µ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'")
        
        full_articles = []
        
        for i, news_item in enumerate(news_items, 1):
            print(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å {i}/{len(news_items)}: {news_item['title'][:50]}...")
            article_data = self.get_full_article_data(news_item)
            full_articles.append(article_data)
            time.sleep(1)
        
        return full_articles

def get_latest_news():
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    parser = FootballUATargetedParser()
    articles = parser.get_latest_news()
    
    result = []
    for article in articles:
        result.append({
            'title': article['title'],
            'link': article['url'],
            'summary': article['summary'],
            'image_url': article['image_url'],
            'content': article['content']
        })
    
    return result

def test_targeted_parser():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞"""
    print("üéØ –¢–ï–°–¢–ò–†–£–ï–ú –ü–ê–†–°–ï–† –î–õ–Ø –ë–õ–û–ö–ê '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'")
    print("=" * 60)
    
    parser = FootballUATargetedParser()
    articles = parser.get_latest_news()
    
    if articles:
        print(f"\n‚úÖ –£–°–ü–ï–®–ù–û! –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –±–ª–æ–∫–∞ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£':")
        print("=" * 60)
        
        for i, article in enumerate(articles, 1):
            print(f"\nüì∞ –ù–û–í–û–°–¢–¨ {i}")
            print(f"üìå –ó–∞–≥–æ–ª–æ–≤–æ–∫: {article['title']}")
            print(f"üìù –í—ã–∂–∏–º–∫–∞: {article['summary'][:100]}...")
            if article['image_url']:
                print(f"üñºÔ∏è  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: ‚úÖ")
                print(f"    URL: {article['image_url']}")
            else:
                print(f"üñºÔ∏è  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: ‚ùå")
            print(f"üîó –°—Å—ã–ª–∫–∞: {article['url']}")
            print("-" * 60)
    else:
        print("‚ùå –ù–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        print("–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("- –ò–∑–º–µ–Ω–∏–ª–∞—Å—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–π—Ç–∞")
        print("- –ë–ª–æ–∫ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –¥—Ä—É–≥–æ–º –º–µ—Å—Ç–µ")
        print("- –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º")

if __name__ == "__main__":
    test_targeted_parser()
