import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from zoneinfo import ZoneInfo

KIEV_TZ = ZoneInfo("Europe/Kiev")

class FootballUATargetedParser:
    def __init__(self, max_consecutive_old=2):
        self.base_url = "https://football.ua/"
        self.max_consecutive_old = max_consecutive_old  # –ù–û–í–´–ô –ü–ê–†–ê–ú–ï–¢–† –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "uk-UA,uk;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
        })
    
    def get_page_content(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            return BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None
    
    def find_golovne_za_dobu_section(self, soup):
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –±–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'"""
        
        # –°–ø–æ—Å–æ–± 1: –ü–æ–∏—Å–∫ –ø–æ —Ç–æ—á–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
        header_texts = [
            "–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£",
            "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É", 
            "–ì–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
        ]
        
        for header_text in header_texts:
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å —Ç–∞–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º
            header_element = soup.find(text=re.compile(header_text, re.I))
            if header_element:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫: '{header_text}'")
                
                # –ù–∞—Ö–æ–¥–∏–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å–µ–∫—Ü–∏–∏
                parent = header_element.parent
                while parent and parent.name not in ['section', 'div', 'article']:
                    parent = parent.parent
                
                if parent:
                    # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å–æ —Å–ø–∏—Å–∫–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π
                    news_container = parent.find_next(['div', 'ul', 'section'])
                    if news_container:
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
                        return news_container
                    else:
                        return parent
        
        # –°–ø–æ—Å–æ–± 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —ç—Ç–æ—Ç –±–ª–æ–∫
        possible_selectors = [
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏/–∫–æ–ª–æ–Ω–∫–∏
            '.sidebar',
            '.right-column', 
            '.side-block',
            '.news-sidebar',
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –±–ª–æ–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
            '.daily-news',
            '.main-today',
            '.today-block',
            '.golovne',
            
            # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '[class*="today"]',
            '[class*="daily"]',
            '[class*="golovne"]'
        ]
        
        for selector in possible_selectors:
            elements = soup.select(selector)
            for element in elements:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —ç–ª–µ–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
                if re.search(r'–≥–æ–ª–æ–≤–Ω–µ.*–∑–∞.*–¥–æ–±—É', element.get_text(), re.I):
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                    return element
        
        # –°–ø–æ—Å–æ–± 3: –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–∏
        print("‚ö†Ô∏è –ò—â–µ–º –±–ª–æ–∫ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
        
        # –ò—â–µ–º –≤—Å–µ –±–ª–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
        all_divs = soup.find_all(['div', 'section'], class_=True)
        
        for div in all_divs:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –±–ª–æ–∫–µ —Ç–µ–∫—Å—Ç "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
            div_text = div.get_text().lower()
            if '–≥–æ–ª–æ–≤–Ω–µ' in div_text and '–¥–æ–±—É' in div_text:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ —Å —Ç–µ–∫—Å—Ç–æ–º '–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É'")
                return div
        
        print("‚ùå –ë–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None
    
    def extract_news_from_section(self, section, since_time: Optional[datetime] = None):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        if not section:
            return []
        
        news_links = []
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ —Å–µ–∫—Ü–∏–∏
        all_links = section.find_all('a', href=True)
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –≤ —Å–µ–∫—Ü–∏–∏")
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            if self.is_news_link(href) and len(text) > 10:
                full_url = urljoin(self.base_url, href)
                news_links.append({
                    'title': text,
                    'url': full_url,
                    'href': href
                })
                
                print(f"üì∞ –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {text[:50]}...")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        seen_urls = set()
        unique_news = []
        
        for news in news_links:
            if news['url'] not in seen_urls:
                unique_news.append(news)
                seen_urls.add(news['url'])
        
        # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –≤—Ä–µ–º—è, –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 (—Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ)
        if since_time:
            print(f"üïí –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å {since_time.strftime('%H:%M %d.%m.%Y')}")
            return unique_news  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        else:
            return unique_news[:5]  # –°—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
    
    def is_news_link(self, href):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π"""
        if not href:
            return False
        
        # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –Ω–∞ football.ua
        news_patterns = [
            r'/news/',
            r'/ukraine/',
            r'/world/',
            r'/europe/',
            r'/england/',
            r'/spain/',
            r'/italy/',
            r'/germany/',
            r'/france/',
            r'/poland/',
            r'/\d+[^/]*\.html'  # –°—Å—ã–ª–∫–∏ —Å ID –Ω–æ–≤–æ—Å—Ç–µ–π
        ]
        
        return any(re.search(pattern, href) for pattern in news_patterns)
    
    def parse_ukrainian_date(self, date_text: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏—Ç —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã"""
        try:
            # –°–ª–æ–≤–∞—Ä—å —É–∫—Ä–∞–∏–Ω—Å–∫–∏—Ö –º–µ—Å—è—Ü–µ–≤
            ukrainian_months = {
                '—Å—ñ—á–Ω—è': 1, '–ª—é—Ç–æ–≥–æ': 2, '–±–µ—Ä–µ–∑–Ω—è': 3, '–∫–≤—ñ—Ç–Ω—è': 4, '—Ç—Ä–∞–≤–Ω—è': 5, '—á–µ—Ä–≤–Ω—è': 6,
                '–ª–∏–ø–Ω—è': 7, '—Å–µ—Ä–ø–Ω—è': 8, '–≤–µ—Ä–µ—Å–Ω—è': 9, '–∂–æ–≤—Ç–Ω—è': 10, '–ª–∏—Å—Ç–æ–ø–∞–¥–∞': 11, '–≥—Ä—É–¥–Ω—è': 12,
                '—Å—ñ—á': 1, '–ª—é—Ç': 2, '–±–µ—Ä': 3, '–∫–≤—ñ': 4, '—Ç—Ä–∞': 5, '—á–µ—Ä': 6,
                '–ª–∏–ø': 7, '—Å–µ—Ä': 8, '–≤–µ—Ä': 9, '–∂–æ–≤': 10, '–ª–∏—Å': 11, '–≥—Ä—É': 12
            }
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            cleaned_text = re.sub(r'[,.]', '', date_text.lower().strip())
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω: "02 —Å–µ—Ä–ø–Ω—è 2025, 10:48"
            pattern1 = r'(\d{1,2})\s+(\w+)\s+(\d{4})[\s,]+(\d{1,2}):(\d{2})'
            match1 = re.search(pattern1, cleaned_text)
            
            if match1:
                day = int(match1.group(1))
                month_name = match1.group(2)
                year = int(match1.group(3))
                hour = int(match1.group(4))
                minute = int(match1.group(5))
                
                if month_name in ukrainian_months:
                    month = ukrainian_months[month_name]
                    return datetime(year, month, day, hour, minute, tzinfo=KIEV_TZ)
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω: "02.08.2025, 10:48"
            pattern2 = r'(\d{1,2})\.(\d{1,2})\.(\d{4})[\s,]+(\d{1,2}):(\d{2})'
            match2 = re.search(pattern2, cleaned_text)
            
            if match2:
                day = int(match2.group(1))
                month = int(match2.group(2))
                year = int(match2.group(3))
                hour = int(match2.group(4))
                minute = int(match2.group(5))
                return datetime(year, month, day, hour, minute, tzinfo=KIEV_TZ)
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω: "10:48" (—Ç–æ–ª—å–∫–æ –≤—Ä–µ–º—è, –±–µ—Ä–µ–º —Å–µ–≥–æ–¥–Ω—è—à–Ω—É—é –¥–∞—Ç—É)
            pattern3 = r'^(\d{1,2}):(\d{2})$'
            match3 = re.search(pattern3, cleaned_text)
            
            if match3:
                hour = int(match3.group(1))
                minute = int(match3.group(2))
                today = datetime.now(KIEV_TZ).replace(hour=hour, minute=minute, second=0, microsecond=0)
                return today
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–π –¥–∞—Ç—ã '{date_text}': {e}")
        
        return None
    
    def estimate_article_publish_time(self, soup, url: str) -> Optional[datetime]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏"""
        try:
            print(f"üïí –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –¥–ª—è: {url}")
            
            # –ò—â–µ–º –º–µ—Ç–∞-—Ç–µ–≥–∏ —Å –¥–∞—Ç–æ–π
            meta_selectors = [
                'meta[property="article:published_time"]',
                'meta[name="publish_date"]',
                'meta[name="date"]',
                'meta[property="og:published_time"]',
                'meta[name="DC.date"]',
                'meta[itemprop="datePublished"]'
            ]
            
            for selector in meta_selectors:
                meta_tag = soup.select_one(selector)
                if meta_tag:
                    content = meta_tag.get('content', '')
                    if content:
                        print(f"üìÖ –ù–∞–π–¥–µ–Ω –º–µ—Ç–∞-—Ç–µ–≥ {selector}: {content}")
                        try:
                            # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å ISO —Ñ–æ—Ä–º–∞—Ç
                            if 'T' in content:
                                parsed_date = datetime.fromisoformat(content.replace('Z', '+00:00').replace('+00:00', ''))
                                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∫–∏–µ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è
                                parsed_date_kiev = parsed_date.astimezone(KIEV_TZ)
                                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω –º–µ—Ç–∞-—Ç–µ–≥: {parsed_date_kiev}")
                                return parsed_date_kiev
                        except Exception as e:
                            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –º–µ—Ç–∞-—Ç–µ–≥: {e}")
                            continue
            
            # –ò—â–µ–º –¥–∞—Ç—É –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            date_selectors = [
                '.article-date',
                '.publish-date', 
                '.news-date',
                '.date',
                '.timestamp',
                'time[datetime]',
                '.article-time',
                '.post-date',
                '.entry-date',
                '[class*="date"]',
                '[class*="time"]'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å datetime –∞—Ç—Ä–∏–±—É—Ç
                    datetime_attr = date_elem.get('datetime')
                    if datetime_attr:
                        print(f"üìÖ –ù–∞–π–¥–µ–Ω datetime –∞—Ç—Ä–∏–±—É—Ç: {datetime_attr}")
                        try:
                            parsed_date = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00').replace('+00:00', ''))
                            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∫–∏–µ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è
                            parsed_date_kiev = parsed_date.astimezone(KIEV_TZ)
                            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω datetime: {parsed_date_kiev}")
                            return parsed_date_kiev
                        except Exception as e:
                            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å datetime: {e}")
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –¥–∞—Ç—ã
                    date_text = date_elem.get_text(strip=True)
                    if date_text:
                        print(f"üìÖ –ù–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç –¥–∞—Ç—ã –≤ {selector}: '{date_text}'")
                        parsed_date = self.parse_ukrainian_date(date_text)
                        if parsed_date:
                            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω —Ç–µ–∫—Å—Ç –¥–∞—Ç—ã: {parsed_date}")
                            return parsed_date
            
            # –ò—â–µ–º –¥–∞—Ç—É –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            all_text = soup.get_text()
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ
            date_patterns = [
                r'(\d{1,2})\s+(—Å—ñ—á–Ω—è|–ª—é—Ç–æ–≥–æ|–±–µ—Ä–µ–∑–Ω—è|–∫–≤—ñ—Ç–Ω—è|—Ç—Ä–∞–≤–Ω—è|—á–µ—Ä–≤–Ω—è|–ª–∏–ø–Ω—è|—Å–µ—Ä–ø–Ω—è|–≤–µ—Ä–µ—Å–Ω—è|–∂–æ–≤—Ç–Ω—è|–ª–∏—Å—Ç–æ–ø–∞–¥–∞|–≥—Ä—É–¥–Ω—è)\s+(\d{4})[\s,]+(\d{1,2}):(\d{2})',
                r'(\d{1,2})\.(\d{1,2})\.(\d{4})[\s,]+(\d{1,2}):(\d{2})',
                r'(\d{1,2})/(\d{1,2})/(\d{4})[\s,]+(\d{1,2}):(\d{2})'
            ]
            
            for pattern in date_patterns:
                matches = re.findall(pattern, all_text, re.IGNORECASE)
                for match in matches[:3]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    if len(match) >= 5:
                        try:
                            if '—Å—ñ—á–Ω—è' in pattern or '–ª—é—Ç–æ–≥–æ' in pattern:  # —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
                                parsed_date = self.parse_ukrainian_date(' '.join(match))
                            else:  # —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
                                day, month, year, hour, minute = map(int, match)
                                parsed_date = datetime(year, month, day, hour, minute, tzinfo=KIEV_TZ)
                            
                            if parsed_date:
                                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –¥–∞—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ: {parsed_date}")
                                return parsed_date
                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –¥–∞—Ç—ã: {e}")
                            continue
            
            # –í–ê–ñ–ù–û: –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è, –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è!
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")
            return None
    
    def count_words(self, text: str) -> int:
        """–¢–û–ß–ù–´–ô –ø–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤ –∫–∞–∫ –¥–µ–ª–∞–µ—Ç —á–µ–ª–æ–≤–µ–∫ - —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞"""
        if not text:
            return 0
    
        # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é
        clean_text = re.sub(r'<[^>]+>', '', text)
    
        # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        service_patterns = [
            r'\d{1,2}\s+\w+\s+\d{4},\s+\d{1,2}:\d{2}',  # –¥–∞—Ç—ã –∏ –≤—Ä–µ–º—è
            r'getty images',
            r'—Ñ–æ—Ç–æ:.*',
            r'–¥–∂–µ—Ä–µ–ª–æ:.*',
            r'—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–æ–∂:.*',
            r'\([^)]*\)',  # —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ —Å–∫–æ–±–∫–∞—Ö –∫–∞–∫ (1:0)
        ]
    
        for pattern in service_patterns:
            clean_text = re.sub(pattern, '', clean_text, flags=re.IGNORECASE)
    
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
        # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —É–∫—Ä–∞–∏–Ω—Å–∫–∏–µ –±—É–∫–≤—ã
        word_text = re.sub(r'[^\w\s]', ' ', clean_text, flags=re.UNICODE)
    
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
        words = []
        for word in word_text.split():
            word = word.strip()
            # –ò—Å–∫–ª—é—á–∞–µ–º —á–∏—Å–ª–∞ –∏ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
            if len(word) >= 2 and not word.isdigit():
                words.append(word)
    
        return len(words)
    
    def extract_clean_article_content(self, soup):
        """–ü–†–ê–í–ò–õ–¨–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¢–û–õ–¨–ö–û –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ –±–µ–∑ —Å–ª—É–∂–µ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        
       # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
        soup_copy = BeautifulSoup(str(soup), 'html.parser')
    
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ü–û–õ–ù–û–°–¢–¨–Æ
        unwanted_selectors = [
            'script', 'style', 'iframe', 'noscript', 'svg',
            'header', 'nav', 'footer', 'aside',
            '.header', '.footer', '.navigation', '.nav',
            '[class*="ad"]', '[class*="banner"]', '[class*="advertisement"]',
            '[class*="social"]', '[class*="share"]', '[class*="related"]',
            '[class*="comment"]', '[class*="sidebar"]', '[class*="widget"]',
            '.breadcrumb', '.tags', '.meta', '.author', '.date', '.time',
            '.article-date', '.publish-date', '.news-date',
            '[class*="date"]', '[class*="time"]', '[class*="meta"]',
            # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –¥–ª—è football.ua
            '.social-buttons', '.article-info', '.news-info',
            '.photo-credit', '.image-caption', '.getty-images'
        ]
    
        for selector in unwanted_selectors:
            for element in soup_copy.select(selector):
                element.decompose()
    
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
        main_selectors = [
            '.article-content p',
            '.news-content p', 
            '.post-content p',
            '.main-content p',
            '.article-body p',
            '.content p',
            'article p'
        ]
    
        article_paragraphs = []
    
        for selector in main_selectors:
            paragraphs = soup_copy.select(selector)
            if paragraphs:
                print(f"üéØ –ù–∞–π–¥–µ–Ω—ã –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                article_paragraphs = paragraphs
                break
    
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, –∏—â–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        if not article_paragraphs:
            print("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
            article_paragraphs = soup_copy.find_all('p')
    
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        meaningful_paragraphs = []
    
        for p in article_paragraphs:
            p_text = p.get_text(strip=True)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            if (len(p_text) > 15 and  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                not any(skip_phrase in p_text.lower() for skip_phrase in [
                    'getty images', '—Ñ–æ—Ç–æ:', '–¥–∂–µ—Ä–µ–ª–æ:', '—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–æ–∂',
                    '–ø—ñ–¥–ø–∏—Å—É–π—Ç–µ—Å—å', '—Å—Ç–µ–∂–∏—Ç–µ', 'telegram', 'facebook', 'twitter',
                    '–ø—Ä–æ —Ü–µ –ø–æ–≤—ñ–¥–æ–º–ª—è—î', 'football.ua', '—Ñ—É—Ç–±–æ–ª.ua',
                    'cookie', '—Ä–µ–∫–ª–∞–º', '–∫–æ–º–µ–Ω—Ç–∞—Ä'
                ]) and
                # –ò—Å–∫–ª—é—á–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Ç–æ–ª—å–∫–æ —Å –¥–∞—Ç–æ–π/–≤—Ä–µ–º–µ–Ω–µ–º
                not re.match(r'^\d{1,2}\s+\w+\s+\d{4},\s+\d{1,2}:\d{2}$', p_text) and
                # –ò—Å–∫–ª—é—á–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Ç–æ–ª—å–∫–æ —Å –∏–º–µ–Ω–∞–º–∏ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
                not re.match(r'^[–ê-–Ø–Ü–Ñ][–∞-—è—ñ—î—ó]+\s+[–ê-–Ø–Ü–Ñ][–∞-—è—ñ—î—ó]+,\s*getty images$', p_text, re.IGNORECASE)
            ):
                meaningful_paragraphs.append(p_text)
    
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        main_content = ' '.join(meaningful_paragraphs)
    
        print(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(main_content)} —Å–∏–º–≤–æ–ª–æ–≤ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")
        print(f"üìä –ò–∑ {len(meaningful_paragraphs)} —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
    
        return main_content
    
    def get_full_article_data(self, news_item, since_time: Optional[datetime] = None):
        """–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –º–µ—Ç–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å–∏ —Å –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤—Ä–µ–º–µ–Ω–∏"""
        url = news_item['url']
        soup = self.get_page_content(url)
    
        if not soup:
            return None
    
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –∫–∞–∫ –±—ã–ª–æ
            if since_time:
                publish_time = self.estimate_article_publish_time(soup, url)
                if publish_time and publish_time <= since_time:
                    print(f"‚è∞ –°—Ç–∞—Ç—å—è –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ {publish_time.strftime('%H:%M %d.%m')} - —Å—Ç–∞—Ä–∞—è")
                    return None
            else:
                publish_time = self.estimate_article_publish_time(soup, url)
        
            # –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            print("üìÑ –ò–∑–≤–ª–µ–∫–∞–µ–º –¢–û–õ–¨–ö–û –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏...")
            clean_content = self.extract_clean_article_content(soup)
        
            # –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ø–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤
            word_count = self.count_words(clean_content)
            print(f"üìä –¢–û–ß–ù–û–ï –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {word_count}")
        
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            preview = clean_content[:200] + "..." if len(clean_content) > 200 else clean_content
            print(f"üîç –ü–µ—Ä–≤—ã–µ —Å–∏–º–≤–æ–ª—ã: {preview}")
        
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É
            if word_count > 500:
                print(f"üìè –°—Ç–∞—Ç—å—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è ({word_count} —Å–ª–æ–≤ > 500) - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return None
        
            print(f"‚úÖ –°—Ç–∞—Ç—å—è –ø–æ–¥—Ö–æ–¥–∏—Ç ({word_count} —Å–ª–æ–≤ ‚â§ 500)")
        
            # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É
            summary = self.create_summary(clean_content, news_item['title'])
        
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_url = self.extract_main_image(soup, url)
            
            return {
                'title': news_item['title'],
                'url': url,
                'content': clean_content,
                'summary': summary,
                'image_url': image_url,
                'publish_time': publish_time,
                'word_count': word_count
            }
        
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {url}: {e}")
            return None
    
    def extract_article_content(self, soup):
        """–£–°–¢–ê–†–ï–í–®–ò–ô –º–µ—Ç–æ–¥ - —Ç–µ–ø–µ—Ä—å –≤—ã–∑—ã–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –º–µ—Ç–æ–¥"""
        return self.extract_clean_article_content(soup)
    
    def create_summary(self, content, title):
        """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É"""
        if not content:
            return title
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', content)
        meaningful_sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        
        if meaningful_sentences:
            summary = '. '.join(meaningful_sentences[:2])
            return summary + '.' if not summary.endswith('.') else summary
        
        return content[:200] + '...' if len(content) > 200 else content
    
    def extract_main_image(self, soup, base_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏"""
        image_selectors = [
            'meta[property="og:image"]',
            '.article-image img',
            '.news-image img',
            'article img',
            '.content img:first-of-type',
            '.main-image img',
            '.post-image img'
        ]
        
        for selector in image_selectors:
            if 'meta' in selector:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('content', '')
            else:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('src', '') or img_elem.get('data-src', '')
            
            if image_url:
                full_image_url = urljoin(base_url, image_url)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –º–∞–ª–µ–Ω—å–∫–∞—è –∏–∫–æ–Ω–∫–∞
                if not any(small in image_url.lower() for small in ['icon', 'logo', 'thumb', 'avatar']):
                    return full_image_url
        
        return ''
    
    def get_latest_news(self, since_time: Optional[datetime] = None):
        """–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ - –ø–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' —Å —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        print("üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É Football.ua...")
        
        if since_time:
            print(f"üïí –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å {since_time.strftime('%H:%M %d.%m.%Y')}")
        
        soup = self.get_page_content(self.base_url)
        if not soup:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return []
        
        print("üéØ –ò—â–µ–º –±–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'...")
        golovne_section = self.find_golovne_za_dobu_section(soup)
        
        if not golovne_section:
            print("‚ùå –ë–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        
        print("üì∞ –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞...")
        news_items = self.extract_news_from_section(golovne_section, since_time)
        
        if not news_items:
            print("‚ùå –ù–æ–≤–æ—Å—Ç–∏ –≤ –±–ª–æ–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –±–ª–æ–∫–µ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏
        full_articles = []
        consecutive_old_articles = 0  # –ù–û–í–ê–Ø –ü–ï–†–ï–ú–ï–ù–ù–ê–Ø: —Å—á–µ—Ç—á–∏–∫ –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö —Å—Ç–∞—Ä—ã—Ö —Å—Ç–∞—Ç–µ–π
        
        for i, news_item in enumerate(news_items, 1):
            print(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å {i}/{len(news_items)}: {news_item['title'][:50]}...")
            
            article_data = self.get_full_article_data(news_item, since_time)
            
            # –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ –ª—é–±—ã–º –ø—Ä–∏—á–∏–Ω–∞–º, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if article_data is None:
                # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏—á–∏–Ω—É –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
                # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –≤—Ä–µ–º—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏, –Ω—É–∂–Ω–æ –ø–æ–Ω—è—Ç—å - —ç—Ç–æ –≤—Ä–µ–º—è –∏–ª–∏ –¥–ª–∏–Ω–∞
                if since_time:
                    # –ü–æ–ª—É—á–∞–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                    soup_temp = self.get_page_content(news_item['url'])
                    if soup_temp:
                        publish_time = self.estimate_article_publish_time(soup_temp, news_item['url'])
                        
                        if publish_time and publish_time <= since_time:
                            consecutive_old_articles += 1
                            print(f"‚è∞ –°—Ç–∞—Ä–∞—è —Å—Ç–∞—Ç—å—è #{consecutive_old_articles} –ø–æ–¥—Ä—è–¥ (–≤—Ä–µ–º—è: {publish_time.strftime('%H:%M %d.%m')})")
                            
                            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –µ—Å–ª–∏ N —Å—Ç–∞—Ç–µ–π –ø–æ–¥—Ä—è–¥ —Å—Ç–∞—Ä—ã–µ - –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
                            if consecutive_old_articles >= self.max_consecutive_old:
                                print(f"üö´ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: {self.max_consecutive_old} —Å—Ç–∞—Ç—å–∏ –ø–æ–¥—Ä—è–¥ –æ–∫–∞–∑–∞–ª–∏—Å—å —Å—Ç–∞—Ä—ã–º–∏ - –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—Å—Ç–∞–ª—å–Ω—ã—Ö")
                                print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {len(news_items) - i} –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å—Ç–∞—Ç–µ–π")
                                break
                            
                            continue
                        else:
                            # –°—Ç–∞—Ç—å—è –Ω–æ–≤–∞—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –Ω–æ –Ω–µ –ø–æ–¥–æ—à–ª–∞ –ø–æ –¥—Ä—É–≥–∏–º –ø—Ä–∏—á–∏–Ω–∞–º (–¥–ª–∏–Ω–∞)
                            consecutive_old_articles = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
                            print(f"‚è≠Ô∏è –°—Ç–∞—Ç—å—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ –¥–ª–∏–Ω–µ/—Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                            continue
                    else:
                        # –ù–µ —Å–º–æ–≥–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É - —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –æ—à–∏–±–∫—É
                        consecutive_old_articles = 0
                        print(f"‚è≠Ô∏è –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                        continue
                else:
                    # –ï—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—á–µ—Ç—á–∏–∫–∞
                    print(f"‚è≠Ô∏è –°—Ç–∞—Ç—å—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
            
            # –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –ø–æ–¥—Ö–æ–¥–∏—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë –∏ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
            consecutive_old_articles = 0
            full_articles.append(article_data)
            print(f"‚úÖ –°—Ç–∞—Ç—å—è –¥–æ–±–∞–≤–ª–µ–Ω–∞: {article_data['title'][:50]}...")
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(1)
        
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(full_articles)} –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å—Ç–∞—Ç–µ–π")
        return full_articles

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
def get_latest_news(since_time: Optional[datetime] = None):
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    parser = FootballUATargetedParser()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –±—É—Ñ–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –∫–∞–∫ –≤ onefootball_parser
    if since_time:
        since_time_buffered = since_time - timedelta(minutes=1)
        articles = parser.get_latest_news(since_time_buffered)
    else:
        articles = parser.get_latest_news()
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç, –æ–∂–∏–¥–∞–µ–º—ã–π –æ—Å–Ω–æ–≤–Ω—ã–º –∫–æ–¥–æ–º
    result = []
    for article in articles:
        result.append({
            'title': article['title'],
            'link': article['url'],  # main.py –æ–∂–∏–¥–∞–µ—Ç 'link', –∞ –Ω–µ 'url'
            'url': article['url'],   # –¥–æ–±–∞–≤–ª—è–µ–º –∏ 'url' –¥–ª—è ai_processor
            'summary': article['summary'],
            'image_url': article['image_url'],
            'content': article['content'],  # –í–ê–ñ–ù–û: –ø–æ–ª–Ω—ã–π —á–∏—Å—Ç—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è AI
            'publish_time': article.get('publish_time'),  # –ù–û–í–û–ï: –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            'word_count': article.get('word_count'),  # –ù–û–í–û–ï: –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
            'source': 'Football.ua'  # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
        })
    
    return result

def test_targeted_parser():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞"""
    print("üéØ –¢–ï–°–¢–ò–†–£–ï–ú –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–ê–†–°–ï–† –î–õ–Ø –ë–õ–û–ö–ê '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'")
    print("=" * 60)
    
    # –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π (—Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ)
    print("\nüìã –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
    parser = FootballUATargetedParser(max_consecutive_old=2)  # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
    articles = parser.get_latest_news()
    
    if articles:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        for i, article in enumerate(articles, 1):
            publish_time = article.get('publish_time')
            word_count = article.get('word_count', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            print(f"   üì∞ {i}. {article['title'][:50]}... ({time_str}, {word_count} —Å–ª–æ–≤)")
    
    # –¢–µ—Å—Ç 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é)
    print(f"\nüìã –¢–µ—Å—Ç 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –º–∏–Ω—É—Ç (—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π)")
    since_time = datetime.now(KIEV_TZ) - timedelta(minutes=30)
    recent_articles = parser.get_latest_news(since_time)
    
    if recent_articles:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(recent_articles)} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        for i, article in enumerate(recent_articles, 1):
            publish_time = article.get('publish_time')
            word_count = article.get('word_count', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            print(f"   üì∞ {i}. {article['title'][:50]}... ({time_str}, {word_count} —Å–ª–æ–≤)")
    else:
        print("üî≠ –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –º–∏–Ω—É—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    # –¢–µ—Å—Ç 3: –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–∑–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    print(f"\nüìã –¢–µ—Å—Ç 3: –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (max_consecutive_old=1)")
    aggressive_parser = FootballUATargetedParser(max_consecutive_old=1)
    since_time_old = datetime.now(KIEV_TZ) - timedelta(hours=2)  # 2 —á–∞—Å–∞ –Ω–∞–∑–∞–¥ (–±–æ–ª—å—à–µ —à–∞–Ω—Å–æ–≤ –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ä—ã–µ)
    aggressive_articles = aggressive_parser.get_latest_news(since_time_old)
    
    print(f"üìä –° –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –Ω–∞–π–¥–µ–Ω–æ: {len(aggressive_articles)} —Å—Ç–∞—Ç–µ–π")
    
    # –¢–µ—Å—Ç 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥—Å—á–µ—Ç–∞ —Å–ª–æ–≤
    print(f"\nüìã –¢–µ—Å—Ç 4: –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ —Å–ª–æ–≤")
    test_texts = [
        "–≠—Ç–æ –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –∏–∑ –ø—è—Ç–∏ —Å–ª–æ–≤.",
        "–¢–µ–∫—Å—Ç —Å    –ª–∏—à–Ω–∏–º–∏   –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏ –∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è!!!",
        "<p>HTML —Ç–µ–∫—Å—Ç</p> —Å <strong>—Ç–µ–≥–∞–º–∏</strong> –∏ –æ–±—ã—á–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º.",
        ""
    ]
    
    for test_text in test_texts:
        word_count = parser.count_words(test_text)
        print(f"   üìù \"{test_text[:30]}...\" ‚Üí {word_count} —Å–ª–æ–≤")
    
    print("\nüöÄ –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:")
    print("   ‚úÖ –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –≤ –Ω–∞—á–∞–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    print("   ‚úÖ –ü—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ N —Å—Ç–∞—Ä—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ–¥—Ä—è–¥")
    print("   ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä max_consecutive_old")
    print("   ‚úÖ –≠–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤")
    print("   ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤ –≤ —á–∏—Å—Ç–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ")

if __name__ == "__main__":
    test_targeted_parser()
