import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from zoneinfo import ZoneInfo

KIEV_TZ = ZoneInfo("Europe/Kiev")

class FootballUATargetedParser:
    def __init__(self):
        self.base_url = "https://football.ua/"
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "uk-UA,uk;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
        })
    
    def get_page_content(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            return BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None
    
    def find_golovne_za_dobu_section(self, soup):
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –±–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'"""
        
        # –°–ø–æ—Å–æ–± 1: –ü–æ–∏—Å–∫ –ø–æ —Ç–æ—á–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
        header_texts = [
            "–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£",
            "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É", 
            "–ì–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
        ]
        
        for header_text in header_texts:
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å —Ç–∞–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º
            header_element = soup.find(text=re.compile(header_text, re.I))
            if header_element:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫: '{header_text}'")
                
                # –ù–∞—Ö–æ–¥–∏–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å–µ–∫—Ü–∏–∏
                parent = header_element.parent
                while parent and parent.name not in ['section', 'div', 'article']:
                    parent = parent.parent
                
                if parent:
                    # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å–æ —Å–ø–∏—Å–∫–æ–º –Ω–æ–≤–æ—Å—Ç–µ–π
                    news_container = parent.find_next(['div', 'ul', 'section'])
                    if news_container:
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
                        return news_container
                    else:
                        return parent
        
        # –°–ø–æ—Å–æ–± 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —ç—Ç–æ—Ç –±–ª–æ–∫
        possible_selectors = [
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏/–∫–æ–ª–æ–Ω–∫–∏
            '.sidebar',
            '.right-column', 
            '.side-block',
            '.news-sidebar',
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –±–ª–æ–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
            '.daily-news',
            '.main-today',
            '.today-block',
            '.golovne',
            
            # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '[class*="today"]',
            '[class*="daily"]',
            '[class*="golovne"]'
        ]
        
        for selector in possible_selectors:
            elements = soup.select(selector)
            for element in elements:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —ç–ª–µ–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
                if re.search(r'–≥–æ–ª–æ–≤–Ω–µ.*–∑–∞.*–¥–æ–±—É', element.get_text(), re.I):
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                    return element
        
        # –°–ø–æ—Å–æ–± 3: –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–∏
        print("‚ö†Ô∏è –ò—â–µ–º –±–ª–æ–∫ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
        
        # –ò—â–µ–º –≤—Å–µ –±–ª–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
        all_divs = soup.find_all(['div', 'section'], class_=True)
        
        for div in all_divs:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –±–ª–æ–∫–µ —Ç–µ–∫—Å—Ç "–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É"
            div_text = div.get_text().lower()
            if '–≥–æ–ª–æ–≤–Ω–µ' in div_text and '–¥–æ–±—É' in div_text:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ —Å —Ç–µ–∫—Å—Ç–æ–º '–≥–æ–ª–æ–≤–Ω–µ –∑–∞ –¥–æ–±—É'")
                return div
        
        print("‚ùå –ë–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None
    
    def extract_news_from_section(self, section, since_time: Optional[datetime] = None):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        if not section:
            return []
        
        news_links = []
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ —Å–µ–∫—Ü–∏–∏
        all_links = section.find_all('a', href=True)
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –≤ —Å–µ–∫—Ü–∏–∏")
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            if self.is_news_link(href) and len(text) > 10:
                full_url = urljoin(self.base_url, href)
                news_links.append({
                    'title': text,
                    'url': full_url,
                    'href': href
                })
                
                print(f"üì∞ –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {text[:50]}...")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        seen_urls = set()
        unique_news = []
        
        for news in news_links:
            if news['url'] not in seen_urls:
                unique_news.append(news)
                seen_urls.add(news['url'])
        
        # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –≤—Ä–µ–º—è, –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 (—Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ)
        if since_time:
            print(f"üïí –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å {since_time.strftime('%H:%M %d.%m.%Y')}")
            return unique_news  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        else:
            return unique_news[:5]  # –°—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
    
    def is_news_link(self, href):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π"""
        if not href:
            return False
        
        # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –Ω–∞ football.ua
        news_patterns = [
            r'/news/',
            r'/ukraine/',
            r'/world/',
            r'/europe/',
            r'/england/',
            r'/spain/',
            r'/italy/',
            r'/germany/',
            r'/france/',
            r'/poland/',
            r'/\d+[^/]*\.html'  # –°—Å—ã–ª–∫–∏ —Å ID –Ω–æ–≤–æ—Å—Ç–µ–π
        ]
        
        return any(re.search(pattern, href) for pattern in news_patterns)
    
    def parse_ukrainian_date(self, date_text: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏—Ç —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã"""
        try:
            # –°–ª–æ–≤–∞—Ä—å —É–∫—Ä–∞–∏–Ω—Å–∫–∏—Ö –º–µ—Å—è—Ü–µ–≤
            ukrainian_months = {
                '—Å—ñ—á–Ω—è': 1, '–ª—é—Ç–æ–≥–æ': 2, '–±–µ—Ä–µ–∑–Ω—è': 3, '–∫–≤—ñ—Ç–Ω—è': 4, '—Ç—Ä–∞–≤–Ω—è': 5, '—á–µ—Ä–≤–Ω—è': 6,
                '–ª–∏–ø–Ω—è': 7, '—Å–µ—Ä–ø–Ω—è': 8, '–≤–µ—Ä–µ—Å–Ω—è': 9, '–∂–æ–≤—Ç–Ω—è': 10, '–ª–∏—Å—Ç–æ–ø–∞–¥–∞': 11, '–≥—Ä—É–¥–Ω—è': 12,
                '—Å—ñ—á': 1, '–ª—é—Ç': 2, '–±–µ—Ä': 3, '–∫–≤—ñ': 4, '—Ç—Ä–∞': 5, '—á–µ—Ä': 6,
                '–ª–∏–ø': 7, '—Å–µ—Ä': 8, '–≤–µ—Ä': 9, '–∂–æ–≤': 10, '–ª–∏—Å': 11, '–≥—Ä—É': 12
            }
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            cleaned_text = re.sub(r'[,.]', '', date_text.lower().strip())
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω: "02 —Å–µ—Ä–ø–Ω—è 2025, 10:48"
            pattern1 = r'(\d{1,2})\s+(\w+)\s+(\d{4})[\s,]+(\d{1,2}):(\d{2})'
            match1 = re.search(pattern1, cleaned_text)
            
            if match1:
                day = int(match1.group(1))
                month_name = match1.group(2)
                year = int(match1.group(3))
                hour = int(match1.group(4))
                minute = int(match1.group(5))
                
                if month_name in ukrainian_months:
                    month = ukrainian_months[month_name]
                    return datetime(year, month, day, hour, minute, tzinfo=KIEV_TZ)
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω: "02.08.2025, 10:48"
            pattern2 = r'(\d{1,2})\.(\d{1,2})\.(\d{4})[\s,]+(\d{1,2}):(\d{2})'
            match2 = re.search(pattern2, cleaned_text)
            
            if match2:
                day = int(match2.group(1))
                month = int(match2.group(2))
                year = int(match2.group(3))
                hour = int(match2.group(4))
                minute = int(match2.group(5))
                return datetime(year, month, day, hour, minute, tzinfo=KIEV_TZ)
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω: "10:48" (—Ç–æ–ª—å–∫–æ –≤—Ä–µ–º—è, –±–µ—Ä–µ–º —Å–µ–≥–æ–¥–Ω—è—à–Ω—É—é –¥–∞—Ç—É)
            pattern3 = r'^(\d{1,2}):(\d{2})$'
            match3 = re.search(pattern3, cleaned_text)
            
            if match3:
                hour = int(match3.group(1))
                minute = int(match3.group(2))
                today = datetime.now(KIEV_TZ).replace(hour=hour, minute=minute, second=0, microsecond=0)
                return today
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–π –¥–∞—Ç—ã '{date_text}': {e}")
        
        return None
    
    def estimate_article_publish_time(self, soup, url: str) -> Optional[datetime]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏"""
        try:
            print(f"üïí –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –¥–ª—è: {url}")
            
            # –ò—â–µ–º –º–µ—Ç–∞-—Ç–µ–≥–∏ —Å –¥–∞—Ç–æ–π
            meta_selectors = [
                'meta[property="article:published_time"]',
                'meta[name="publish_date"]',
                'meta[name="date"]',
                'meta[property="og:published_time"]',
                'meta[name="DC.date"]',
                'meta[itemprop="datePublished"]'
            ]
            
            for selector in meta_selectors:
                meta_tag = soup.select_one(selector)
                if meta_tag:
                    content = meta_tag.get('content', '')
                    if content:
                        print(f"üìÖ –ù–∞–π–¥–µ–Ω –º–µ—Ç–∞-—Ç–µ–≥ {selector}: {content}")
                        try:
                            # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å ISO —Ñ–æ—Ä–º–∞—Ç
                            if 'T' in content:
                                parsed_date = datetime.fromisoformat(content.replace('Z', '+00:00').replace('+00:00', ''))
                                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∫–∏–µ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è
                                parsed_date_kiev = parsed_date.astimezone(KIEV_TZ)
                                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω –º–µ—Ç–∞-—Ç–µ–≥: {parsed_date_kiev}")
                                return parsed_date_kiev
                        except Exception as e:
                            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –º–µ—Ç–∞-—Ç–µ–≥: {e}")
                            continue
            
            # –ò—â–µ–º –¥–∞—Ç—É –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            date_selectors = [
                '.article-date',
                '.publish-date', 
                '.news-date',
                '.date',
                '.timestamp',
                'time[datetime]',
                '.article-time',
                '.post-date',
                '.entry-date',
                '[class*="date"]',
                '[class*="time"]'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å datetime –∞—Ç—Ä–∏–±—É—Ç
                    datetime_attr = date_elem.get('datetime')
                    if datetime_attr:
                        print(f"üìÖ –ù–∞–π–¥–µ–Ω datetime –∞—Ç—Ä–∏–±—É—Ç: {datetime_attr}")
                        try:
                            parsed_date = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00').replace('+00:00', ''))
                            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∫–∏–µ–≤—Å–∫–æ–µ –≤—Ä–µ–º—è
                            parsed_date_kiev = parsed_date.astimezone(KIEV_TZ)
                            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω datetime: {parsed_date_kiev}")
                            return parsed_date_kiev
                        except Exception as e:
                            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å datetime: {e}")
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –¥–∞—Ç—ã
                    date_text = date_elem.get_text(strip=True)
                    if date_text:
                        print(f"üìÖ –ù–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç –¥–∞—Ç—ã –≤ {selector}: '{date_text}'")
                        parsed_date = self.parse_ukrainian_date(date_text)
                        if parsed_date:
                            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω —Ç–µ–∫—Å—Ç –¥–∞—Ç—ã: {parsed_date}")
                            return parsed_date
            
            # –ò—â–µ–º –¥–∞—Ç—É –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            all_text = soup.get_text()
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ
            date_patterns = [
                r'(\d{1,2})\s+(—Å—ñ—á–Ω—è|–ª—é—Ç–æ–≥–æ|–±–µ—Ä–µ–∑–Ω—è|–∫–≤—ñ—Ç–Ω—è|—Ç—Ä–∞–≤–Ω—è|—á–µ—Ä–≤–Ω—è|–ª–∏–ø–Ω—è|—Å–µ—Ä–ø–Ω—è|–≤–µ—Ä–µ—Å–Ω—è|–∂–æ–≤—Ç–Ω—è|–ª–∏—Å—Ç–æ–ø–∞–¥–∞|–≥—Ä—É–¥–Ω—è)\s+(\d{4})[\s,]+(\d{1,2}):(\d{2})',
                r'(\d{1,2})\.(\d{1,2})\.(\d{4})[\s,]+(\d{1,2}):(\d{2})',
                r'(\d{1,2})/(\d{1,2})/(\d{4})[\s,]+(\d{1,2}):(\d{2})'
            ]
            
            for pattern in date_patterns:
                matches = re.findall(pattern, all_text, re.IGNORECASE)
                for match in matches[:3]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    if len(match) >= 5:
                        try:
                            if '—Å—ñ—á–Ω—è' in pattern or '–ª—é—Ç–æ–≥–æ' in pattern:  # —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
                                parsed_date = self.parse_ukrainian_date(' '.join(match))
                            else:  # —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
                                day, month, year, hour, minute = map(int, match)
                                parsed_date = datetime(year, month, day, hour, minute, tzinfo=KIEV_TZ)
                            
                            if parsed_date:
                                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –¥–∞—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ: {parsed_date}")
                                return parsed_date
                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –¥–∞—Ç—ã: {e}")
                            continue
            
            # –í–ê–ñ–ù–û: –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è, –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è!
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")
            return None
    
    def count_words(self, text: str) -> int:
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ø–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if not text:
            return 0
        
        # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏, –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å
        clean_text = re.sub(r'<[^>]+>', '', text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –∏ —Ç–∞–±—ã
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        word_text = re.sub(r'[^\w\s]', ' ', clean_text, flags=re.UNICODE)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        words = [word for word in word_text.split() if len(word.strip()) > 0]
        
        return len(words)
    
    def extract_clean_article_content(self, soup):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¢–û–õ–¨–ö–û –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ –±–µ–∑ —Å–ª—É–∂–µ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        
        # –°–Ω–∞—á–∞–ª–∞ —É–¥–∞–ª—è–µ–º –≤—Å–µ –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        unwanted_selectors = [
            'script', 'style', 'iframe', 'noscript',
            'header', 'nav', 'footer', 'aside',
            '[class*="ad"]', '[class*="banner"]', '[class*="advertisement"]',
            '[class*="social"]', '[class*="share"]', '[class*="related"]',
            '[class*="comment"]', '[class*="sidebar"]', '[class*="widget"]',
            '.breadcrumb', '.tags', '.meta', '.author', '.date',
            '.navigation', '.pagination', '.menu', '.header', '.footer'
        ]
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é soup —á—Ç–æ–±—ã –Ω–µ –ø–æ–≤—Ä–µ–¥–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª
        soup_copy = BeautifulSoup(str(soup), 'html.parser')
        
        for selector in unwanted_selectors:
            for element in soup_copy.select(selector):
                element.decompose()
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏
        main_content_selectors = [
            '.article-content',
            '.news-content', 
            '.post-content',
            '.main-content',
            '.article-body',
            '.news-body',
            '.content',
            'article .content',
            '.text-content',
            '[class*="article"] .content',
            '[class*="news"] .content'
        ]
        
        main_content = ""
        
        for selector in main_content_selectors:
            content_elem = soup_copy.select_one(selector)
            if content_elem:
                print(f"üéØ –ù–∞–π–¥–µ–Ω –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                paragraphs = content_elem.find_all('p')
                if paragraphs:
                    paragraph_texts = []
                    for p in paragraphs:
                        p_text = p.get_text(strip=True)
                        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                        if (len(p_text) > 20 and 
                            not any(skip in p_text.lower() for skip in [
                                '—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ', '–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å', '—Å–ª–µ–¥–∏—Ç–µ',
                                '–∏—Å—Ç–æ—á–Ω–∏–∫', '—Ñ–æ—Ç–æ', '–≤–∏–¥–µ–æ', '—Ä–µ–∫–ª–∞–º–∞',
                                'cookie', '–ø—ñ–¥–ø–∏—Å', '–¥–∂–µ—Ä–µ–ª–æ', '—á–∏—Ç–∞–π—Ç–µ',
                                'telegram', 'facebook', 'twitter', 'instagram'
                            ])):
                            paragraph_texts.append(p_text)
                    
                    main_content = ' '.join(paragraph_texts)
                    if main_content:
                        break
        
        # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ article –∏–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π div
        if not main_content:
            print("‚ö†Ô∏è –û—Å–Ω–æ–≤–Ω–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—â–µ–º —á–µ—Ä–µ–∑ article/div")
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ç–µ–≥ article
            article_tag = soup_copy.find('article')
            if article_tag:
                paragraphs = article_tag.find_all('p')
                paragraph_texts = []
                for p in paragraphs:
                    p_text = p.get_text(strip=True)
                    if (len(p_text) > 20 and 
                        not any(skip in p_text.lower() for skip in [
                            '—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–æ–∂', '–ø—ñ–¥–ø–∏—Å—É–π—Ç–µ—Å—å', '—Å—Ç–µ–∂–∏—Ç–µ',
                            '–¥–∂–µ—Ä–µ–ª–æ', '—Ñ–æ—Ç–æ', '–≤—ñ–¥–µ–æ', '—Ä–µ–∫–ª–∞–º–∞',
                            'cookie', '–ø—ñ–¥–ø–∏—Å', '—á–∏—Ç–∞–π—Ç–µ',
                            'telegram', 'facebook', 'twitter', 'instagram'
                        ])):
                        paragraph_texts.append(p_text)
                
                main_content = ' '.join(paragraph_texts)
        
        # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ, –∏—â–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        if not main_content:
            print("‚ö†Ô∏è Article –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—â–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã")
            all_paragraphs = soup_copy.find_all('p')
            meaningful_paragraphs = []
            
            for p in all_paragraphs:
                p_text = p.get_text(strip=True)
                # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
                if (len(p_text) > 30 and 
                    not any(skip in p_text.lower() for skip in [
                        'cookie', '—Ä–µ–∫–ª–∞–º', '–ø—ñ–¥–ø–∏—Å', '—Ñ–æ—Ç–æ', '–¥–∂–µ—Ä–µ–ª–æ',
                        '—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–æ–∂', '–ø—ñ–¥–ø–∏—Å—É–π—Ç–µ—Å—å', '–∫–æ–º–µ–Ω—Ç–∞—Ä',
                        'telegram', 'facebook', 'twitter', 'instagram',
                        '—Å–ª—ñ–¥–∫—É–π—Ç–µ', '–Ω–æ–≤–∏–Ω–∏', '–≥–æ–ª–æ–≤–Ω', '—Å–ø–æ—Ä—Ç',
                        '—Ñ—É—Ç–±–æ–ª.ua', 'football.ua', '—Å–∞–π—Ç', '–ø–æ—Ä—Ç–∞–ª'
                    ]) and
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –Ω–∞–≤–∏–≥–∞—Ü–∏—è –∏–ª–∏ –º–µ–Ω—é
                    len([word for word in p_text.split() if len(word) > 2]) > 5):
                    meaningful_paragraphs.append(p_text)
            
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã (–æ–±—ã—á–Ω–æ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –∏–¥–µ—Ç –≤ –Ω–∞—á–∞–ª–µ)
            main_content = ' '.join(meaningful_paragraphs[:10])
        
        print(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(main_content)} —Å–∏–º–≤–æ–ª–æ–≤ —á–∏—Å—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
        return main_content
    
    def get_full_article_data(self, news_item, since_time: Optional[datetime] = None):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ —Å –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ô –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤—Ä–µ–º–µ–Ω–∏ –∏ –¥–ª–∏–Ω—ã"""
        url = news_item['url']
        soup = self.get_page_content(url)
        
        if not soup:
            return None
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            publish_time = self.estimate_article_publish_time(soup, url)
            
            # –ò–ó–ú–ï–ù–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê: –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –≤—Ä–µ–º—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Ç–æ—á–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
            if since_time and publish_time:
                if publish_time <= since_time:
                    print(f"‚è∞ –°—Ç–∞—Ç—å—è –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ {publish_time.strftime('%H:%M %d.%m')} - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–¥–æ {since_time.strftime('%H:%M %d.%m')})")
                    return None
                else:
                    print(f"‚úÖ –°—Ç–∞—Ç—å—è –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ {publish_time.strftime('%H:%M %d.%m')} - –Ω–æ–≤–∞—è!")
            elif since_time and not publish_time:
                # –ï—Å–ª–∏ –Ω–µ —Å–º–æ–≥–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è, —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç—å—é –Ω–æ–≤–æ–π
                print(f"‚ö†Ô∏è –í—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ - —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç—å—é –Ω–æ–≤–æ–π")
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò–∑–≤–ª–µ–∫–∞–µ–º –ß–ò–°–¢–´–ô –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            clean_content = self.extract_clean_article_content(soup)
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –ß–ò–°–¢–û–ú –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            word_count = self.count_words(clean_content)
            print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –ß–ò–°–¢–û–ô —Å—Ç–∞—Ç—å–µ: {word_count}")
            
            if word_count > 450:
                print(f"üìè –°—Ç–∞—Ç—å—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è ({word_count} —Å–ª–æ–≤ > 450) - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return None
            
            print(f"‚úÖ –°—Ç–∞—Ç—å—è –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ –¥–ª–∏–Ω–µ ({word_count} —Å–ª–æ–≤ ‚â§ 450)")
            
            # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É –∏–∑ —á–∏—Å—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            summary = self.create_summary(clean_content, news_item['title'])
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_url = self.extract_main_image(soup, url)
            
            return {
                'title': news_item['title'],
                'url': url,
                'content': clean_content,  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —á–∏—Å—Ç—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                'summary': summary,
                'image_url': image_url,
                'publish_time': publish_time,
                'word_count': word_count  # –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {url}: {e}")
            return None
    
    def extract_article_content(self, soup):
        """–£–°–¢–ê–†–ï–í–®–ò–ô –º–µ—Ç–æ–¥ - –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        # –¢–µ–ø–µ—Ä—å –ø—Ä–æ—Å—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥
        return self.extract_clean_article_content(soup)
    
    def create_summary(self, content, title):
        """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É"""
        if not content:
            return title
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', content)
        meaningful_sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        
        if meaningful_sentences:
            summary = '. '.join(meaningful_sentences[:2])
            return summary + '.' if not summary.endswith('.') else summary
        
        return content[:200] + '...' if len(content) > 200 else content
    
    def extract_main_image(self, soup, base_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏"""
        image_selectors = [
            'meta[property="og:image"]',
            '.article-image img',
            '.news-image img',
            'article img',
            '.content img:first-of-type',
            '.main-image img',
            '.post-image img'
        ]
        
        for selector in image_selectors:
            if 'meta' in selector:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('content', '')
            else:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('src', '') or img_elem.get('data-src', '')
            
            if image_url:
                full_image_url = urljoin(base_url, image_url)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –º–∞–ª–µ–Ω—å–∫–∞—è –∏–∫–æ–Ω–∫–∞
                if not any(small in image_url.lower() for small in ['icon', 'logo', 'thumb', 'avatar']):
                    return full_image_url
        
        return ''
    
    def get_latest_news(self, since_time: Optional[datetime] = None):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ - –ø–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –¥–ª–∏–Ω–µ"""
        print("üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É Football.ua...")
        
        if since_time:
            since_time_buffered = since_time - timedelta(minutes=1)
            print(f"üïí –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å {since_time_buffered.strftime('%H:%M %d.%m.%Y')}")
        
        soup = self.get_page_content(self.base_url)
        if not soup:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return []
        
        print("üéØ –ò—â–µ–º –±–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'...")
        golovne_section = self.find_golovne_za_dobu_section(soup)
        
        if not golovne_section:
            print("‚ùå –ë–ª–æ–∫ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
        
        print("üì∞ –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–∞...")
        news_items = self.extract_news_from_section(golovne_section, since_time_buffered)
        
        if not news_items:
            print("‚ùå –ù–æ–≤–æ—Å—Ç–∏ –≤ –±–ª–æ–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –±–ª–æ–∫–µ '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏
        full_articles = []
        
        for i, news_item in enumerate(news_items, 1):
            print(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å {i}/{len(news_items)}: {news_item['title'][:50]}...")
            
            article_data = self.get_full_article_data(news_item, since_time)
            
            # –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
            if since_time_buffered and article_data is None:
                print(f"üõë –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –Ω–æ–≤–æ—Å—Ç—å, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
                break
            
            # –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë
            if article_data:
                full_articles.append(article_data)
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(1)
        
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(full_articles)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π")
        return full_articles

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
def get_latest_news(since_time: Optional[datetime] = None):
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    parser = FootballUATargetedParser()
    articles = parser.get_latest_news(since_time - timedelta(minutes=1))
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç, –æ–∂–∏–¥–∞–µ–º—ã–π –æ—Å–Ω–æ–≤–Ω—ã–º –∫–æ–¥–æ–º
    result = []
    for article in articles:
        result.append({
            'title': article['title'],
            'link': article['url'],  # main.py –æ–∂–∏–¥–∞–µ—Ç 'link', –∞ –Ω–µ 'url'
            'url': article['url'],   # –¥–æ–±–∞–≤–ª—è–µ–º –∏ 'url' –¥–ª—è ai_processor
            'summary': article['summary'],
            'image_url': article['image_url'],
            'content': article['content'],  # –í–ê–ñ–ù–û: –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è AI
            'publish_time': article.get('publish_time'),  # –ù–û–í–û–ï: –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            'word_count': article.get('word_count')  # –ù–û–í–û–ï: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
        })
    
    return result

def test_targeted_parser():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞"""
    print("üéØ –¢–ï–°–¢–ò–†–£–ï–ú –ü–ê–†–°–ï–† –î–õ–Ø –ë–õ–û–ö–ê '–ì–û–õ–û–í–ù–ï –ó–ê –î–û–ë–£'")
    print("=" * 60)
    
    # –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π (—Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ)
    print("\nüìã –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
    parser = FootballUATargetedParser()
    articles = parser.get_latest_news()
    
    if articles:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        for i, article in enumerate(articles, 1):
            publish_time = article.get('publish_time')
            word_count = article.get('word_count', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            print(f"   üì∞ {i}. {article['title'][:50]}... ({time_str}, {word_count} —Å–ª–æ–≤)")
    
    # –¢–µ—Å—Ç 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    print(f"\nüìã –¢–µ—Å—Ç 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –º–∏–Ω—É—Ç")
    since_time = datetime.now(KIEV_TZ) - timedelta(minutes=30)
    recent_articles = parser.get_latest_news(since_time)
    
    if recent_articles:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(recent_articles)} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        for i, article in enumerate(recent_articles, 1):
            publish_time = article.get('publish_time')
            word_count = article.get('word_count', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            print(f"   üì∞ {i}. {article['title'][:50]}... ({time_str}, {word_count} —Å–ª–æ–≤)")
    else:
        print("üî≠ –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –º–∏–Ω—É—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

if __name__ == "__main__":
    test_targeted_parser()

