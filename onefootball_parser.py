import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import logging
import random
import time
import json


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CONFIG = {
    'USER_AGENTS': [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0'
    ],
    'BASE_URL': 'https://onefootball.com/en/home',
    'NEWS_API_URL': 'https://onefootball.com/en/news',
    'MAX_NEWS': 15,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
    'RETRY_ATTEMPTS': 3,
    'RETRY_DELAY': 2,
    'REQUEST_DELAY': 1.5,  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ —Å—Ç–∞—Ç—å—è–º
}

KIEV_TZ = ZoneInfo("Europe/Kiev")

class OneFootballParser:
    def __init__(self):
        self.base_url = CONFIG['BASE_URL']
        self.news_url = CONFIG['NEWS_API_URL']
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": random.choice(CONFIG['USER_AGENTS']),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9,uk;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Referer": "https://onefootball.com/",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache"
        })
        

    def parse_publish_time(self, time_str: str, current_time: datetime = None) -> datetime:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≤—Ä–µ–º–µ–Ω–∏ –≤ –æ–±—ä–µ–∫—Ç datetime —Å –∫–∏–µ–≤—Å–∫–∏–º —á–∞—Å–æ–≤—ã–º –ø–æ—è—Å–æ–º (EEST)."""
        try:
            if not current_time:
                current_time = datetime.now(KIEV_TZ)
            logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä–µ–º–µ–Ω–∏: {time_str}, —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è: {current_time}")

            if 'ago' in time_str.lower():
                # –ü–∞—Ä—Å–∏–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "2 hours ago", "30 minutes ago")
                numbers = re.findall(r'\d+', time_str)
                if numbers:
                    value = int(numbers[0])
                    if 'second' in time_str.lower():
                        delta = timedelta(seconds=value)
                    elif 'minute' in time_str.lower():
                        delta = timedelta(minutes=value)
                    elif 'hour' in time_str.lower():
                        delta = timedelta(hours=value)
                    elif 'day' in time_str.lower():
                        delta = timedelta(days=value)
                    else:
                        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º –º–∏–Ω—É—Ç—ã
                        delta = timedelta(minutes=value)
                    return current_time - delta
                else:
                    return current_time - timedelta(minutes=5)  # –î–µ—Ñ–æ–ª—Ç –¥–ª—è –Ω–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ "ago"

            # –ü–∞—Ä—Å–∏–º ISO —Ñ–æ—Ä–º–∞—Ç
            if 'T' in time_str:
                dt = datetime.fromisoformat(time_str.replace('Z', '+00:00')).astimezone(KIEV_TZ)
                logger.debug(f"–£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ ISO –≤—Ä–µ–º—è: {time_str} -> {dt}")
                return dt
            
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç—ã
            date_formats = [
                '%Y-%m-%d %H:%M %Z',
                '%Y-%m-%d %H:%M',
                '%Y-%m-%dT%H:%M:%S',
                '%d.%m.%Y %H:%M',
                '%d/%m/%Y %H:%M'
            ]
            
            for fmt in date_formats:
                try:
                    dt = datetime.strptime(time_str, fmt)
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=KIEV_TZ)
                    else:
                        dt = dt.astimezone(KIEV_TZ)
                    logger.debug(f"–£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ –≤—Ä–µ–º—è: {time_str} -> {dt}")
                    return dt
                except ValueError:
                    continue
            
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –≤—Ä–µ–º—è '{time_str}', –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è")
            return current_time
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä–µ–º–µ–Ω–∏ '{time_str}': {e}")
            return current_time

    def get_page_content(self, url: str) -> BeautifulSoup:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
        for attempt in range(CONFIG['RETRY_ATTEMPTS']):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                return BeautifulSoup(response.text, "html.parser")
            except Exception as e:
                logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt+1}/{CONFIG['RETRY_ATTEMPTS']} –ø—Ä–æ–≤–∞–ª–µ–Ω–∞ –¥–ª—è {url}: {e}")
                if attempt < CONFIG['RETRY_ATTEMPTS'] - 1:
                    time.sleep(CONFIG['RETRY_DELAY'])
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url} –ø–æ—Å–ª–µ {CONFIG['RETRY_ATTEMPTS']} –ø–æ–ø—ã—Ç–æ–∫")
        return None

    def find_top_news_section(self, soup: BeautifulSoup) -> BeautifulSoup:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–µ–∫—Ü–∏—é —Å –≤–µ—Ä—Ö–Ω–∏–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏."""
        possible_selectors = [
            '.of-feed',
            '[data-testid="feed"]',
            '.news-feed',
            '.latest-articles',
            '.article-feed',
            '[data-testid="news-list"]',
            '[class*="feed"]',
            '[class*="news"]',
            '[class*="articles"]'
        ]

        for selector in possible_selectors:
            section = soup.select_one(selector)
            if section:
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                return section

        all_divs = soup.find_all('div', class_=True)
        for div in all_divs:
            class_str = str(div.get('class', ''))
            if any(bad in class_str.lower() for bad in ['banner', 'promo', 'advert', 'sponsored', 'teaser']):
                continue
            if re.search(r'news|articles|feed|latest|content', class_str, re.I):
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {class_str}")
                return div

        logger.error("‚ùå –°–µ–∫—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return None

    def fetch_full_article(self, url: str) -> tuple[str, str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ —Å—Ç–∞—Ç—å–∏."""
        try:
            soup = self.get_page_content(url)
            if not soup:
                return "", ""

            content_selectors = [
                '.article-content',
                '.post-content',
                '[class*="body"]',
                'article',
                '.main-text',
                '.content'
            ]

            article_text = ""
            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    for unwanted in content_div.find_all(['script', 'style', 'iframe', 'div[class*="ad"]']):
                        unwanted.decompose()
                    paragraphs = content_div.find_all('p')
                    if paragraphs:
                        article_text = '\n'.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
                        break
                    else:
                        article_text = content_div.get_text(strip=True)
                        break

            if not article_text:
                all_paragraphs = soup.find_all('p')
                meaningful_paragraphs = []
                for p in all_paragraphs:
                    text = p.get_text(strip=True)
                    if (len(text) > 30 and
                        not any(skip in text.lower() for skip in ['cookie', 'advertisement', 'subscribe', 'photo', 'source'])):
                        meaningful_paragraphs.append(text)
                article_text = '\n'.join(meaningful_paragraphs)
                if len(article_text) > 1500:
                    sentences = re.split(r'[.!?]+', article_text)
                    trimmed_content = ""
                    current_length = 0
                    for sentence in sentences:
                        if current_length + len(sentence) <= 1500:
                            trimmed_content += sentence + '. '
                            current_length += len(sentence) + 2
                        else:
                            break
                    article_text = trimmed_content.rstrip()

            image_selectors = [
                'meta[property="og:image"]',
                'meta[name="twitter:image"]',
                '.article-content img:first-of-type',
                '.main-image img',
                '.post-image img',
                'img[class*="featured"]'
            ]

            image_url = ""
            for selector in image_selectors:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('content', '') or img_elem.get('src', '') or img_elem.get('data-src', '')
                    if image_url:
                        image_url = urljoin(url, image_url)
                        if not any(small in image_url.lower() for small in ['icon', 'logo', 'thumb', 'avatar']):
                            break
            return article_text, image_url

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
            return "", ""

    def get_latest_news(self, since_time: datetime = None) -> list:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å OneFootball —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å—Ç–∞—Ç–µ–π."""
        current_time = datetime.now(KIEV_TZ)
        if since_time is None:
            current_hour = current_time.hour
            current_minute = current_time.minute
            if 5 <= current_hour < 6 and current_minute >= 50 or current_hour == 6 and current_minute <= 10:
                since_time = current_time.replace(hour=1, minute=0, second=0, microsecond=0)
                logger.info(f"–†–µ–∂–∏–º 5 —á–∞—Å–æ–≤: since_time —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ {since_time}")
            else:
                since_time = current_time - timedelta(minutes=20)
                logger.info(f"–†–µ–∂–∏–º 20 –º–∏–Ω—É—Ç: since_time —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ {since_time}")

        logger.info(f"üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É OneFootball... (—Å {since_time.strftime('%H:%M %d.%m.%Y')})")

        soup = self.get_page_content(self.base_url)
        if not soup:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return []

        news_container = self.find_top_news_section(soup)
        if not news_container:
            logger.error("‚ùå –°–µ–∫—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return []

        articles = news_container.find_all(['article', 'div', 'li', 'section'], recursive=True)[:CONFIG['MAX_NEWS']]
        logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(articles)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ —Å–µ–∫—Ü–∏–∏")

        news_items = []
        for article in articles:
            try:
                title_elem = article.select_one('h1, h2, h3, [class*="title"], [class*="headline"]')
                title = title_elem.get_text(strip=True) if title_elem else ''
                if not title or len(title) < 10:
                    continue

                link_elem = article.select_one('a[href]')
                url = link_elem['href'] if link_elem else ''
                if not url:
                    continue
                if not url.startswith('http'):
                    url = urljoin(self.base_url, url)

                time_elem = article.select_one('time, [class*="date"], [class*="time"]')
                time_str = time_elem['datetime'] if time_elem and 'datetime' in time_elem.attrs else ''
                if not time_str:
                    time_text = time_elem.get_text(strip=True) if time_elem else ''
                    time_str = time_text if time_text else str(current_time)
                logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ –≤—Ä–µ–º—è –Ω–æ–≤–æ—Å—Ç–∏: {time_str}")

                publish_time = self.parse_publish_time(time_str, current_time)
                if publish_time < since_time:
                    logger.info(f"–ù–æ–≤–æ—Å—Ç—å '{title[:50]}...' —Å—Ç–∞—Ä–∞—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (publish_time={publish_time}, since_time={since_time})")
                    continue

                article_text, image_url = self.fetch_full_article(url)
                if not image_url:
                    thumb_img = article.select_one('img')
                    if thumb_img:
                        thumb_url = thumb_img.get('src', '') or thumb_img.get('data-src', '')
                        if thumb_url:
                            image_url = urljoin(self.base_url, thumb_url) if not thumb_url.startswith('http') else thumb_url

                translated_title, processed_content = self.translate_and_process_article(title, article_text, url)

                news_item = {
                    'title': translated_title,
                    'url': url,
                    'content': processed_content,
                    'summary': processed_content[:300] + "..." if len(processed_content) > 300 else processed_content,
                    'publish_time': publish_time,
                    'image_url': image_url,
                    'source': 'OneFootball',
                    'original_title': title,
                    'original_content': article_text
                }
                news_items.append(news_item)
                logger.info(f"üì∞ –î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {translated_title[:50]}...")

                time.sleep(1)

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                continue

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(news_items)} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å OneFootball")
        return news_items

def get_latest_news(since_time: datetime = None) -> list:
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å main.py."""
    parser = OneFootballParser()
    return parser.get_latest_news(since_time)

if __name__ == "__main__":
    logger.info("üéØ –¢–ï–°–¢–ò–†–£–ï–ú –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–ê–†–°–ï–† –î–õ–Ø ONEFOOTBALL")
    logger.info("=" * 60)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    test_time = datetime.now(KIEV_TZ) - timedelta(hours=6)  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 6 —á–∞—Å–æ–≤
    
    articles = get_latest_news(since_time=test_time)
    
    if articles:
        logger.info(f"‚úÖ –£–°–ü–ï–®–ù–û! –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        logger.info("üèÜ –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        
        methods_count = {}
        for article in articles:
            method = article.get('extraction_method', 'unknown')
            methods_count[method] = methods_count.get(method, 0) + 1
        
        logger.info("üìà –ü–æ –º–µ—Ç–æ–¥–∞–º –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:")
        for method, count in methods_count.items():
            logger.info(f"   {method}: {count} —Å—Ç–∞—Ç–µ–π")
        
        logger.info("\nüì∞ –°–ü–ò–°–û–ö –ù–û–í–û–°–¢–ï–ô (—Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è ai_processor):")
        for i, article in enumerate(articles, 1):
            publish_time = article.get('publish_time')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            logger.info(f"   {i:2d}. {article['title'][:60]}... ({time_str})")
            if article.get('image_url'):
                logger.info(f"       üñºÔ∏è  {article['image_url'][:60]}...")
            logger.info(f"       üìÑ –ö–æ–Ω—Ç–µ–Ω—Ç: {len(article.get('content', ''))} —Å–∏–º–≤–æ–ª–æ–≤")
    else:
        logger.error("‚ùå –û–®–ò–ë–ö–ê! –ù–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        logger.info("\nüîß –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –û–¢–õ–ê–î–ö–ï:")
        logger.info("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã onefootball_debug_*.html")
        logger.info("2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–∞–π—Ç –¥–æ—Å—Ç—É–ø–µ–Ω")
        logger.info("3. –í–æ–∑–º–æ–∂–Ω–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–π—Ç–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å")
    
    logger.info("=" * 60)
