import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import logging
import random

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CONFIG = {
    'USER_AGENTS': [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0'
    ],
    'BASE_URL': 'https://onefootball.com/en/home',
    'MAX_NEWS': 10
}

KIEV_TZ = ZoneInfo("Europe/Kiev")

def parse_publish_time(time_str: str, current_time: datetime = None) -> datetime:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≤—Ä–µ–º–µ–Ω–∏ –≤ –æ–±—ä–µ–∫—Ç datetime —Å –∫–∏–µ–≤—Å–∫–∏–º —á–∞—Å–æ–≤—ã–º –ø–æ—è—Å–æ–º (EEST).
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, '15 minutes ago') –∏ ISO —Ñ–æ—Ä–º–∞—Ç."""
    try:
        if not current_time:
            current_time = datetime.now(KIEV_TZ)
        logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä–µ–º–µ–Ω–∏: {time_str}, —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è: {current_time}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, '15 minutes ago')
        if 'ago' in time_str.lower():
            for unit in ['minutes', 'hours', 'days']:
                if unit in time_str.lower():
                    value = int(''.join(filter(str.isdigit, time_str)))
                    if unit == 'minutes':
                        delta = timedelta(minutes=value)
                    elif unit == 'hours':
                        delta = timedelta(hours=value)
                    elif unit == 'days':
                        delta = timedelta(days=value)
                    return current_time - delta
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ ISO —Ñ–æ—Ä–º–∞—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, '2025-08-08T18:02:00Z')
        if 'T' in time_str:
            dt = datetime.fromisoformat(time_str.replace('Z', '+00:00')).astimezone(KIEV_TZ)
        else:
            try:
                dt = datetime.strptime(time_str, '%Y-%m-%d %H:%M %Z').astimezone(KIEV_TZ)
            except ValueError:
                dt = datetime.strptime(time_str, '%Y-%m-%d %H:%M').replace(tzinfo=KIEV_TZ)
        logger.debug(f"–£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ –≤—Ä–µ–º—è: {time_str} -> {dt}")
        return dt
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä–µ–º–µ–Ω–∏ '{time_str}': {e}")
        return current_time  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏

def fetch_full_article(url: str) -> tuple[str, str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ —Å—Ç–∞—Ç—å–∏."""
    try:
        headers = {'User-Agent': random.choice(CONFIG['USER_AGENTS'])}
        logger.info(f"üåê –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç—å—é: {url}")
        response = requests.get(url, headers=headers, timeout=15)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ - –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –¥–ª—è AI
        content_selectors = [
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è OneFootball
            '[data-testid="article-body"]',
            '.ArticleBody',
            '.article-body',
            '.article-content',
            '.post-content', 
            '[class*="body"]',
            '[class*="content"]',
            'article',
            '.main-text',
            '.content'
        ]
        
        article_text = ""
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {selector}")
                
                # –£–±–∏—Ä–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                for unwanted in content_div.find_all(['script', 'style', 'iframe', 'div[class*="ad"]', 'aside', 'nav']):
                    unwanted.decompose()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                paragraphs = content_div.find_all('p')
                if paragraphs:
                    meaningful_paragraphs = []
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if len(text) > 20:  # –¢–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                            meaningful_paragraphs.append(text)
                    
                    if meaningful_paragraphs:
                        article_text = '\n'.join(meaningful_paragraphs)
                        logger.info(f"üìù –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤: {len(article_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                        break
                else:
                    # –ï—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –Ω–µ—Ç, –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
                    article_text = content_div.get_text(strip=True)
                    if len(article_text) > 50:
                        logger.info(f"üìù –ò–∑–≤–ª–µ—á–µ–Ω –æ–±—â–∏–π —Ç–µ–∫—Å—Ç: {len(article_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                        break
        
        # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—â–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        if not article_text or len(article_text) < 100:
            logger.warning("üìÑ –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—â–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –ø–æ –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
            all_paragraphs = soup.find_all('p')
            meaningful_paragraphs = []
            
            for p in all_paragraphs:
                text = p.get_text(strip=True)
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                if (len(text) > 50 and 
                    not any(skip in text.lower() for skip in [
                        'cookie', 'advertisement', 'subscribe', 'follow', 'social', 
                        'newsletter', 'privacy', 'terms', 'copyright', '¬©'
                    ])):
                    meaningful_paragraphs.append(text)
            
            if meaningful_paragraphs:
                article_text = '\n\n'.join(meaningful_paragraphs)
                logger.info(f"üìù –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑ –≤—Å–µ—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤: {len(article_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –º–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, —Ä–∞—Å—à–∏—Ä—è–µ–º –ø–æ–∏—Å–∫
            if len(article_text) < 200:
                logger.warning("üìÑ –ú–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, —Ä–∞—Å—à–∏—Ä—è–µ–º –ø–æ–∏—Å–∫")
                # –ò—â–µ–º –ª—é–±—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏
                text_elements = soup.find_all(['div', 'span'], string=True)
                additional_text = []
                
                for elem in text_elements:
                    text = elem.get_text(strip=True)
                    if (len(text) > 30 and 
                        text not in article_text and
                        not any(skip in text.lower() for skip in ['cookie', 'advertisement', 'menu', 'navigation'])):
                        additional_text.append(text)
                        if len('\n'.join(additional_text)) > 1000:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—ä–µ–º
                            break
                
                if additional_text:
                    if article_text:
                        article_text += '\n\n' + '\n'.join(additional_text)
                    else:
                        article_text = '\n'.join(additional_text)
                    logger.info(f"üìù –î–æ–±–∞–≤–ª–µ–Ω –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç: {len(article_text)} —Å–∏–º–≤–æ–ª–æ–≤")

        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        if article_text:
            import re
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
            article_text = re.sub(r'\n\s*\n\s*\n', '\n\n', article_text)
            # –£–±–∏—Ä–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ (–≤–µ—Ä–æ—è—Ç–Ω–æ –º—É—Å–æ—Ä)
            lines = article_text.split('\n')
            clean_lines = []
            for line in lines:
                line = line.strip()
                if len(line) > 15 or (len(line) > 5 and any(word in line.lower() for word in ['goal', 'match', 'player', 'team', 'football', 'soccer'])):
                    clean_lines.append(line)
            article_text = '\n'.join(clean_lines)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è AI –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if len(article_text) > 2000:
                sentences = re.split(r'[.!?]+', article_text)
                trimmed_content = ""
                for sentence in sentences:
                    if len(trimmed_content + sentence) < 2000:
                        trimmed_content += sentence + ". "
                    else:
                        break
                article_text = trimmed_content.strip()

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
        image_selectors = [
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
            '[data-testid="article-image"] img',
            '.article-image img',
            '.featured-image img', 
            '[class*="image"] img',
            'article img:first-of-type',
            '.main-image img',
            '.post-image img',
            'img[src*="onefootball"]',  # –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è OneFootball
            'img[src*="wp-content"]',   # WordPress –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            'figure img',
            '.hero-image img'
        ]
        
        image_url = ""
        for selector in image_selectors:
            if 'meta' in selector:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('content', '')
            else:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = (img_elem.get('src', '') or 
                               img_elem.get('data-src', '') or 
                               img_elem.get('data-lazy-src', ''))
            
            if image_url:
                # –î–µ–ª–∞–µ–º –ø–æ–ª–Ω—ã–π URL –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if not image_url.startswith('http'):
                    if image_url.startswith('//'):
                        image_url = 'https:' + image_url
                    elif image_url.startswith('/'):
                        image_url = 'https://onefootball.com' + image_url
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if (not any(small in image_url.lower() for small in ['icon', 'logo', 'thumb', 'avatar', 'placeholder']) 
                    and len(image_url) > 20
                    and ('onefootball' in image_url or 'wp-content' in image_url or 'cloudinary' in image_url)):
                    logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_url}")
                    break
                else:
                    image_url = ""  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –µ—Å–ª–∏ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç
        
        logger.info(f"‚úÖ –°—Ç–∞—Ç—å—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {len(article_text)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞, {'—Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º' if image_url else '–±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è'}")
        return article_text, image_url

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
        return "", ""

def translate_and_process_article(title: str, content: str, url: str) -> tuple[str, str]:
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç—å—é —Å –ø–æ–º–æ—â—å—é AI"""
    try:
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ AI –æ–±—Ä–∞–±–æ—Ç–∫–∏
        from ai_processor import create_enhanced_summary, has_gemini_key
        
        if not has_gemini_key():
            logger.warning("AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –ø–µ—Ä–µ–≤–æ–¥")
            return title, content[:200] + "..." if len(content) > 200 else content
        
        logger.info(f"ü§ñ –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å—é: {title[:50]}...")
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º
        translated_summary = create_enhanced_summary({
            'title': title,
            'content': content,
            'url': url,
            'source': 'OneFootball'
        })
        
        # –û—á–∏—â–∞–µ–º –æ—Ç –≤–æ–∑–º–æ–∂–Ω—ã—Ö markdown —Å–∏–º–≤–æ–ª–æ–≤
        if translated_summary.startswith('**') and translated_summary.endswith('**'):
            translated_summary = translated_summary.strip('* ').strip()
        
        # –£–±–∏—Ä–∞–µ–º markdown –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Ç–æ–∂–µ
        if content:
            content = content.replace('**', '')
        
        logger.info(f"‚úÖ –°—Ç–∞—Ç—å—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ AI: {len(translated_summary)} —Å–∏–º–≤–æ–ª–æ–≤")
        return translated_summary, content
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ AI –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
        return title, content[:200] + "..." if len(content) > 200 else content

def get_latest_news(since_time: datetime = None) -> list:
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å OneFootball —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å—Ç–∞—Ç–µ–π."""
    logger.info("–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å OneFootball...")
    news_items = []

    try:
        headers = {'User-Agent': random.choice(CONFIG['USER_AGENTS'])}
        response = requests.get(CONFIG['BASE_URL'], headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        news_container = soup.select('section article')
        if not news_container:
            logger.warning("–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []

        current_time = datetime.now(KIEV_TZ)
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ since_time –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if since_time is None:
            current_hour = current_time.hour
            current_minute = current_time.minute
            # –î–∏–∞–ø–∞–∑–æ–Ω 5:50 - 6:10 —É—Ç—Ä–∞
            if 5 <= current_hour < 6 and current_minute >= 50 or current_hour == 6 and current_minute <= 10:
                since_time = current_time.replace(hour=1, minute=0, second=0, microsecond=0)
                logger.info(f"–†–µ–∂–∏–º 5 —á–∞—Å–æ–≤: since_time —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ {since_time}")
            else:
                since_time = current_time - timedelta(minutes=20)
                logger.info(f"–†–µ–∂–∏–º 20 –º–∏–Ω—É—Ç: since_time —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ {since_time}")

        for article in news_container[:CONFIG['MAX_NEWS']]:
            try:
                title_elem = article.select_one('h3, [class*="title"]')
                title = title_elem.get_text(strip=True) if title_elem else ''

                link_elem = article.select_one('a[href]')
                url = link_elem['href'] if link_elem else ''
                if url and not url.startswith('http'):
                    url = 'https://onefootball.com' + url

                time_elem = article.select_one('time, [class*="date"]')
                time_str = time_elem['datetime'] if time_elem and 'datetime' in time_elem.attrs else ''
                if not time_str:  # –ï—Å–ª–∏ datetime –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –ø–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ —Ç–µ–∫—Å—Ç–∞
                    time_text = time_elem.get_text(strip=True) if time_elem else ''
                    time_str = time_text if time_text else str(current_time)
                logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ –≤—Ä–µ–º—è –Ω–æ–≤–æ—Å—Ç–∏: {time_str}")

                publish_time = parse_publish_time(time_str, current_time)

                if publish_time < since_time:
                    logger.info(f"–ù–æ–≤–æ—Å—Ç—å '{title[:50]}...' —Å—Ç–∞—Ä–∞—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (publish_time={publish_time}, since_time={since_time})")
                    continue

                # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                logger.info(f"üìñ –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏: {url}")
                article_text, image_url = fetch_full_article(url)
                
                logger.info(f"üìÑ –ü–æ–ª—É—á–µ–Ω–æ {len(article_text)} —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
                
                # –ï—Å–ª–∏ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Å—Ç–∞—Ç—å–∏, –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å thumbnail
                if not image_url:
                    thumb_img = article.select_one('img')
                    if thumb_img:
                        thumb_url = thumb_img.get('src', '') or thumb_img.get('data-src', '')
                        if thumb_url:
                            if not thumb_url.startswith('http'):
                                if thumb_url.startswith('//'):
                                    thumb_url = 'https:' + thumb_url
                                elif thumb_url.startswith('/'):
                                    thumb_url = 'https://onefootball.com' + thumb_url
                            image_url = thumb_url
                            logger.info(f"üñºÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º thumbnail: {image_url}")

                # –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º AI –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                logger.info(f"ü§ñ –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å AI...")
                translated_title, processed_content = translate_and_process_article(title, article_text, url)

                # –°–æ–∑–¥–∞–µ–º summary –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                summary = processed_content[:300] + "..." if len(processed_content) > 300 else processed_content

                news_item = {
                    'title': translated_title,
                    'url': url,
                    'content': article_text,  # –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π AI –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    'summary': summary,       # –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ
                    'publish_time': publish_time,
                    'image_url': image_url,
                    'source': 'OneFootball',
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    'original_title': title,
                    'original_content': article_text,
                    'processed_content': processed_content  # –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π AI –∫–æ–Ω—Ç–µ–Ω—Ç
                }
                news_items.append(news_item)
                logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {translated_title[:50]}...")
                
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å—Ç–∞—Ç–µ–π
                import time
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                continue

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(news_items)} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å OneFootball")
        return news_items

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π —Å OneFootball: {e}")
        return []
