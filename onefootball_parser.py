import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import logging
import random
import time
import os
import google.generativeai as genai
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CONFIG = {
    'USER_AGENTS': [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0'
    ],
    'BASE_URL': 'https://onefootball.com/en/home',
    'MAX_NEWS': 10
}

KIEV_TZ = ZoneInfo("Europe/Kiev")

class OneFootballParser:
    def __init__(self):
        self.base_url = CONFIG['BASE_URL']
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": random.choice(CONFIG['USER_AGENTS']),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9,uk;q=0.8",
            "Connection": "keep-alive",
            "Referer": "https://onefootball.com/",
        })
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini
        self.GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
        self.model = None
        self.init_gemini()

    def init_gemini(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç–∞ Gemini."""
        if not self.GEMINI_API_KEY:
            logger.warning("GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω - AI —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç–∫–ª—é—á–µ–Ω—ã")
            return
        try:
            genai.configure(api_key=self.GEMINI_API_KEY)
            self.model = genai.GenerativeModel("gemini-2.5-flash")
            logger.info("Gemini –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Gemini: {e}")

    def parse_publish_time(self, time_str: str, current_time: datetime = None) -> datetime:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≤—Ä–µ–º–µ–Ω–∏ –≤ –æ–±—ä–µ–∫—Ç datetime —Å –∫–∏–µ–≤—Å–∫–∏–º —á–∞—Å–æ–≤—ã–º –ø–æ—è—Å–æ–º (EEST)."""
        try:
            if not current_time:
                current_time = datetime.now(KIEV_TZ)
            logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä–µ–º–µ–Ω–∏: {time_str}, —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è: {current_time}")

            if 'ago' in time_str.lower():
                for unit in ['minutes', 'hours', 'days']:
                    if unit in time_str.lower():
                        value = int(''.join(filter(str.isdigit, time_str)))
                        if unit == 'minutes':
                            delta = timedelta(minutes=value)
                        elif unit == 'hours':
                            delta = timedelta(hours=value)
                        elif unit == 'days':
                            delta = timedelta(days=value)
                        return current_time - delta
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è")

            if 'T' in time_str:
                dt = datetime.fromisoformat(time_str.replace('Z', '+00:00')).astimezone(KIEV_TZ)
            else:
                try:
                    dt = datetime.strptime(time_str, '%Y-%m-%d %H:%M %Z').astimezone(KIEV_TZ)
                except ValueError:
                    dt = datetime.strptime(time_str, '%Y-%m-%d %H:%M').replace(tzinfo=KIEV_TZ)
            logger.debug(f"–£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ –≤—Ä–µ–º—è: {time_str} -> {dt}")
            return dt
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä–µ–º–µ–Ω–∏ '{time_str}': {e}")
            return current_time

    def get_page_content(self, url: str) -> BeautifulSoup:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥."""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            with open('onefootball_static.html', 'w', encoding='utf-8') as f:
                f.write(response.text)
            return BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None

    def find_top_news_section(self, soup: BeautifulSoup) -> BeautifulSoup:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–µ–∫—Ü–∏—é —Å –≤–µ—Ä—Ö–Ω–∏–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏."""
        possible_selectors = [
            '.of-feed',
            '[data-testid="feed"]',
            '.news-feed',
            '.latest-articles',
            '.article-feed',
            '[data-testid="news-list"]',
            '[class*="feed"]',
            '[class*="news"]',
            '[class*="articles"]'
        ]

        for selector in possible_selectors:
            section = soup.select_one(selector)
            if section:
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                return section

        all_divs = soup.find_all('div', class_=True)
        for div in all_divs:
            class_str = str(div.get('class', ''))
            if any(bad in class_str.lower() for bad in ['banner', 'promo', 'advert', 'sponsored', 'teaser']):
                continue
            if re.search(r'news|articles|feed|latest|content', class_str, re.I):
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {class_str}")
                return div

        logger.error("‚ùå –°–µ–∫—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return None

    def fetch_full_article(self, url: str) -> tuple[str, str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ —Å—Ç–∞—Ç—å–∏."""
        try:
            soup = self.get_page_content(url)
            if not soup:
                return "", ""

            content_selectors = [
                '.article-content',
                '.post-content',
                '[class*="body"]',
                'article',
                '.main-text',
                '.content'
            ]

            article_text = ""
            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    for unwanted in content_div.find_all(['script', 'style', 'iframe', 'div[class*="ad"]']):
                        unwanted.decompose()
                    paragraphs = content_div.find_all('p')
                    if paragraphs:
                        article_text = '\n'.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
                        break
                    else:
                        article_text = content_div.get_text(strip=True)
                        break

            if not article_text:
                all_paragraphs = soup.find_all('p')
                meaningful_paragraphs = []
                for p in all_paragraphs:
                    text = p.get_text(strip=True)
                    if (len(text) > 30 and
                        not any(skip in text.lower() for skip in ['cookie', 'advertisement', 'subscribe', 'photo', 'source'])):
                        meaningful_paragraphs.append(text)
                article_text = '\n'.join(meaningful_paragraphs)
                if len(article_text) > 1500:
                    sentences = re.split(r'[.!?]+', article_text)
                    trimmed_content = ""
                    current_length = 0
                    for sentence in sentences:
                        if current_length + len(sentence) <= 1500:
                            trimmed_content += sentence + '. '
                            current_length += len(sentence) + 2
                        else:
                            break
                    article_text = trimmed_content.rstrip()

            image_selectors = [
                'meta[property="og:image"]',
                'meta[name="twitter:image"]',
                '.article-content img:first-of-type',
                '.main-image img',
                '.post-image img',
                'img[class*="featured"]'
            ]

            image_url = ""
            for selector in image_selectors:
                img_elem = soup.select_one(selector)
                if img_elem:
                    image_url = img_elem.get('content', '') or img_elem.get('src', '') or img_elem.get('data-src', '')
                    if image_url:
                        image_url = urljoin(url, image_url)
                        if not any(small in image_url.lower() for small in ['icon', 'logo', 'thumb', 'avatar']):
                            break
            return article_text, image_url

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
            return "", ""

    def translate_and_process_article(self, title: str, content: str, url: str) -> tuple[str, str]:
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –∏ —Å–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Å—Ç–∞—Ç—å–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Gemini."""
        if not self.model:
            logger.warning("Gemini –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
            return title, content[:200] + "..." if len(content) > 200 else content

        prompt = f"""–¢–∏ —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ñ—É—Ç–±–æ–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω. –ü–µ—Ä–µ–∫–ª–∞–¥–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —ñ —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—Ç—ñ –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É, –ø–æ—Ç—ñ–º —Å—Ç–≤–æ—Ä–∏ –ö–û–†–û–¢–ö–ò–ô –ø–æ—Å—Ç –¥–ª—è Telegram (–º–∞–∫—Å. 150 —Å–ª—ñ–≤).

–ü—Ä–∞–≤–∏–ª–∞ –ø–µ—Ä–µ–∫–ª–∞–¥—É:
- –ó–±–µ—Ä—ñ–≥–∞–π —Ç–æ—á–Ω—ñ—Å—Ç—å —Ñ—É—Ç–±–æ–ª—å–Ω–æ—ó —Ç–µ—Ä–º—ñ–Ω–æ–ª–æ–≥—ñ—ó
- –£–Ω–∏–∫–∞–π –¥–æ—Å–ª—ñ–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É, –∞–¥–∞–ø—Ç—É–π –¥–æ –ø—Ä–∏—Ä–æ–¥–Ω–æ—ó —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏
- –ó–±–µ—Ä—ñ–≥–∞–π —ñ–º–µ–Ω–∞ –≥—Ä–∞–≤—Ü—ñ–≤ —ñ –∫–æ–º–∞–Ω–¥ –±–µ–∑ –∑–º—ñ–Ω

–ü—Ä–∞–≤–∏–ª–∞ –ø–æ—Å—Ç—É:
- –¢—ñ–ª—å–∫–∏ –∫–ª—é—á–æ–≤—ñ —Ñ–∞–∫—Ç–∏
- –ú–∞–∫—Å–∏–º—É–º 1-2 —Ä–µ—á–µ–Ω–Ω—è –ø—Ä—è–º–æ—ó –º–æ–≤–∏
- –î–ª—è —Ä–µ–π—Ç–∏–Ω–≥—ñ–≤: –ª–∏—à–µ —Ç–æ–ø-5
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –≥–æ–ª–æ–≤–Ω–∏–π —Ñ–∞–∫—Ç (1-2 —Ä–µ—á–µ–Ω–Ω—è), –¥–µ—Ç–∞–ª—ñ (2-4 —Ä–µ—á–µ–Ω–Ω—è)

–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
–¢–µ–∫—Å—Ç: {content[:1500]}

–í–Ü–î–ü–û–í–Ü–î–¨ –£ –§–û–†–ú–ê–¢–Ü JSON:
{{
    "translated_title": "...",
    "summary": "..."
}}
"""
        try:
            response = self.model.generate_content(prompt)
            result = response.text.strip()
            parsed = json.loads(result)
            return parsed['translated_title'], parsed['summary']
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ Gemini –¥–ª—è {url}: {e}")
            return title, content[:200] + "..." if len(content) > 200 else content

    def get_latest_news(self, since_time: datetime = None) -> list:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å OneFootball —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å—Ç–∞—Ç–µ–π."""
        current_time = datetime.now(KIEV_TZ)
        if since_time is None:
            current_hour = current_time.hour
            current_minute = current_time.minute
            if 5 <= current_hour < 6 and current_minute >= 50 or current_hour == 6 and current_minute <= 10:
                since_time = current_time.replace(hour=1, minute=0, second=0, microsecond=0)
                logger.info(f"–†–µ–∂–∏–º 5 —á–∞—Å–æ–≤: since_time —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ {since_time}")
            else:
                since_time = current_time - timedelta(minutes=20)
                logger.info(f"–†–µ–∂–∏–º 20 –º–∏–Ω—É—Ç: since_time —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ {since_time}")

        logger.info(f"üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É OneFootball... (—Å {since_time.strftime('%H:%M %d.%m.%Y')})")

        soup = self.get_page_content(self.base_url)
        if not soup:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return []

        news_container = self.find_top_news_section(soup)
        if not news_container:
            logger.error("‚ùå –°–µ–∫—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return []

        articles = news_container.find_all(['article', 'div', 'li', 'section'], recursive=True)[:CONFIG['MAX_NEWS']]
        logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(articles)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ —Å–µ–∫—Ü–∏–∏")

        news_items = []
        for article in articles:
            try:
                title_elem = article.select_one('h1, h2, h3, [class*="title"], [class*="headline"]')
                title = title_elem.get_text(strip=True) if title_elem else ''
                if not title or len(title) < 10:
                    continue

                link_elem = article.select_one('a[href]')
                url = link_elem['href'] if link_elem else ''
                if not url:
                    continue
                if not url.startswith('http'):
                    url = urljoin(self.base_url, url)

                time_elem = article.select_one('time, [class*="date"], [class*="time"]')
                time_str = time_elem['datetime'] if time_elem and 'datetime' in time_elem.attrs else ''
                if not time_str:
                    time_text = time_elem.get_text(strip=True) if time_elem else ''
                    time_str = time_text if time_text else str(current_time)
                logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ –≤—Ä–µ–º—è –Ω–æ–≤–æ—Å—Ç–∏: {time_str}")

                publish_time = self.parse_publish_time(time_str, current_time)
                if publish_time < since_time:
                    logger.info(f"–ù–æ–≤–æ—Å—Ç—å '{title[:50]}...' —Å—Ç–∞—Ä–∞—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (publish_time={publish_time}, since_time={since_time})")
                    continue

                article_text, image_url = self.fetch_full_article(url)
                if not image_url:
                    thumb_img = article.select_one('img')
                    if thumb_img:
                        thumb_url = thumb_img.get('src', '') or thumb_img.get('data-src', '')
                        if thumb_url:
                            image_url = urljoin(self.base_url, thumb_url) if not thumb_url.startswith('http') else thumb_url

                translated_title, processed_content = self.translate_and_process_article(title, article_text, url)

                news_item = {
                    'title': translated_title,
                    'url': url,
                    'content': processed_content,
                    'summary': processed_content[:300] + "..." if len(processed_content) > 300 else processed_content,
                    'publish_time': publish_time,
                    'image_url': image_url,
                    'source': 'OneFootball',
                    'original_title': title,
                    'original_content': article_text
                }
                news_items.append(news_item)
                logger.info(f"üì∞ –î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {translated_title[:50]}...")

                time.sleep(1)

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                continue

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(news_items)} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å OneFootball")
        return news_items

def get_latest_news(since_time: datetime = None) -> list:
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å main.py."""
    parser = OneFootballParser()
    return parser.get_latest_news(since_time)

if __name__ == "__main__":
    logger.info("üéØ –¢–ï–°–¢–ò–†–£–ï–ú –ü–ê–†–°–ï–† –î–õ–Ø ONEFOOTBALL")
    logger.info("=" * 60)
    articles = get_latest_news()
    if articles:
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        for i, article in enumerate(articles, 1):
            publish_time = article.get('publish_time')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            logger.info(f"   üì∞ {i}. {article['title'][:50]}... ({time_str})")
    else:
        logger.info("üì≠ –ù–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
