import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from zoneinfo import ZoneInfo
import google.generativeai as genai
import logging
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

KIEV_TZ = ZoneInfo("Europe/Kiev")

class OneFootballParser:
    def __init__(self):
        self.base_url = "https://onefootball.com/en/home"
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                          "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9,uk;q=0.8",
            "Connection": "keep-alive",
            "Referer": "https://onefootball.com/",
        })
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini
        self.GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
        self.model = None
        self.init_gemini()

    def init_gemini(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç–∞ Gemini."""
        if not self.GEMINI_API_KEY:
            logger.warning("GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω - AI —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç–∫–ª—é—á–µ–Ω—ã")
            return
        try:
            genai.configure(api_key=self.GEMINI_API_KEY)
            self.model = genai.GenerativeModel("gemini-2.5-flash")
            logger.info("Gemini –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Gemini: {e}")

    def get_page_content(self, url: str) -> Optional[BeautifulSoup]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥."""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            with open('onefootball_static.html', 'w', encoding='utf-8') as f:
                f.write(response.text)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            return BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None

    def find_top_news_section(self, soup: BeautifulSoup) -> Optional[BeautifulSoup]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–µ–∫—Ü–∏—é —Å –≤–µ—Ä—Ö–Ω–∏–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏."""
        possible_selectors = [
            '.of-feed',
            '[data-testid="feed"]',
            '.news-feed',
            '.latest-articles',
            '.article-feed',
            '[data-testid="news-list"]',
            '[class*="feed"]',
            '[class*="news"]',
            '[class*="articles"]',
        ]

        for selector in possible_selectors:
            section = soup.select_one(selector)
            if section:
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                return section

        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫: –≤—Å–µ div —Å –∫–ª–∞—Å—Å–∞–º–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ –Ω–æ–≤–æ—Å—Ç–∏
        all_divs = soup.find_all('div', class_=True)
        for div in all_divs:
            class_str = str(div.get('class', ''))
            if any(bad in class_str.lower() for bad in ['banner', 'promo', 'advert', 'sponsored']):
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–µ–∫–ª–∞–º–Ω—ã–µ –±–ª–æ–∫–∏
            if re.search(r'news|articles|feed|latest|content', class_str, re.I):
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {class_str}")
                return div

        logger.error("‚ùå –°–µ–∫—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return None

    def extract_news_from_section(self, section: BeautifulSoup, max_items: int = 10) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ 10 –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Å–µ–∫—Ü–∏–∏."""
        if not section:
            return []

        news_links = []
        articles = section.find_all(['article', 'div', 'li', 'section'], recursive=True)[:max_items]

        logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(articles)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ —Å–µ–∫—Ü–∏–∏")

        for article in articles:
            link = article.find('a', href=True)
            if not link:
                continue

            href = link.get('href', '')
            title_elem = link.find(['h1', 'h2', 'h3', 'span', 'div'], class_=re.compile(r'title|headline|text', re.I))
            title = title_elem.get_text(strip=True) if title_elem else link.get_text(strip=True)

            if not title or len(title) < 10 or not self.is_news_link(href):
                continue

            full_url = urljoin(self.base_url, href)
            news_links.append({
                'title': title,
                'url': full_url,
                'href': href
            })
            logger.info(f"üì∞ –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {title[:50]}...")

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        seen_urls = set()
        unique_news = []
        for news in news_links:
            if news['url'] not in seen_urls:
                seen_urls.add(news['url'])
                unique_news.append(news)

        return unique_news[:max_items]

    def is_news_link(self, href: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π."""
        if not href:
            return False
        return (
            href.startswith('/') or
            href.startswith(self.base_url) or
            'news' in href.lower() or
            'article' in href.lower()
        ) and not any(ext in href.lower() for ext in ['login', 'signup', 'profile', '#'])

    def get_article_publish_time(self, soup: BeautifulSoup) -> Optional[datetime]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–∏."""
        time_selectors = [
            'time[datetime]',
            '.date',
            '.publish-date',
            '[class*="date"]',
            '[class*="time"]',
            'meta[property="article:published_time"]',
            'meta[name="pubdate"]'
        ]

        for selector in time_selectors:
            time_elem = soup.select_one(selector)
            if time_elem:
                time_str = time_elem.get('datetime') or time_elem.get('content') or time_elem.get_text(strip=True)
                try:
                    pub_time = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                    return pub_time.astimezone(KIEV_TZ)
                except ValueError:
                    try:
                        pub_time = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
                        return pub_time.replace(tzinfo=KIEV_TZ)
                    except ValueError:
                        continue
        return None

    def get_article_content(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏."""
        content_selectors = [
            '.article-content',
            '.post-content',
            '.entry-content',
            '[class*="content"]',
            '.article-body',
            '.post-body',
            '.story-body'
        ]

        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                for unwanted in content_div.find_all(['script', 'style', 'iframe', 'ads', 'aside']):
                    unwanted.decompose()
                content = content_div.get_text(strip=True)
                if content and len(content) > 50:
                    return content[:2000]

        paragraphs = soup.find_all('p')
        content = ' '.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
        return content[:2000] if content else ""

    def get_article_image(self, soup: BeautifulSoup, base_url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –≥–ª–∞–≤–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏."""
        image_selectors = [
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
            '.article-content img:first-of-type',
            '.main-image img',
            '.post-image img'
        ]

        for selector in image_selectors:
            img_elem = soup.select_one(selector)
            if img_elem:
                image_url = img_elem.get('content', '') or img_elem.get('src', '') or img_elem.get('data-src', '')
                if image_url:
                    full_image_url = urljoin(base_url, image_url)
                    if not any(small in image_url.lower() for small in ['icon', 'logo', 'thumb', 'avatar']):
                        return full_image_url
        return ""

    def translate_and_summarize(self, title: str, content: str) -> Dict[str, str]:
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –∏ —Å–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Å—Ç–∞—Ç—å–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Gemini."""
        if not self.model:
            logger.warning("Gemini –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
            return {'translated_title': title, 'summary': title[:200]}

        prompt = f"""–¢–∏ —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ñ—É—Ç–±–æ–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω. –ü–µ—Ä–µ–∫–ª–∞–¥–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —ñ —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—Ç—ñ –∑ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É, –ø–æ—Ç—ñ–º —Å—Ç–≤–æ—Ä–∏ –ö–û–†–û–¢–ö–ò–ô –ø–æ—Å—Ç –¥–ª—è Telegram (–º–∞–∫—Å. 150 —Å–ª—ñ–≤).

–ü—Ä–∞–≤–∏–ª–∞ –ø–µ—Ä–µ–∫–ª–∞–¥—É:
- –ó–±–µ—Ä—ñ–≥–∞–π —Ç–æ—á–Ω—ñ—Å—Ç—å —Ñ—É—Ç–±–æ–ª—å–Ω–æ—ó —Ç–µ—Ä–º—ñ–Ω–æ–ª–æ–≥—ñ—ó
- –£–Ω–∏–∫–∞–π –¥–æ—Å–ª—ñ–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É, –∞–¥–∞–ø—Ç—É–π –¥–æ –ø—Ä–∏—Ä–æ–¥–Ω–æ—ó —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏
- –ó–±–µ—Ä—ñ–≥–∞–π —ñ–º–µ–Ω–∞ –≥—Ä–∞–≤—Ü—ñ–≤ —ñ –∫–æ–º–∞–Ω–¥ –±–µ–∑ –∑–º—ñ–Ω

–ü—Ä–∞–≤–∏–ª–∞ –ø–æ—Å—Ç—É:
- –¢—ñ–ª—å–∫–∏ –∫–ª—é—á–æ–≤—ñ —Ñ–∞–∫—Ç–∏
- –ú–∞–∫—Å–∏–º—É–º 1-2 —Ä–µ—á–µ–Ω–Ω—è –ø—Ä—è–º–æ—ó –º–æ–≤–∏
- –î–ª—è —Ä–µ–π—Ç–∏–Ω–≥—ñ–≤: –ª–∏—à–µ —Ç–æ–ø-5
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –≥–æ–ª–æ–≤–Ω–∏–π —Ñ–∞–∫—Ç (1-2 —Ä–µ—á–µ–Ω–Ω—è), –¥–µ—Ç–∞–ª—ñ (2-4 —Ä–µ—á–µ–Ω–Ω—è)

–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
–¢–µ–∫—Å—Ç: {content[:1500]}

–í–Ü–î–ü–û–í–Ü–î–¨ –£ –§–û–†–ú–ê–¢–Ü JSON:
{{
    "translated_title": "...",
    "summary": "..."
}}
"""
        try:
            response = self.model.generate_content(prompt)
            result = response.text.strip()
            import json
            return json.loads(result)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ Gemini: {e}")
            return {'translated_title': title, 'summary': content[:200]}

    def get_full_article_data(self, news_item: Dict[str, Any], since_time: Optional[datetime]) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏, –≤–∫–ª—é—á–∞—è –ø–µ—Ä–µ–≤–æ–¥ –∏ —Ä–µ–∑—é–º–µ."""
        url = news_item['url']
        logger.info(f"üìñ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç—å—é: {url}")

        soup = self.get_page_content(url)
        if not soup:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ç—å—é: {url}")
            return None

        publish_time = self.get_article_publish_time(soup)
        if since_time and publish_time and publish_time < since_time:
            logger.info(f"üõë –°—Ç–∞—Ç—å—è —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä–∞—è: {publish_time.strftime('%H:%M %d.%m')}")
            return None

        content = self.get_article_content(soup)
        image_url = self.get_article_image(soup, url)
        ai_result = self.translate_and_summarize(news_item['title'], content)

        return {
            'title': ai_result['translated_title'],
            'original_title': news_item['title'],
            'url': url,
            'summary': ai_result['summary'],
            'content': content,
            'image_url': image_url,
            'publish_time': publish_time,
            'source': 'OneFootball'
        }

    def get_latest_news(self, since_time: Optional[datetime] = None) -> List[Dict[str, Any]]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ - –ø–æ–ª—É—á–∞–µ—Ç –≤–µ—Ä—Ö–Ω–∏–µ 10 –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏."""
        current_time_kiev = datetime.now(KIEV_TZ)
        is_morning = (5*60 + 50 <= current_time_kiev.hour*60 + current_time_kiev.minute <= 6*60 + 10)
        time_delta = timedelta(hours=5) if is_morning else timedelta(minutes=20)
        since_time = since_time or (current_time_kiev - time_delta)

        logger.info(f"üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É OneFootball... (—Å {since_time.strftime('%H:%M %d.%m.%Y')})")

        soup = self.get_page_content(self.base_url)
        if not soup:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return []

        news_section = self.find_top_news_section(soup)
        if not news_section:
            logger.error("‚ùå –°–µ–∫—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return []

        news_items = self.extract_news_from_section(news_section, max_items=10)
        if not news_items:
            logger.error("‚ùå –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π")

        full_articles = []
        for i, news_item in enumerate(news_items, 1):
            logger.info(f"üìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å {i}/{len(news_items)}: {news_item['title'][:50]}...")
            article_data = self.get_full_article_data(news_item, since_time)
            if article_data:
                full_articles.append(article_data)
            time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(full_articles)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π")
        return full_articles

def get_latest_news(since_time: Optional[datetime] = None) -> List[Dict[str, Any]]:
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å main.py."""
    parser = OneFootballParser()
    articles = parser.get_latest_news(since_time)
    return [{
        'title': article['title'],
        'link': article['url'],
        'url': article['url'],
        'summary': article['summary'],
        'image_url': article['image_url'],
        'content': article['content'],
        'publish_time': article['publish_time'],
        'source': 'OneFootball',
        'original_title': article['original_title']
    } for article in articles]

def test_onefootball_parser():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ OneFootball."""
    logger.info("üéØ –¢–ï–°–¢–ò–†–£–ï–ú –ü–ê–†–°–ï–† –î–õ–Ø ONEFOOTBALL")
    logger.info("=" * 60)

    # –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    logger.info("\nüìã –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
    parser = OneFootballParser()
    articles = parser.get_latest_news()

    if articles:
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        for i, article in enumerate(articles, 1):
            publish_time = article.get('publish_time')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            logger.info(f"   üì∞ {i}. {article['title'][:50]}... ({time_str})")
    else:
        logger.info("üì≠ –ù–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    # –¢–µ—Å—Ç 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    logger.info("\nüìã –¢–µ—Å—Ç 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 –º–∏–Ω—É—Ç")
    since_time = datetime.now(KIEV_TZ) - timedelta(minutes=20)
    recent_articles = parser.get_latest_news(since_time)

    if recent_articles:
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(recent_articles)} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        for i, article in enumerate(recent_articles, 1):
            publish_time = article.get('publish_time')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            logger.info(f"   üì∞ {i}. {article['title'][:50]}... ({time_str})")
    else:
        logger.info("üì≠ –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 –º–∏–Ω—É—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

if __name__ == "__main__":
    test_onefootball_parser()
