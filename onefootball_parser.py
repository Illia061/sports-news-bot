import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import logging
import random
import time
import json


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CONFIG = {
    'USER_AGENTS': [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0'
    ],
    'BASE_URL': 'https://onefootball.com/en/home',
    'NEWS_API_URL': 'https://onefootball.com/en/news',
    'MAX_NEWS': 15,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
    'RETRY_ATTEMPTS': 3,
    'RETRY_DELAY': 2,
    'REQUEST_DELAY': 1.5,  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ —Å—Ç–∞—Ç—å—è–º
}

KIEV_TZ = ZoneInfo("Europe/Kiev")

class OneFootballParser:
    def __init__(self):
        self.base_url = CONFIG['BASE_URL']
        self.news_url = CONFIG['NEWS_API_URL']
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": random.choice(CONFIG['USER_AGENTS']),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9,uk;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Referer": "https://onefootball.com/",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache"
        })
        

    def parse_publish_time(self, time_str: str, current_time: datetime = None) -> datetime:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≤—Ä–µ–º–µ–Ω–∏ –≤ –æ–±—ä–µ–∫—Ç datetime —Å –∫–∏–µ–≤—Å–∫–∏–º —á–∞—Å–æ–≤—ã–º –ø–æ—è—Å–æ–º (EEST)."""
        try:
            if not current_time:
                current_time = datetime.now(KIEV_TZ)
            logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä–µ–º–µ–Ω–∏: {time_str}, —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è: {current_time}")

            if 'ago' in time_str.lower():
                # –ü–∞—Ä—Å–∏–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "2 hours ago", "30 minutes ago")
                numbers = re.findall(r'\d+', time_str)
                if numbers:
                    value = int(numbers[0])
                    if 'second' in time_str.lower():
                        delta = timedelta(seconds=value)
                    elif 'minute' in time_str.lower():
                        delta = timedelta(minutes=value)
                    elif 'hour' in time_str.lower():
                        delta = timedelta(hours=value)
                    elif 'day' in time_str.lower():
                        delta = timedelta(days=value)
                    else:
        logger.error("‚ùå –û–®–ò–ë–ö–ê! –ù–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        logger.info("\nüîß –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –û–¢–õ–ê–î–ö–ï:")
        logger.info("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã onefootball_debug_*.html")
        logger.info("2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–∞–π—Ç –¥–æ—Å—Ç—É–ø–µ–Ω")
        logger.info("3. –í–æ–∑–º–æ–∂–Ω–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–π—Ç–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å")
    
    logger.info("=" * 60)
                        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º –º–∏–Ω—É—Ç—ã
                        delta = timedelta(minutes=value)
                    return current_time - delta
                else:
                    return current_time - timedelta(minutes=5)  # –î–µ—Ñ–æ–ª—Ç –¥–ª—è –Ω–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ "ago"

            # –ü–∞—Ä—Å–∏–º ISO —Ñ–æ—Ä–º–∞—Ç
            if 'T' in time_str:
                dt = datetime.fromisoformat(time_str.replace('Z', '+00:00')).astimezone(KIEV_TZ)
                logger.debug(f"–£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ ISO –≤—Ä–µ–º—è: {time_str} -> {dt}")
                return dt
            
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç—ã
            date_formats = [
                '%Y-%m-%d %H:%M %Z',
                '%Y-%m-%d %H:%M',
                '%Y-%m-%dT%H:%M:%S',
                '%d.%m.%Y %H:%M',
                '%d/%m/%Y %H:%M'
            ]
            
            for fmt in date_formats:
                try:
                    dt = datetime.strptime(time_str, fmt)
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=KIEV_TZ)
                    else:
                        dt = dt.astimezone(KIEV_TZ)
                    logger.debug(f"–£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ –≤—Ä–µ–º—è: {time_str} -> {dt}")
                    return dt
                except ValueError:
                    continue
            
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –≤—Ä–µ–º—è '{time_str}', –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–µ–µ")
            return current_time
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä–µ–º–µ–Ω–∏ '{time_str}': {e}")
            return current_time

    def get_page_content(self, url: str, attempt: int = 1) -> BeautifulSoup:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
        try:
            logger.info(f"üåê –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{CONFIG['RETRY_ATTEMPTS']}): {url}")
            
            # –ú–µ–Ω—è–µ–º User-Agent –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–∏
            self.session.headers.update({
                "User-Agent": random.choice(CONFIG['USER_AGENTS'])
            })
            
            response = self.session.get(url, timeout=20)
            response.raise_for_status()
            
            logger.info(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(response.content)} –±–∞–π—Ç")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö
            if attempt > 1:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –±—ã–ª–∞ –ø—Ä–æ–±–ª–µ–º–∞
                debug_filename = f'onefootball_debug_{attempt}.html'
                with open(debug_filename, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                logger.info(f"üîç HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏: {debug_filename}")
            
            return BeautifulSoup(response.text, "html.parser")
            
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–µ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt}): {e}")
            if attempt < CONFIG['RETRY_ATTEMPTS']:
                logger.info(f"‚è≥ –ñ–¥–µ–º {CONFIG['RETRY_DELAY']} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                time.sleep(CONFIG['RETRY_DELAY'])
                return self.get_page_content(url, attempt + 1)
            return None
        except Exception as e:
            logger.error(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt}): {e}")
            if attempt < CONFIG['RETRY_ATTEMPTS']:
                time.sleep(CONFIG['RETRY_DELAY'])
                return self.get_page_content(url, attempt + 1)
            return None

    def debug_page_structure(self, soup: BeautifulSoup, show_details: bool = False):
        """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        if not show_details:
            return
            
        logger.info("üîç –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –°–¢–†–ê–ù–ò–¶–´:")
        logger.info("=" * 50)
        
        # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö div'–æ–≤ —Å –∫–ª–∞—Å—Å–∞–º–∏
        all_divs = soup.find_all('div', class_=True)[:20]  # –ü–µ—Ä–≤—ã–µ 20
        logger.info(f"üì¶ –ù–∞–π–¥–µ–Ω–æ {len(all_divs)} div —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –∫–ª–∞—Å—Å–∞–º–∏ (–ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 20):")
        for i, div in enumerate(all_divs, 1):
            classes = ' '.join(div.get('class', []))
            logger.info(f"   {i:2d}. div class=\"{classes}\"")
        
        # –ê–Ω–∞–ª–∏–∑ article —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        articles = soup.find_all('article')
        logger.info(f"üì∞ –ù–∞–π–¥–µ–Ω–æ {len(articles)} article —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        for i, article in enumerate(articles[:5], 1):
            classes = ' '.join(article.get('class', []))
            logger.info(f"   {i}. article class=\"{classes}\"")
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
        links = soup.find_all('a', href=True)
        news_links = [link for link in links if any(word in link['href'] for word in ['/news/', '/match/', '/article/'])][:10]
        logger.info(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(news_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ (–ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10):")
        for i, link in enumerate(news_links, 1):
            href = link['href']
            text = link.get_text(strip=True)[:50]
            logger.info(f"   {i:2d}. {href} -> \"{text}...\"")
        
        logger.info("=" * 50)

    def find_news_articles_advanced(self, soup: BeautifulSoup) -> list:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏."""
        found_articles = []
        
        # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –ø–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º OneFootball
        logger.info("üîç –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –ø–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º OneFootball")
        
        modern_selectors = [
            # –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã OneFootball
            '[data-testid*="teaser"]',
            '[data-testid*="card"]',
            '[data-testid*="article"]',
            '[data-testid*="story"]',
            '.of-teaser',
            '.teaser-card',
            '.article-teaser',
            '.story-teaser',
            '[class*="Teaser"]',
            '[class*="Card"]',
            '[class*="Article"]'
        ]
        
        for selector in modern_selectors:
            elements = soup.select(selector)
            logger.info(f"   –°–µ–ª–µ–∫—Ç–æ—Ä '{selector}': –Ω–∞–π–¥–µ–Ω–æ {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            
            for element in elements:
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ —ç–ª–µ–º–µ–Ω—Ç–µ
                link = element.find('a', href=True)
                title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'h5', 'span', 'p'])
                
                if link and title_elem:
                    href = link.get('href', '')
                    title_text = title_elem.get_text(strip=True)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–æ–≤–æ—Å—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞ —Å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                    if (any(pattern in href for pattern in ['/news/', '/match/', '/article/', '/story/']) and
                        title_text and len(title_text) > 15 and len(title_text) < 200):
                        
                        found_articles.append({
                            'element': element,
                            'link': link,
                            'title': title_text,
                            'url': href,
                            'method': f'modern_{selector}'
                        })
        
        logger.info(f"   –ù–∞–π–¥–µ–Ω–æ {len(found_articles)} —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã")
        
        # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –ø–æ —Å—Å—ã–ª–∫–∞–º –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ (–µ—Å–ª–∏ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
        if len(found_articles) < 5:
            logger.info("üîç –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –Ω–æ–≤–æ—Å—Ç–Ω—ã–º —Å—Å—ã–ª–∫–∞–º")
            news_links = soup.find_all('a', href=True)
            
            for link in news_links:
                href = link.get('href', '')
                if any(pattern in href for pattern in ['/news/', '/match/', '/article/', '/story/']):
                    title_text = link.get_text(strip=True)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    if (title_text and 15 < len(title_text) < 200 and
                        not any(skip in title_text.lower() for skip in 
                               ['menu', 'navigation', 'cookie', 'subscribe', 'follow', 'share'])):
                        
                        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
                        parent_container = link.find_parent(['article', 'div', 'li', 'section'])
                        if parent_container:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥–æ–±–∞–≤–∏–ª–∏ –ª–∏ —É–∂–µ —ç—Ç—É —Å—Ç–∞—Ç—å—é
                            if not any(art['url'] == href for art in found_articles):
                                found_articles.append({
                                    'element': parent_container,
                                    'link': link,
                                    'title': title_text,
                                    'url': href,
                                    'method': 'link_based'
                                })
        
        logger.info(f"   –ù–∞–π–¥–µ–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ —Å—Å—ã–ª–∫–∏: {len(found_articles)} —Å—Ç–∞—Ç–µ–π –≤—Å–µ–≥–æ")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        unique_articles = []
        seen_urls = set()
        for article in found_articles:
            normalized_url = article['url'].split('?')[0]  # –£–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            if normalized_url not in seen_urls:
                unique_articles.append(article)
                seen_urls.add(normalized_url)
        
        logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {len(unique_articles)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
        return unique_articles

    def extract_article_data(self, article_data: dict, current_time: datetime) -> dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–∏."""
        try:
            element = article_data['element']
            link = article_data['link']
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            url = link.get('href', '')
            if not url.startswith('http'):
                url = urljoin(self.base_url, url)
            
            title = article_data['title']
            
            # –ò—â–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –±–æ–ª–µ–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ
            time_elem = None
            time_str = ""
            
            # –ü–æ–∏—Å–∫ –≤—Ä–µ–º–µ–Ω–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
            time_selectors = [
                'time[datetime]',
                '[data-testid*="time"]',
                '[class*="time"]',
                '[class*="date"]',
                '.timestamp',
                '.publish-time',
                '.article-time'
            ]
            
            for selector in time_selectors:
                time_elem = element.select_one(selector)
                if time_elem:
                    time_str = time_elem.get('datetime', '') or time_elem.get_text(strip=True)
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤—Ä–µ–º–µ–Ω–∏ –≤ —Ç–µ–∫—Å—Ç–µ
            if not time_str:
                element_text = element.get_text()
                time_patterns = [
                    r'(\d+)\s*hours?\s*ago',
                    r'(\d+)\s*minutes?\s*ago',
                    r'(\d+)\s*days?\s*ago'
                ]
                
                for pattern in time_patterns:
                    match = re.search(pattern, element_text, re.IGNORECASE)
                    if match:
                        time_str = match.group(0)
                        break
            
            publish_time = self.parse_publish_time(time_str, current_time) if time_str else current_time
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–æ–ª–µ–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ
            image_url = ""
            image_selectors = [
                'img[src]',
                'img[data-src]',
                'img[data-lazy-src]',
                '[style*="background-image"]'
            ]
            
            for selector in image_selectors:
                img_elem = element.select_one(selector)
                if img_elem:
                    if 'background-image' in selector:
                        style = img_elem.get('style', '')
                        bg_match = re.search(r'background-image:\s*url\(["\']?([^"\']+)["\']?\)', style)
                        if bg_match:
                            image_url = bg_match.group(1)
                    else:
                        image_url = (img_elem.get('src', '') or 
                                   img_elem.get('data-src', '') or 
                                   img_elem.get('data-lazy-src', ''))
                    
                    if image_url:
                        if not image_url.startswith('http'):
                            image_url = urljoin(self.base_url, image_url)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–µ –º–µ–ª–∫–∞—è –∏–∫–æ–Ω–∫–∞
                        if not any(skip in image_url.lower() for skip in 
                                 ['icon', 'logo', 'avatar', '16x16', '32x32', 'favicon']):
                            break
                        else:
                            image_url = ""  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –µ—Å–ª–∏ —ç—Ç–æ –∏–∫–æ–Ω–∫–∞
            
            # –ò—â–µ–º –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            summary = ""
            summary_selectors = [
                '[data-testid*="description"]',
                '[data-testid*="excerpt"]',
                '.description',
                '.excerpt',
                '.summary',
                '.teaser-text',
                'p'
            ]
            
            for selector in summary_selectors:
                summary_elem = element.select_one(selector)
                if summary_elem:
                    potential_summary = summary_elem.get_text(strip=True)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ
                    if (potential_summary and len(potential_summary) > 20 and 
                        potential_summary.lower() != title.lower()):
                        summary = potential_summary
                        break
            
            result = {
                'title': title,
                'url': url,
                'summary': summary,
                'publish_time': publish_time,
                'image_url': image_url,
                'method': article_data['method'],
                'time_str': time_str
            }
            
            logger.info(f"üì∞ –ò–∑–≤–ª–µ—á–µ–Ω–∞ —Å—Ç–∞—Ç—å—è ({article_data['method']}): {title[:50]}...")
            logger.info(f"   üîó URL: {url}")
            logger.info(f"   ‚è∞ –í—Ä–µ–º—è: {time_str} -> {publish_time.strftime('%H:%M %d.%m')}")
            if image_url:
                logger.info(f"   üñºÔ∏è  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_url[:50]}...")
            if summary:
                logger.info(f"   üìù –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {summary[:50]}...")
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å–∏: {e}")
            return None

    def fetch_full_article(self, url: str) -> tuple[str, str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ —Å—Ç–∞—Ç—å–∏."""
        try:
            logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏...")
            
            soup = self.get_page_content(url)
            if not soup:
                return "", ""

            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ OneFootball
            content_selectors = [
                # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è OneFootball
                '[data-testid*="article-body"]',
                '[data-testid*="story-body"]',
                '[data-testid*="content"]',
                '.article-content',
                '.story-content',
                '.post-content',
                '.main-content',
                'article [class*="content"]',
                'article [class*="text"]',
                'article [class*="body"]',
                # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                'article',
                '.content',
                'main'
            ]

            article_text = ""
            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    # –£–¥–∞–ª—è–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                    for unwanted in content_div.find_all(['script', 'style', 'iframe', 'nav', 'aside', 'footer', 'header']):
                        unwanted.decompose()
                    
                    # –£–¥–∞–ª—è–µ–º —Ä–µ–∫–ª–∞–º–Ω—ã–µ –±–ª–æ–∫–∏
                    for ad in content_div.find_all(['div', 'section'], class_=re.compile(r'ad|banner|promo', re.I)):
                        ad.decompose()
                    
                    paragraphs = content_div.find_all('p')
                    if paragraphs:
                        meaningful_paragraphs = []
                        for p in paragraphs:
                            text = p.get_text(strip=True)
                            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                            if (len(text) > 30 and 
                                not any(skip in text.lower() for skip in 
                                       ['cookie', 'advertisement', 'subscribe', 'follow us', 
                                        'photo:', 'source:', 'getty images', 'read more'])):
                                meaningful_paragraphs.append(text)
                        
                        if meaningful_paragraphs:
                            article_text = '\n'.join(meaningful_paragraphs)
                            logger.info(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç —á–µ—Ä–µ–∑ {selector}: {len(article_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                            break
                    else:
                        # –ï—Å–ª–∏ –Ω–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤, –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
                        article_text = content_div.get_text(strip=True)
                        if len(article_text) > 100:
                            logger.info(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ {selector}: {len(article_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                            break

            # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –ø—Ä–æ–±—É–µ–º –æ–±—â–∏–π –ø–æ–∏—Å–∫
            if not article_text or len(article_text) < 100:
                logger.info("   üîÑ –û—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–µ –¥–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –ø—Ä–æ–±—É–µ–º –æ–±—â–∏–π –ø–æ–∏—Å–∫...")
                all_paragraphs = soup.find_all('p')
                meaningful_paragraphs = []
                
                for p in all_paragraphs:
                    text = p.get_text(strip=True)
                    if (len(text) > 40 and
                        not any(skip in text.lower() for skip in 
                               ['cookie', 'advertisement', 'subscribe', 'photo', 'source', 
                                'menu', 'navigation', 'follow', 'share', 'related articles'])):
                        meaningful_paragraphs.append(text)
                
                if meaningful_paragraphs:
                    article_text = '\n'.join(meaningful_paragraphs)
                    # –û–±—Ä–µ–∑–∞–µ–º –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
                    if len(article_text) > 2000:
                        sentences = re.split(r'[.!?]+', article_text)
                        trimmed_content = ""
                        current_length = 0
                        for sentence in sentences:
                            sentence = sentence.strip()
                            if sentence and current_length + len(sentence) <= 2000:
                                trimmed_content += sentence + '. '
                                current_length += len(sentence) + 2
                            else:
                                break
                        article_text = trimmed_content.rstrip()
                    logger.info(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –æ–±—â–∏–º –ø–æ–∏—Å–∫–æ–º: {len(article_text)} —Å–∏–º–≤–æ–ª–æ–≤")

            # –ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏
            image_selectors = [
                'meta[property="og:image"]',
                'meta[name="twitter:image"]',
                '[data-testid*="hero-image"] img',
                '[data-testid*="featured-image"] img',
                '.article-image img:first-of-type',
                '.story-image img:first-of-type',
                '.featured-image img',
                'article img:first-of-type',
                '.main-image img'
            ]

            image_url = ""
            for selector in image_selectors:
                img_elem = soup.select_one(selector)
                if img_elem:
                    if img_elem.name == 'meta':
                        image_url = img_elem.get('content', '')
                    else:
                        image_url = (img_elem.get('src', '') or 
                                   img_elem.get('data-src', '') or 
                                   img_elem.get('data-lazy-src', ''))
                    
                    if image_url:
                        image_url = urljoin(url, image_url)
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        if not any(small in image_url.lower() for small in 
                                 ['icon', 'logo', 'thumb', 'avatar', 'placeholder', '150x', '100x']):
                            logger.info(f"   üñºÔ∏è  –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ {selector}")
                            break
                        else:
                            image_url = ""  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            
            return article_text, image_url

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
            return "", ""

    def get_latest_news(self, since_time: datetime = None) -> list:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å OneFootball —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –ø–æ–∏—Å–∫–∞."""
        current_time = datetime.now(KIEV_TZ)
        if since_time is None:
            current_hour = current_time.hour
            current_minute = current_time.minute
            if 5 <= current_hour < 6 and current_minute >= 50 or current_hour == 6 and current_minute <= 10:
                since_time = current_time.replace(hour=1, minute=0, second=0, microsecond=0)
                logger.info(f"–†–µ–∂–∏–º 5 —á–∞—Å–æ–≤: since_time —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ {since_time}")
            else:
                since_time = current_time - timedelta(minutes=20)
                logger.info(f"–†–µ–∂–∏–º 20 –º–∏–Ω—É—Ç: since_time —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ {since_time}")

        logger.info(f"üîç –ó–∞–≥—Ä—É–∂–∞–µ–º OneFootball (—Å {since_time.strftime('%H:%M %d.%m.%Y')})...")

        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ URL
        urls_to_try = [
            self.base_url,
            self.news_url,
            'https://onefootball.com/en/news/all',
            'https://onefootball.com/en/news/football'
        ]
        
        soup = None
        successful_url = None
        
        for url in urls_to_try:
            logger.info(f"üåê –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å: {url}")
            soup = self.get_page_content(url)
            if soup:
                successful_url = url
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {url}")
                break
            else:
                logger.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å: {url}")
                time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
        
        if not soup:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–∏–Ω –∏–∑ URL")
            return []

        # –û—Ç–ª–∞–¥–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö)
        self.debug_page_structure(soup, show_details=False)
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π
        found_articles = self.find_news_articles_advanced(soup)
        
        if not found_articles:
            logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞")
            # –ü—Ä–∏ –ø–æ–ª–Ω–æ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∫–ª—é—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –æ—Ç–ª–∞–¥–∫—É
            self.debug_page_structure(soup, show_details=True)
            return []
        
        logger.info(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(found_articles)} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π...")
        
        news_items = []
        processed_count = 0
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        articles_to_process = found_articles[:CONFIG['MAX_NEWS']]
        
        for i, article_data in enumerate(articles_to_process, 1):
            try:
                logger.info(f"üì∞ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å—é {i}/{len(articles_to_process)}...")
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                article_info = self.extract_article_data(article_data, current_time)
                if not article_info:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                if article_info['publish_time'] < since_time:
                    logger.info(f"   ‚è∞ –°—Ç–∞—Ç—å—è —Å—Ç–∞—Ä–∞—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–≤—Ä–µ–º—è: {article_info['publish_time'].strftime('%H:%M %d.%m')})")
                    continue
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏
                logger.info(f"   üìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç...")
                article_text, full_image_url = self.fetch_full_article(article_info['url'])
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ –ø–æ–ª–Ω–æ–π —Å—Ç–∞—Ç—å–∏, –µ—Å–ª–∏ –æ–Ω–æ –ª—É—á—à–µ
                final_image_url = full_image_url or article_info['image_url']
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ–±—ä–µ–∫—Ç –Ω–æ–≤–æ—Å—Ç–∏ (–ë–ï–ó –ø–µ—Ä–µ–≤–æ–¥–∞ - —ç—Ç–æ –¥–µ–ª–∞–µ—Ç ai_processor)
                news_item = {
                    'title': article_info['title'],  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    'url': article_info['url'],
                    'content': article_text,  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
                    'summary': article_info['summary'],  # –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                    'publish_time': article_info['publish_time'],
                    'image_url': final_image_url,
                    'source': 'OneFootball',
                    'extraction_method': article_info['method']
                }
                
                news_items.append(news_item)
                processed_count += 1
                
                logger.info(f"   ‚úÖ –°—Ç–∞—Ç—å—è –¥–æ–±–∞–≤–ª–µ–Ω–∞: {article_info['title'][:50]}...")
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ —Å—Ç–∞—Ç—å—è–º
                if i < len(articles_to_process):
                    time.sleep(CONFIG['REQUEST_DELAY'])

            except Exception as e:
                logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏ {i}: {e}")
                continue

        logger.info(f"‚úÖ OneFootball: –Ω–∞–π–¥–µ–Ω–æ {processed_count} –∏–∑ {len(found_articles)} —Å—Ç–∞—Ç–µ–π")
        logger.info("   üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø–µ—Ä–µ–≤–æ–¥ –±—É–¥—É—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –≤ ai_processor.py")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
        news_items.sort(key=lambda x: x.get('publish_time') or datetime.min.replace(tzinfo=KIEV_TZ), reverse=True)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if news_items:
            logger.info("üìä –°–ü–ò–°–û–ö –ù–ê–ô–î–ï–ù–ù–´–• –ù–û–í–û–°–¢–ï–ô (—Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ):")
            for i, item in enumerate(news_items, 1):
                publish_time = item.get('publish_time')
                time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
                method = item.get('extraction_method', 'unknown')
                logger.info(f"   {i:2d}. [{method}] {item['title'][:50]}... ({time_str})")
        
        return news_items


def get_latest_news(since_time: datetime = None) -> list:
    """–§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å main.py."""
    parser = OneFootballParser()
    return parser.get_latest_news(since_time)


if __name__ == "__main__":
    logger.info("üéØ –¢–ï–°–¢–ò–†–£–ï–ú –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–ê–†–°–ï–† –î–õ–Ø ONEFOOTBALL")
    logger.info("=" * 60)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    test_time = datetime.now(KIEV_TZ) - timedelta(hours=6)  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 6 —á–∞—Å–æ–≤
    
    articles = get_latest_news(since_time=test_time)
    
    if articles:
        logger.info(f"‚úÖ –£–°–ü–ï–®–ù–û! –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        logger.info("üèÜ –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        
        methods_count = {}
        for article in articles:
            method = article.get('extraction_method', 'unknown')
            methods_count[method] = methods_count.get(method, 0) + 1
        
        logger.info("üìà –ü–æ –º–µ—Ç–æ–¥–∞–º –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:")
        for method, count in methods_count.items():
            logger.info(f"   {method}: {count} —Å—Ç–∞—Ç–µ–π")
        
        logger.info("\nüì∞ –°–ü–ò–°–û–ö –ù–û–í–û–°–¢–ï–ô (—Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è ai_processor):")
        for i, article in enumerate(articles, 1):
            publish_time = article.get('publish_time')
            time_str = publish_time.strftime('%H:%M %d.%m') if publish_time else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            logger.info(f"   {i:2d}. {article['title'][:60]}... ({time_str})")
            if article.get('image_url'):
                logger.info(f"       üñºÔ∏è  {article['image_url'][:60]}...")
            logger.info(f"       üìÑ –ö–æ–Ω—Ç–µ–Ω—Ç: {len(article.get('content', ''))} —Å–∏–º–≤–æ–ª–æ–≤")
    else:
        logger.error("‚ùå –û–®–ò–ë–ö–ê! –ù–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        logger.info("\nüîß –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –û–¢–õ–ê–î–ö–ï:")
        logger.info("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã onefootball_debug_*.html")
        logger.info("2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–∞–π—Ç –¥–æ—Å—Ç—É–ø–µ–Ω")
        logger.info("3. –í–æ–∑–º–æ–∂–Ω–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–π—Ç–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å")
    
    logger.info("=" * 60)
