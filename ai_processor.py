import os
import requests
from typing import Dict, Any
from urllib.parse import urlparse
import google.generativeai as genai
import time
from bs4 import BeautifulSoup

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_AVAILABLE = False
model = None

def init_gemini():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç–∞ Gemini"""
    global GEMINI_AVAILABLE, model
    if not GEMINI_API_KEY:
        print("‚ö†Ô∏è GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω - AI —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç–∫–ª—é—á–µ–Ω—ã")
        return
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel("models/gemini-pro")
        GEMINI_AVAILABLE = True
        print("‚úÖ Gemini –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Gemini: {e}")

def has_gemini_key() -> bool:
    if not GEMINI_AVAILABLE:
        init_gemini()
    return GEMINI_AVAILABLE

def fetch_full_article_content(url: str) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ URL"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏ (–∞–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –ø–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä—É football.ua)
        content_selectors = [
            '.article-content',
            '.post-content', 
            '.entry-content',
            '[class*="content"]',
            '.article-body',
            '.post-body'
        ]
        
        article_text = ""
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                for unwanted in content_div.find_all(['script', 'style', 'iframe', 'ads']):
                    unwanted.decompose()
                
                article_text = content_div.get_text(strip=True)
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä, –±–µ—Ä–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        if not article_text:
            paragraphs = soup.find_all('p')
            article_text = ' '.join([p.get_text(strip=True) for p in paragraphs])
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        article_text = ' '.join(article_text.split())  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        
        return article_text[:2000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è AI
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏: {e}")
        return ""

def create_enhanced_summary(article_data: Dict[str, Any]) -> str:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—é–º–µ —á–µ—Ä–µ–∑ Gemini"""
    if not has_gemini_key() or not model:
        return article_data.get('summary', '') or article_data.get('title', '')

    title = article_data.get('title', '')
    content = article_data.get('content', '')
    
    # –ü–∞—Ä—Å–µ—Ä —É–∂–µ –∑–∞–≥—Ä—É–∑–∏–ª –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
    if content:
        print(f"ü§ñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ AI")
    else:
        # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º summary
        content = article_data.get('summary', '') or title
        print(f"‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º summary: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")

    prompt = f"""–¢–∏ —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ñ—É—Ç–±–æ–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω.
–ü–µ—Ä–µ—Ñ—Ä–∞–∑—É–π —ñ —Å—Ç–≤–æ—Ä–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∏–π –≤–∏–∫–ª–∞–¥ —Ü—ñ—î—ó —Ñ—É—Ç–±–æ–ª—å–Ω–æ—ó –Ω–æ–≤–∏–Ω–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é.

–í–∏–º–æ–≥–∏:
- –ó—Ä–æ–∑—É–º—ñ–ª–æ —Ç–∞ —Ü—ñ–∫–∞–≤–æ
- –ó–±–µ—Ä–µ–∂–∏ –≤—Å—ñ –≤–∞–∂–ª–∏–≤—ñ —Ñ–∞–∫—Ç–∏
- –£–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é
- –ë–µ–∑ –∑–∞–π–≤–∏—Ö –¥–µ—Ç–∞–ª–µ–π
- –Ø–∫—â–æ —î —Ä–µ–π—Ç–∏–Ω–≥ ‚Äî –ø—É–±–ª—ñ–∫—É–π –ø–æ–≤–Ω—ñ—Å—Ç—é
- –Ø–∫—â–æ —î –ø—Ä—è–º–∞ –º–æ–≤–∞ ‚Äî –≤–∏–∫–ª–∞–¥–∏ —ó—ó –∫–æ—Ä–æ—Ç–∫–æ –Ω–∞ 3‚Äì4 —Ä–µ—á–µ–Ω–Ω—è
- –ú–∞–∫—Å–∏–º—É–º 200-250 —Å–ª—ñ–≤

–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
–¢–µ–∫—Å—Ç: {content}

–°—Ç–∏—Å–ª–∏–π –≤–∏–∫–ª–∞–¥:
"""
    try:
        response = model.generate_content(prompt)
        summary = response.text.strip()
        # Ensure summary isn't just the title
        if summary.lower() == title.lower():
            return content[:200] + '...' if len(content) > 200 else content
        return summary
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ Gemini: {e}")
        time.sleep(1)  # Small delay to prevent rate limiting
        return content[:200] + '...' if len(content) > 200 else content

def format_for_social_media(article_data: Dict[str, Any]) -> str:
    title = article_data.get('title', '')
    content = article_data.get('content', '')
    summary = article_data.get('summary', '')

    if has_gemini_key():
        ai_summary = create_enhanced_summary({
            'title': title,
            'content': content,
            'summary': summary,
            'url': article_data.get('url', '') or article_data.get('link', '')  # –ü–µ—Ä–µ–¥–∞–µ–º URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        })
    else:
        ai_summary = summary or content[:200] + '...' if len(content) > 200 else content

    # Remove unwanted prefixes
    unwanted_prefixes = ["–Ü–Ω—à–µ", "–Ü—Ç–∞–ª—ñ—è", "–Ü—Å–ø–∞–Ω—ñ—è", "–ù—ñ–º–µ—á—á–∏–Ω–∞", "–ß–µ–º–ø—ñ–æ–Ω–∞—Ç", "–°—å–æ–≥–æ–¥–Ω—ñ", "–í—á–µ—Ä–∞"]
    for prefix in unwanted_prefixes:
        if ai_summary.startswith(prefix):
            ai_summary = ai_summary[len(prefix):].strip(": ").lstrip()

    post = f"<b>‚öΩ {title}</b>\n\n"
    if ai_summary and ai_summary != title:
        post += f"{ai_summary}\n\n"

    post += "#—Ñ—É—Ç–±–æ–ª #–Ω–æ–≤–∏–Ω–∏ #—Å–ø–æ—Ä—Ç"
    return post

def download_image(image_url: str, filename: str = None) -> str:
    try:
        if not image_url:
            return ""
        images_dir = "images"
        os.makedirs(images_dir, exist_ok=True)
        if not filename:
            parsed_url = urlparse(image_url)
            filename = os.path.basename(parsed_url.path)
            if not filename or '.' not in filename:
                filename = f"image_{hash(image_url) % 10000}.jpg"
        filepath = os.path.join(images_dir, filename)
        response = requests.get(image_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        response.raise_for_status()
        with open(filepath, 'wb') as f:
            f.write(response.content)
        return filepath
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        return ""

def process_article_for_posting(article_data: Dict[str, Any]) -> Dict[str, Any]:
    post_text = format_for_social_media(article_data)
    image_path = download_image(article_data['image_url']) if article_data.get('image_url') else ""
    return {
        'title': article_data.get('title', ''),
        'post_text': post_text,
        'image_path': image_path,
        'image_url': article_data.get('image_url', ''),
        'url': article_data.get('url', '') or article_data.get('link', ''),
        'summary': article_data.get('summary', '')
    }

# Old compatible interfaces
def summarize_news(title: str, url: str, content: str = '') -> str:
    article_data = {'title': title, 'url': url, 'content': content, 'summary': title}
    return create_enhanced_summary(article_data) if has_gemini_key() else f"üî∏ {title}"

def simple_summarize(title: str, url: str) -> str:
    return f"üî∏ {title}"

