import os
import requests
from typing import Dict, Any
from urllib.parse import urlparse
import google.generativeai as genai
import time
from bs4 import BeautifulSoup
import logging
import random

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
CONFIG = {
    'CONTENT_MAX_LENGTH': 2000,
    'SUMMARY_MAX_WORDS': 150,
    'USER_AGENTS': [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0'
    ]
}

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_AVAILABLE = False
model = None

def init_gemini():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç–∞ Gemini."""
    global GEMINI_AVAILABLE, model
    if GEMINI_AVAILABLE:  # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        return
    if not GEMINI_API_KEY:
        logger.warning("GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω - AI —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç–∫–ª—é—á–µ–Ω—ã")
        return
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel("gemini-2.5-flash")
        GEMINI_AVAILABLE = True
        logger.info("Gemini –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Gemini: {e}")

def has_gemini_key() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–∞ Gemini –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ."""
    if not GEMINI_AVAILABLE:
        init_gemini()
    return GEMINI_AVAILABLE

def fetch_full_article_content(url: str) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ URL."""
    try:
        headers = {'User-Agent': random.choice(CONFIG['USER_AGENTS'])}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        content_selectors = (
            [
                '.RichTextStoryBody', '.Story__Body', '.ArticleBody',
                '[data-module="ArticleBody"]', '.story-body', '.article-body'
            ] if 'espn.com' in url else
            [
                # OneFootball —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                '[data-testid="article-body"]', '.ArticleBody',
                # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                '.article-content', '.post-content', '.entry-content',
                '[class*="content"]', '.article-body', '.post-body'
            ] if 'onefootball.com' in url else
            [
                '.article-content', '.post-content', '.entry-content',
                '[class*="content"]', '.article-body', '.post-body'
            ]
        )

        article_text = ""
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                for unwanted in content_div.find_all(['script', 'style', 'iframe', 'ads', 'aside']):
                    unwanted.decompose()
                article_text = content_div.get_text(strip=True)
                break

        if not article_text:
            paragraphs = soup.find_all('p')
            article_text = ' '.join(p.get_text(strip=True) for p in paragraphs)

        article_text = ' '.join(article_text.split())
        return article_text[:CONFIG['CONTENT_MAX_LENGTH']]

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
        return ""

def create_basic_summary(article_data: Dict[str, Any]) -> str:
    """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI."""
    content = article_data.get('content', '')
    summary = article_data.get('summary', '')
    title = article_data.get('title', '')

    if content and len(content) > 50:
        sentences = content.split('. ')
        meaningful_sentences = [s.strip() for s in sentences if len(s.strip()) > 20][:2]
        if meaningful_sentences:
            result = '. '.join(meaningful_sentences)
            return result + '.' if not result.endswith('.') else result
    return summary or title

def create_enhanced_summary(article_data: Dict[str, Any]) -> str:
    """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑—é–º–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Gemini –∏–ª–∏ –±–∞–∑–æ–≤–æ–µ —Ä–µ–∑—é–º–µ."""
    title = article_data.get('title', '')
    content = article_data.get('content', '')
    summary = article_data.get('summary', '')
    url = article_data.get('url', '')
    source = article_data.get('source', '')
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–¥–ª—è OneFootball)
    processed_content = article_data.get('processed_content', '')
    is_onefootball = source == 'OneFootball'
    is_espn_translated = source == 'ESPN Soccer' and article_data.get('original_content')

    if not has_gemini_key() or not model:
        return create_basic_summary(article_data)

    # –î–ª—è OneFootball –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –µ—Å–ª–∏ –Ω–µ—Ç processed_content
    if is_onefootball and not processed_content:
        logger.info(f"OneFootball: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è AI –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –∫–æ—Ä–æ—Ç–∫–∏–π –∏ –Ω–µ—Ç –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –∑–∞–≥—Ä—É–∂–∞–µ–º
        if len(content) < 100 and url:
            logger.info(f"OneFootball: –∫–æ–Ω—Ç–µ–Ω—Ç –∫–æ—Ä–æ—Ç–∫–∏–π ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤), –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç...")
            content = fetch_full_article_content(url) or summary or title
            logger.info(f"OneFootball: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    elif is_onefootball and processed_content:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        logger.info(f"OneFootball: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç ({len(processed_content)} —Å–∏–º–≤–æ–ª–æ–≤)")
        return processed_content

    # –î–ª—è –¥—Ä—É–≥–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    if not is_onefootball and len(content) < 100 and url:
        logger.info(f"–ö–æ–Ω—Ç–µ–Ω—Ç –∫–æ—Ä–æ—Ç–∫–∏–π ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤), –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç...")
        content = fetch_full_article_content(url) or summary or title
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")

    if len(content) < 20:
        logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return summary or title

    logger.info(f"–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Gemini {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø—Ä–æ–º–ø—Ç–∞
    if is_onefootball:
        prompt = f"""–¢–∏ —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ñ—É—Ç–±–æ–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω. –ü–µ—Ä–µ–∫–ª–∞–¥–∏ –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏–π —Ç–µ–∫—Å—Ç —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é —Ç–∞ —Å—Ç–≤–æ—Ä–∏ –ö–û–†–û–¢–ö–ò–ô –ø–æ—Å—Ç –¥–ª—è Telegram (–º–∞–∫—Å. {CONFIG['SUMMARY_MAX_WORDS']} —Å–ª—ñ–≤).

–ü—Ä–∞–≤–∏–ª–∞:
- –ü–µ—Ä–µ–∫–ª–∞–¥–∏ —Ç–æ—á–Ω–æ —Ç–∞ –ø—Ä–∏—Ä–æ–¥–Ω–æ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é
- –¢—ñ–ª—å–∫–∏ –∫–ª—é—á–æ–≤—ñ —Ñ–∞–∫—Ç–∏, –±–µ–∑ –ø—Ä–∏–∫—Ä–∞—Å
- –ú–∞–∫—Å–∏–º—É–º 1-2 —Ä–µ—á–µ–Ω–Ω—è –ø—Ä—è–º–æ—ó –º–æ–≤–∏
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –≥–æ–ª–æ–≤–Ω–∏–π —Ñ–∞–∫—Ç (1-2 —Ä–µ—á–µ–Ω–Ω—è), –¥–µ—Ç–∞–ª—ñ (2-4 —Ä–µ—á–µ–Ω–Ω—è)
- –î–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä—ñ–≤: –≤–∫–∞–∑—É–π —Å—É–º—É, —Ç–µ—Ä–º—ñ–Ω –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É
- –î–ª—è –º–∞—Ç—á—ñ–≤: —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∫–ª—é—á–æ–≤—ñ –º–æ–º–µ–Ω—Ç–∏

–ó–∞–≥–æ–ª–æ–≤–æ–∫ (–∞–Ω–≥–ª.): {title}
–¢–µ–∫—Å—Ç (–∞–Ω–≥–ª.): {content}

–ö–û–†–û–¢–ö–ò–ô –ü–û–°–¢ –£–ö–†–ê–á–ù–°–¨–ö–û–Æ:"""
    elif is_espn_translated:
        prompt = f"""–¢–∏ —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ñ—É—Ç–±–æ–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω. –°—Ç–≤–æ—Ä–∏ –ö–û–†–û–¢–ö–ò–ô –ø–æ—Å—Ç –¥–ª—è Telegram (–º–∞–∫—Å. {CONFIG['SUMMARY_MAX_WORDS']} —Å–ª—ñ–≤) –∑ –ø–µ—Ä–µ–∫–ª–∞–¥–µ–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É ESPN.

–ü—Ä–∞–≤–∏–ª–∞:
- –¢—ñ–ª—å–∫–∏ –∫–ª—é—á–æ–≤—ñ —Ñ–∞–∫—Ç–∏
- –ö–æ–Ω—Ç–µ–Ω—Ç —É–∂–µ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é
- –ú–∞–∫—Å–∏–º—É–º 1-2 —Ä–µ—á–µ–Ω–Ω—è –ø—Ä—è–º–æ—ó –º–æ–≤–∏
- –î–ª—è —Ä–µ–π—Ç–∏–Ω–≥—ñ–≤: –ª–∏—à–µ —Ç–æ–ø-5
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –≥–æ–ª–æ–≤–Ω–∏–π —Ñ–∞–∫—Ç (1-2 —Ä–µ—á–µ–Ω–Ω—è), –¥–µ—Ç–∞–ª—ñ (2-4 —Ä–µ—á–µ–Ω–Ω—è)

–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
–¢–µ–∫—Å—Ç: {content}

–ö–û–†–û–¢–ö–ò–ô –ü–û–°–¢:"""
    else:
        prompt = f"""–¢–∏ —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ñ—É—Ç–±–æ–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω. –°—Ç–≤–æ—Ä–∏ –ö–û–†–û–¢–ö–ò–ô –ø–æ—Å—Ç –¥–ª—è Telegram (–º–∞–∫—Å. {CONFIG['SUMMARY_MAX_WORDS']} —Å–ª—ñ–≤).

–ü—Ä–∞–≤–∏–ª–∞:
- –¢—ñ–ª—å–∫–∏ –∫–ª—é—á–æ–≤—ñ —Ñ–∞–∫—Ç–∏, –±–µ–∑ –ø—Ä–∏–∫—Ä–∞—Å
- –£–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é
- –ú–∞–∫—Å–∏–º—É–º 1-2 —Ä–µ—á–µ–Ω–Ω—è –ø—Ä—è–º–æ—ó –º–æ–≤–∏
- –ù–µ –ø–æ–≤—Ç–æ—Ä—é–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
- –î–ª—è —Ä–µ–π—Ç–∏–Ω–≥—ñ–≤: –ª–∏—à–µ —Ç–æ–ø-5
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –≥–æ–ª–æ–≤–Ω–∏–π —Ñ–∞–∫—Ç (1-2 —Ä–µ—á–µ–Ω–Ω—è), –¥–µ—Ç–∞–ª—ñ (2-4 —Ä–µ—á–µ–Ω–Ω—è)

–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
–¢–µ–∫—Å—Ç: {content}

–ö–û–†–û–¢–ö–ò–ô –ü–û–°–¢:"""

    try:
        response = model.generate_content(prompt)
        summary_result = response.text.strip()
        if summary_result.lower() == title.lower():
            logger.warning("AI –≤–µ—Ä–Ω—É–ª —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç")
            return content[:200] + '...' if len(content) > 200 else content
        logger.info(f"AI –æ–±—Ä–∞–±–æ—Ç–∞–ª –∫–æ–Ω—Ç–µ–Ω—Ç: {len(summary_result)} —Å–∏–º–≤–æ–ª–æ–≤")
        return summary_result
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ Gemini: {e}")
        time.sleep(1)
        return content[:200] + '...' if len(content) > 200 else content

def format_for_social_media(article_data: Dict[str, Any]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—å—é –¥–ª—è —Å–æ—Ü—Å–µ—Ç–µ–π."""
    title = article_data.get('title', '')
    content = article_data.get('content', '')
    summary = article_data.get('summary', '')
    url = article_data.get('url', '') or article_data.get('link', '')
    source = article_data.get('source', '')

    logger.info(f"–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è —Å–æ—Ü—Å–µ—Ç–µ–π [{source}]: {title[:50]}...")
    
    # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ
    ai_summary = create_enhanced_summary({
        'title': title, 
        'content': content, 
        'summary': summary,
        'url': url, 
        'source': source, 
        'original_content': article_data.get('original_content', ''),
        'processed_content': article_data.get('processed_content', '')
    })

    # –£–±–∏—Ä–∞–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
    unwanted_prefixes = ["–Ü–Ω—à–µ", "–Ü—Ç–∞–ª—ñ—è", "–Ü—Å–ø–∞–Ω—ñ—è", "–ù—ñ–º–µ—á—á–∏–Ω–∞", "–ß–µ–º–ø—ñ–æ–Ω–∞—Ç", "–°—å–æ–≥–æ–¥–Ω—ñ", "–í—á–µ—Ä–∞"]
    for prefix in unwanted_prefixes:
        if ai_summary.startswith(prefix):
            ai_summary = ai_summary[len(prefix):].strip(": ").lstrip()

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø–æ—Å—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    if source == 'OneFootball':
        post = f"<b>üåç {title}</b>\n\n{ai_summary}\n\nüì∞ OneFootball\n#—Ñ—É—Ç–±–æ–ª #–Ω–æ–≤–∏–Ω–∏ #—Å–≤—ñ—Ç"
    elif source == 'ESPN Soccer':
        post = f"<b>üåç {title}</b>\n\n{ai_summary}\n\nüì∞ ESPN Soccer\n#—Ñ—É—Ç–±–æ–ª #–Ω–æ–≤–∏–Ω–∏ #ESPN #—Å–≤—ñ—Ç"
    else:
        post = f"<b>‚öΩ {title}</b>\n\n{ai_summary}\n\n#—Ñ—É—Ç–±–æ–ª #–Ω–æ–≤–∏–Ω–∏ #—Å–ø–æ—Ä—Ç"
    
    logger.info(f"–ì–æ—Ç–æ–≤—ã–π –ø–æ—Å—Ç [{source}]: {len(post)} —Å–∏–º–≤–æ–ª–æ–≤")
    return post

def download_image(image_url: str, filename: str = None) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ URL."""
    if not image_url:
        return ""
    try:
        images_dir = "images"
        os.makedirs(images_dir, exist_ok=True)
        if not filename:
            parsed_url = urlparse(image_url)
            filename = os.path.basename(parsed_url.path) or f"image_{hash(image_url) % 10000}.jpg"
        filepath = os.path.join(images_dir, filename)

        headers = {
            "User-Agent": random.choice(CONFIG['USER_AGENTS']),
            **({"Referer": "https://www.espn.com/", "Accept": "image/webp,image/apng,image/*,*/*;q=0.8"}
               if 'espn.com' in image_url else 
               {"Referer": "https://onefootball.com/", "Accept": "image/webp,image/apng,image/*,*/*;q=0.8"}
               if 'onefootball.com' in image_url else {})
        }
        response = requests.get(image_url, headers=headers, timeout=10)
        response.raise_for_status()
        with open(filepath, 'wb') as f:
            f.write(response.content)
        logger.info(f"üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {filepath}")
        return filepath
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_url}: {e}")
        return ""

def process_article_for_posting(article_data: Dict[str, Any]) -> Dict[str, Any]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç—å—é –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏."""
    source = article_data.get('source', 'Unknown')
    logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å—é [{source}]: {article_data.get('title', '')[:50]}...")
    
    post_text = format_for_social_media(article_data)
    image_path = download_image(article_data.get('image_url', ''))

    result = {
        'title': article_data.get('title', ''),
        'post_text': post_text,
        'image_path': image_path,
        'image_url': article_data.get('image_url', ''),
        'url': article_data.get('url', '') or article_data.get('link', ''),
        'summary': article_data.get('summary', ''),
        'source': source,
        **(
            {
                'original_title': article_data.get('original_title', ''),
                'original_content': article_data.get('original_content', ''),
                'processed_content': article_data.get('processed_content', '')
            }
            if source in ['ESPN Soccer', 'OneFootball'] else {}
        )
    }
    logger.info(f"–°—Ç–∞—Ç—å—è [{source}] –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    return result

# –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ —Å—Ç–∞—Ä—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º
def summarize_news(title: str, url: str, content: str = '') -> str:
    return create_enhanced_summary({'title': title, 'url': url, 'content': content, 'summary': title})

def simple_summarize(title: str, url: str) -> str:
    return f"üî∏ {title}"
